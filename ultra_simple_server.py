# Deploy timestamp: 20251227001900 - Neon PostgreSQL migration
#!/usr/bin/env python3
"""
‚ö†Ô∏è CRITICAL FOR AI ASSISTANTS: READ START_HERE.md BEFORE MODIFYING THIS FILE

PROTECTION RULES:
- Tab Isolation: Only modify files for the tab you're working on (see TAB_ISOLATION_MAP.md)
- Protected Functions: Account management functions are PROTECTED (see ACCOUNT_MGMT_SNAPSHOT.md)
- Verify Before Fixing: Don't fix things that aren't broken
- One Change at a Time: Make minimal, focused changes

See START_HERE.md for complete protection rules.
"""
from __future__ import annotations
from functools import wraps
import sqlite3
import logging
# Early logger for paper trading (before main logger is configured)
logger = logging.getLogger(__name__)
import asyncio
import argparse
import sys
import os
import json
import re
import time
import threading
import csv
import io
import secrets
import requests
from typing import Optional
from queue import Queue, Empty, Full
from flask import Flask, request, jsonify, render_template, redirect, url_for, flash, session, Response
from flask_socketio import SocketIO, emit
from datetime import datetime, timedelta
try:
    from zoneinfo import ZoneInfo
    CHICAGO_TZ = ZoneInfo('America/Chicago')
except ImportError:
    import pytz
    CHICAGO_TZ = pytz.timezone('America/Chicago')

def get_chicago_time():
    """Get current time in Chicago timezone."""
    return datetime.now(CHICAGO_TZ)

# ============================================================================
# USER AUTHENTICATION MODULE
# ============================================================================
try:
    from user_auth import (
        init_auth_system, login_required, admin_required,
        get_current_user, get_current_user_id, is_logged_in,
        login_user, logout_user, authenticate_user, create_user,
        auth_context_processor, create_initial_admin, assign_existing_data_to_user
    )
    USER_AUTH_AVAILABLE = True
except ImportError as e:
    USER_AUTH_AVAILABLE = False
    print(f"‚ö†Ô∏è User authentication module not available: {e}")

# ============================================================================
# SECURITY DECORATORS ‚Äî Admin API Key + Session Auth (Dual Mode)
# ============================================================================
def admin_or_api_key_required(f):
    """Require either a valid ADMIN_API_KEY header/param or an admin session."""
    @wraps(f)
    def decorated(*args, **kwargs):
        # Check API key first (for curl/monitoring access)
        api_key = request.headers.get('X-Admin-Key') or request.args.get('admin_key')
        expected_key = os.environ.get('ADMIN_API_KEY')
        if expected_key and api_key == expected_key:
            return f(*args, **kwargs)
        # Fall back to session-based admin check
        if USER_AUTH_AVAILABLE and is_logged_in():
            user = get_current_user()
            if user and user.is_admin:
                return f(*args, **kwargs)
        return jsonify({'error': 'Admin access required'}), 403
    return decorated

def api_login_required(f):
    """Require a logged-in session for API endpoints (returns JSON 401, not redirect)."""
    @wraps(f)
    def decorated(*args, **kwargs):
        if USER_AUTH_AVAILABLE and is_logged_in():
            user = get_current_user()
            if user and user.is_active:
                return f(*args, **kwargs)
        return jsonify({'error': 'Login required'}), 401
    return decorated

# ============================================================================
# DISCORD MODULE (extracted from this file)
# ============================================================================
from discord_notifications import (
    init_discord_notifications, send_discord_dm,
    get_discord_enabled_users, get_users_for_recorder_notifications,
    notify_trade_execution, notify_tp_sl_hit, notify_error,
    notify_daily_summary, broadcast_announcement, is_discord_enabled
)
from discord_routes import discord_bp, init_discord_routes

# ============================================================================
# SUBSCRIPTION SYSTEM
# ============================================================================
try:
    from subscription_models import (
        init_subscription_system, get_user_subscription, get_feature_status,
        check_feature_access, check_limit, get_user_plan_tier, get_all_plans
    )
    from whop_integration import (
        subscription_required, feature_required, link_user_to_whop,
        create_webhook_handler
    )
    SUBSCRIPTION_SYSTEM_AVAILABLE = True
except ImportError as e:
    SUBSCRIPTION_SYSTEM_AVAILABLE = False
    print(f"‚ö†Ô∏è Subscription system not available: {e}")

# ============================================================================
# SCALABILITY MODULES - PostgreSQL, Async Safety, Redis Caching
# ============================================================================
try:
    from async_utils import run_async, async_executor, SafeTradovateClient
    ASYNC_UTILS_AVAILABLE = True
except ImportError:
    ASYNC_UTILS_AVAILABLE = False
    
try:
    from cache import token_cache, position_cache, cache, get_cache_status
    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False

try:
    from production_db import get_db_connection as prod_get_db_connection, check_db_health, get_db_type
    PRODUCTION_DB_AVAILABLE = True
except ImportError:
    PRODUCTION_DB_AVAILABLE = False

# ============================================================================
# TRADINGVIEW REAL-TIME PRICE SERVICE
# ============================================================================
try:
    from tv_price_service import (
        get_ticker, get_paper_engine, start_price_service,
        TradingViewTicker, PaperTradingEngine,
        get_tradingview_auth_token
    )
    TV_PRICE_SERVICE_AVAILABLE = True
except ImportError as e:
    TV_PRICE_SERVICE_AVAILABLE = False
    print(f"‚ö†Ô∏è TradingView price service not available: {e}")
    def get_tradingview_auth_token(session_data):
        return None


# --- Paper trade filter tracking (mirrors broker-side filters) ---
_paper_last_signal_time = {}   # recorder_id -> last signal timestamp
_paper_daily_signal_count = {}  # (recorder_id, date_str) -> count
_paper_trade_dedup = {}  # (recorder_id, symbol, action) -> timestamp


# --- SSE Price Streaming Infrastructure ---
_sse_price_queues = []       # list of queue.Queue, one per connected SSE client
_sse_price_lock = threading.Lock()


def _broadcast_sse_price(root: str):
    """Push current cached price for a symbol to all SSE subscribers.
    Called from TradingView WebSocket handler after _market_data_cache update.
    Thread-safe, non-blocking. Silently skips if no subscribers."""
    if not _sse_price_queues:
        return
    data = _market_data_cache.get(root)
    if not data or not isinstance(data, dict):
        return
    msg = {
        'type': 'price_update',
        'symbol': root,
        'price': data.get('last'),
        'bid': data.get('bid'),
        'ask': data.get('ask'),
        'change': data.get('ch'),
        'change_percent': data.get('chp'),
        'source': data.get('source'),
    }
    with _sse_price_lock:
        dead = []
        for q in _sse_price_queues:
            try:
                q.put_nowait(msg)
            except Full:
                dead.append(q)
        for q in dead:
            _sse_price_queues.remove(q)


def _paper_should_execute_signal(recorder_id: int, action: str, recorder: dict) -> tuple:
    """
    Check if a paper trade signal should execute, applying the same filters the broker uses.
    CLOSE/FLAT/EXIT signals always bypass all filters (capital protection).

    Returns (should_execute: bool, reason: str)
    """
    import time as _time
    from datetime import datetime

    action_upper = action.upper()

    # Close signals ALWAYS execute ‚Äî capital protection
    if action_upper in ['CLOSE', 'FLAT', 'EXIT']:
        return True, 'close_bypass'

    try:
        now = _time.time()
        today_str = datetime.now().strftime('%Y-%m-%d')

        # --- Filter 1: Signal Cooldown ---
        signal_cooldown = int(recorder.get('signal_cooldown') or 0)
        if signal_cooldown > 0:
            last_time = _paper_last_signal_time.get(recorder_id, 0)
            elapsed = now - last_time
            if elapsed < signal_cooldown:
                return False, f'signal_cooldown ({elapsed:.0f}s < {signal_cooldown}s)'

        # --- Filter 2: Max Signals Per Session ---
        max_signals = int(recorder.get('max_signals_per_session') or 0)
        if max_signals > 0:
            count_key = (recorder_id, today_str)
            current_count = _paper_daily_signal_count.get(count_key, 0)
            if current_count >= max_signals:
                return False, f'max_signals_per_session ({current_count} >= {max_signals})'

        # --- Filter 3: Max Daily Loss ---
        max_daily_loss = float(recorder.get('max_daily_loss') or 0)
        if max_daily_loss > 0:
            try:
                import os
                database_url = os.environ.get('DATABASE_URL')
                if database_url:
                    import psycopg2
                    _mdl_conn = psycopg2.connect(database_url)
                    _mdl_ph = '%s'
                    _mdl_today = "CURRENT_DATE"
                else:
                    _mdl_conn = sqlite3.connect('paper_trades.db')
                    _mdl_ph = '?'
                    _mdl_today = "date('now')"
                try:
                    _mdl_cursor = _mdl_conn.cursor()
                    _mdl_cursor.execute(f'''
                        SELECT COALESCE(SUM(pnl), 0) FROM paper_trades
                        WHERE recorder_id = {_mdl_ph} AND status = 'closed'
                        AND DATE(closed_at) = {_mdl_today}
                    ''', (recorder_id,))
                    daily_pnl = float(_mdl_cursor.fetchone()[0])
                    if daily_pnl <= -abs(max_daily_loss):
                        return False, f'max_daily_loss (${daily_pnl:.2f} <= -${max_daily_loss:.2f})'
                finally:
                    _mdl_conn.close()
            except Exception as _mdl_err:
                pass  # Fail open ‚Äî never block trading on DB error

        # --- Filter 4: Time Filters ---
        try:
            from zoneinfo import ZoneInfo
            # Get user timezone
            _user_tz_name = 'America/Chicago'
            try:
                import os
                database_url = os.environ.get('DATABASE_URL')
                _rec_user_id = recorder.get('user_id')
                if _rec_user_id and database_url:
                    import psycopg2
                    _tz_conn = psycopg2.connect(database_url)
                    try:
                        _tz_cursor = _tz_conn.cursor()
                        _tz_cursor.execute('SELECT settings_json FROM users WHERE id = %s', (_rec_user_id,))
                        _tz_row = _tz_cursor.fetchone()
                        if _tz_row and _tz_row[0]:
                            import json as _json
                            _sj = _json.loads(_tz_row[0]) if isinstance(_tz_row[0], str) else _tz_row[0]
                            _user_tz_name = _sj.get('timezone', 'America/Chicago')
                    finally:
                        _tz_conn.close()
            except Exception:
                pass
            _user_tz = ZoneInfo(_user_tz_name)
            current_dt = datetime.now(_user_tz)
            current_time = current_dt.time()

            def _parse_time(time_str):
                if not time_str:
                    return None
                time_str = time_str.strip()
                try:
                    if 'AM' in time_str.upper() or 'PM' in time_str.upper():
                        return datetime.strptime(time_str.upper(), '%I:%M %p').time()
                    return datetime.strptime(time_str, '%H:%M').time()
                except Exception:
                    return None

            def _in_window(ct, start_str, stop_str):
                start = _parse_time(start_str)
                stop = _parse_time(stop_str)
                if not start or not stop:
                    return True
                if start <= stop:
                    return start <= ct <= stop
                else:
                    return ct >= start or ct <= stop

            tf1_en = recorder.get('time_filter_1_enabled', False)
            tf1_start = recorder.get('time_filter_1_start', '')
            tf1_stop = recorder.get('time_filter_1_stop', '')
            tf2_en = recorder.get('time_filter_2_enabled', False)
            tf2_start = recorder.get('time_filter_2_start', '')
            tf2_stop = recorder.get('time_filter_2_stop', '')

            has_tf1 = tf1_en and tf1_start and tf1_stop
            has_tf2 = tf2_en and tf2_start and tf2_stop

            if has_tf1 or has_tf2:
                in_w1 = _in_window(current_time, tf1_start, tf1_stop) if has_tf1 else False
                in_w2 = _in_window(current_time, tf2_start, tf2_stop) if has_tf2 else False
                if not in_w1 and not in_w2:
                    return False, f'time_filter ({current_dt.strftime("%I:%M %p")} outside windows)'
        except Exception:
            pass  # Fail open

        # All filters passed
        return True, 'passed'

    except Exception as _filter_err:
        # Fail open ‚Äî never block trading on filter error
        print(f"‚ö†Ô∏è Paper filter error (fail-open): {_filter_err}", flush=True)
        return True, 'error_failopen'


def _paper_update_filter_tracking(recorder_id: int):
    """Update paper trade filter tracking dicts after a successful paper trade execution."""
    import time as _time
    from datetime import datetime
    _paper_last_signal_time[recorder_id] = _time.time()
    today_str = datetime.now().strftime('%Y-%m-%d')
    count_key = (recorder_id, today_str)
    _paper_daily_signal_count[count_key] = _paper_daily_signal_count.get(count_key, 0) + 1


def record_paper_trade_from_webhook(recorder_id: int, symbol: str, action: str, quantity: int = 1, price: float = None, signal_time: float = 0):
    """
    Record a paper trade from webhook signal.
    Called automatically when webhooks are processed.

    Args:
        recorder_id: The recorder ID
        symbol: The trading symbol (e.g., 'NQ', 'ES')
        action: The action ('LONG', 'SHORT', 'CLOSE')
        quantity: Number of contracts
        price: Entry/exit price (uses live feed if not provided)
        signal_time: Unix timestamp when signal was received (for staleness check)
    """
    print(f"üß™ PAPER TRADE: rec={recorder_id}, sym={symbol}, act={action}, qty={quantity}, price={price}", flush=True)

    # Signal staleness check ‚Äî skip signals older than 30s (matches broker worker threshold)
    import time as _staleness_time
    if signal_time > 0:
        age = _staleness_time.time() - signal_time
        if age > 30:
            print(f"üß™ PAPER TRADE STALE: signal is {age:.1f}s old (>30s), skipping", flush=True)
            return None

    # Normalize symbol to root (e.g., CME_MINI:NQ1!1! -> NQ, MGCJ2026 -> MGC)
    clean_symbol = extract_symbol_root(symbol) or symbol.upper()

    # Dedup: skip if same recorder/symbol/action was just processed within 1 second
    # (process_webhook_directly calls this function twice per signal)
    import time as _time
    dedup_key = (recorder_id, clean_symbol, action.upper())
    now = _time.time()
    last_seen = _paper_trade_dedup.get(dedup_key, 0)
    if now - last_seen < 1.0:
        print(f"üîÑ Paper trade dedup: skipping duplicate {action} {clean_symbol} for rec={recorder_id} ({now - last_seen:.3f}s since last)", flush=True)
        return None
    _paper_trade_dedup[dedup_key] = now

    # If price is provided, we can record directly to DB without needing price service
    if price:
        try:
            return _record_paper_trade_direct(recorder_id, clean_symbol, action, quantity, price)
        except Exception as e:
            print(f"‚ö†Ô∏è Direct paper trade failed: {e}", flush=True)
            import traceback
            print(traceback.format_exc(), flush=True)

    # Fall back to price service if available and no price provided
    if not TV_PRICE_SERVICE_AVAILABLE:
        print("‚ö†Ô∏è No price provided and TV_PRICE_SERVICE not available", flush=True)
        return None

    try:
        paper_engine = get_paper_engine()
        ticker = get_ticker()

        # Get live price
        for tv_format in [f"CME_MINI:{clean_symbol}1!", clean_symbol]:
            price_data = ticker.get_price(tv_format)
            if price_data:
                if action in ['LONG', 'BUY']:
                    price = price_data.get('ask') or price_data.get('last_price')
                elif action in ['SHORT', 'SELL']:
                    price = price_data.get('bid') or price_data.get('last_price')
                else:
                    price = price_data.get('last_price')
                if price:
                    break

        if price:
            return _record_paper_trade_direct(recorder_id, clean_symbol, action, quantity, price)

        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error recording paper trade: {e}", flush=True)
        return None


import threading
_paper_trade_lock = threading.Lock()
_PAPER_FILL_DELAY_S = 0  # No delay ‚Äî use signal price like real broker

def _record_paper_trade_direct(recorder_id: int, symbol: str, action: str, quantity: int, price: float):
    """Direct database recording for paper trades - works without price service"""
    with _paper_trade_lock:  # Serialize paper trades so concurrent signals DCA instead of duplicating
        return _record_paper_trade_direct_inner(recorder_id, symbol, action, quantity, price)

def _record_paper_trade_direct_inner(recorder_id: int, symbol: str, action: str, quantity: int, price: float):
    from datetime import datetime
    import os, time

    # Determine side
    if action in ['LONG', 'BUY']:
        side = 'LONG'
    elif action in ['SHORT', 'SELL']:
        side = 'SHORT'
    else:
        side = 'CLOSE'

    # Adjust entry price to bid/ask (real market orders fill at ask for buys, bid for sells)
    global _market_data_cache
    signal_price = price  # preserve original for logging
    if side in ('LONG', 'SHORT') and price:
        _cache = _market_data_cache.get(symbol, {})
        if side == 'LONG':
            _entry_ask = _cache.get('ask')
            if _entry_ask:
                price = float(_entry_ask)
        else:
            _entry_bid = _cache.get('bid')
            if _entry_bid:
                price = float(_entry_bid)

    print(f"üìù Recording paper trade: {action} {quantity} {symbol} @ {price} (signal={signal_price}, fill={'ask' if side == 'LONG' else 'bid' if side == 'SHORT' else 'signal'})", flush=True)

    # Get database connection
    database_url = os.environ.get('DATABASE_URL')
    use_postgres = bool(database_url)

    if use_postgres:
        import psycopg2
        conn = psycopg2.connect(database_url)
        ph = '%s'
    else:
        conn = sqlite3.connect('paper_trades.db')
        ph = '?'

    cursor = conn.cursor()
    now = datetime.now().isoformat()

    # Ensure table exists
    if use_postgres:
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS paper_trades (
                id SERIAL PRIMARY KEY,
                recorder_id INTEGER NOT NULL,
                symbol TEXT NOT NULL,
                side TEXT NOT NULL,
                quantity REAL NOT NULL,
                entry_price REAL NOT NULL,
                exit_price REAL,
                pnl REAL,
                drawdown REAL,
                cumulative_pnl REAL,
                tp_price REAL,
                sl_price REAL,
                tp_legs TEXT,
                exit_reason TEXT,
                commission REAL DEFAULT 0,
                opened_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                closed_at TIMESTAMP,
                status TEXT DEFAULT 'open'
            )
        ''')
        # Add columns if they don't exist (for existing tables)
        try:
            cursor.execute("ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS drawdown REAL")
            cursor.execute("ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS cumulative_pnl REAL")
            cursor.execute("ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS tp_price REAL")
            cursor.execute("ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS sl_price REAL")
            cursor.execute("ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS tp_legs TEXT")
            cursor.execute("ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS exit_reason TEXT")
            cursor.execute("ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS commission REAL DEFAULT 0")
        except:
            pass
    else:
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS paper_trades (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                recorder_id INTEGER NOT NULL,
                symbol TEXT NOT NULL,
                side TEXT NOT NULL,
                quantity REAL NOT NULL,
                entry_price REAL NOT NULL,
                exit_price REAL,
                pnl REAL,
                drawdown REAL,
                cumulative_pnl REAL,
                tp_price REAL,
                sl_price REAL,
                tp_legs TEXT,
                exit_reason TEXT,
                commission REAL DEFAULT 0,
                opened_at TEXT NOT NULL,
                closed_at TEXT,
                status TEXT DEFAULT 'open'
            )
        ''')
        # Add columns if they don't exist (for existing tables)
        for col in ['drawdown', 'cumulative_pnl', 'tp_price', 'sl_price', 'tp_legs', 'exit_reason', 'commission']:
            try:
                cursor.execute(f"ALTER TABLE paper_trades ADD COLUMN IF NOT EXISTS {col} {'REAL' if col not in ('exit_reason', 'tp_legs') else 'TEXT'}")
            except:
                pass
    conn.commit()

    # Helper to calculate drawdown after closing a trade
    def _calc_drawdown_stats(cursor, ph, recorder_id, new_pnl):
        """Calculate cumulative P&L and drawdown after closing a trade"""
        # Get sum of all closed trades P&L (before this trade)
        cursor.execute(f'''
            SELECT COALESCE(SUM(pnl), 0) FROM paper_trades
            WHERE recorder_id = {ph} AND status = 'closed'
        ''', (recorder_id,))
        prev_total = cursor.fetchone()[0] or 0
        cumulative = prev_total + new_pnl

        # Get peak equity (max cumulative P&L achieved)
        cursor.execute(f'''
            SELECT COALESCE(MAX(cumulative_pnl), 0) FROM paper_trades
            WHERE recorder_id = {ph} AND status = 'closed' AND cumulative_pnl IS NOT NULL
        ''', (recorder_id,))
        peak = cursor.fetchone()[0] or 0

        # If current cumulative is higher than peak, no drawdown
        if cumulative >= peak:
            drawdown = 0
        else:
            drawdown = peak - cumulative

        return cumulative, drawdown

    # Look up first enabled trader's multiplier for this recorder (used by _calc_tp_sl and qty scaling)
    _paper_mult = 1.0
    try:
        cursor.execute(f'''
            SELECT multiplier FROM traders
            WHERE recorder_id = {ph} AND enabled = {'TRUE' if use_postgres else '1'}
            ORDER BY id LIMIT 1
        ''', (recorder_id,))
        _pm_row = cursor.fetchone()
        if _pm_row and _pm_row[0]:
            _paper_mult = float(_pm_row[0])
    except Exception:
        pass

    try:
        # Helper: fetch recorder TP/SL settings and calculate prices
        def _calc_tp_sl(entry, direction, total_qty=None, multiplier=1.0):
            _tp, _sl = None, None
            _tp_legs_list = None
            try:
                cursor.execute(f'SELECT tp_targets, sl_enabled, sl_amount, sl_units, trim_units FROM recorders WHERE id = {ph}', (recorder_id,))
                rec_row = cursor.fetchone()
                if rec_row:
                    tp_targets_raw, sl_en, sl_amt, sl_un, trim_un = rec_row

                    # Overlay trader-level overrides (matches real execution at ~17216-17250)
                    try:
                        cursor.execute(f'''
                            SELECT tp_targets, sl_enabled, sl_amount, sl_units
                            FROM traders WHERE recorder_id = {ph} AND enabled = {'TRUE' if use_postgres else '1'}
                            ORDER BY id LIMIT 1
                        ''', (recorder_id,))
                        _t_row = cursor.fetchone()
                        if _t_row:
                            _t_tp, _t_sl_en, _t_sl_amt, _t_sl_un = _t_row
                            if _t_tp is not None:
                                tp_targets_raw = _t_tp
                            if _t_sl_en is not None:
                                sl_en = _t_sl_en
                            if _t_sl_amt is not None:
                                sl_amt = _t_sl_amt
                            if _t_sl_un is not None:
                                sl_un = _t_sl_un
                    except Exception:
                        pass  # Fall back to recorder settings if trader lookup fails

                    from recorder_service import get_tick_size
                    tick_sz = get_tick_size(symbol)
                    if tp_targets_raw:
                        import json
                        try:
                            tp_list = json.loads(tp_targets_raw) if isinstance(tp_targets_raw, str) else tp_targets_raw
                            if tp_list and len(tp_list) > 0:
                                # First TP leg -> backward-compatible tp_price
                                first_tp = tp_list[0]
                                tp_ticks = first_tp.get('ticks') or first_tp.get('value') or 0
                                if tp_ticks and float(tp_ticks) > 0:
                                    tp_off = float(tp_ticks) * tick_sz
                                    _tp = entry + tp_off if direction == 'LONG' else entry - tp_off
                                    _tp = round(round(_tp / tick_sz) * tick_sz, 10)

                                # Build all TP legs with prices and trim quantities
                                if total_qty and total_qty > 0:
                                    _tp_legs_list = []
                                    remaining = int(total_qty)
                                    for i, leg in enumerate(tp_list):
                                        leg_ticks = float(leg.get('ticks') or leg.get('value') or 0)
                                        leg_trim = float(leg.get('trim', 0))
                                        if leg_ticks <= 0:
                                            continue
                                        leg_off = leg_ticks * tick_sz
                                        leg_price = entry + leg_off if direction == 'LONG' else entry - leg_off
                                        leg_price = round(round(leg_price / tick_sz) * tick_sz, 10)
                                        # Calculate trim qty (Rule 13: Contracts mode scales by multiplier)
                                        if i == len(tp_list) - 1:
                                            leg_qty = remaining  # Last leg gets remainder
                                        elif trim_un == 'Percent':
                                            leg_qty = max(1, int(round(total_qty * (leg_trim / 100.0))))
                                        else:  # Contracts ‚Äî scale by multiplier, cap at remaining
                                            leg_qty = max(1, int(round(leg_trim * multiplier))) if leg_trim > 0 else 1
                                            leg_qty = min(leg_qty, remaining)
                                        if leg_qty <= 0:
                                            continue
                                        remaining -= leg_qty
                                        _tp_legs_list.append({
                                            'ticks': leg_ticks,
                                            'price': leg_price,
                                            'qty': leg_qty,
                                            'filled': False
                                        })
                                        if remaining <= 0:
                                            break
                        except:
                            pass
                    if sl_en and sl_amt and float(sl_amt) > 0:
                        sl_ticks = float(sl_amt)
                        sl_off = sl_ticks if sl_un == 'Points' else sl_ticks * tick_sz
                        _sl = entry - sl_off if direction == 'LONG' else entry + sl_off
                        _sl = round(round(_sl / tick_sz) * tick_sz, 10)
            except Exception as e:
                print(f"‚ö†Ô∏è Could not fetch recorder TP/SL settings: {e}", flush=True)
            return _tp, _sl, _tp_legs_list

        # Helper: close a single open position with P&L
        def _close_position(pos_id, pos_side, pos_qty, pos_entry, exit_px, reason):
            from tv_price_service import FUTURES_SPECS
            spec = FUTURES_SPECS.get(extract_symbol_root(symbol), {'point_value': 1.0})
            pv = spec['point_value']
            if pos_side == 'LONG':
                _gross = (exit_px - pos_entry) * pv * pos_qty
            else:
                _gross = (pos_entry - exit_px) * pv * pos_qty
            _comm = _calc_paper_commission(symbol, pos_qty)
            _pnl = _gross - _comm
            _cum, _dd = _calc_drawdown_stats(cursor, ph, recorder_id, _pnl)
            # Per-trade drawdown from MAE tracking (how deep underwater before close)
            global _paper_trade_mae
            global _paper_trail_state
            mae = _paper_trade_mae.pop(pos_id, 0.0)
            _paper_trail_state.pop(pos_id, None)
            trade_dd = abs(mae) if mae < 0 else 0
            cursor.execute(f'''
                UPDATE paper_trades SET status = 'closed', exit_price = {ph}, pnl = {ph},
                cumulative_pnl = {ph}, drawdown = {ph}, exit_reason = {ph}, commission = {ph}, closed_at = {ph}
                WHERE id = {ph}
            ''', (exit_px, _pnl, _cum, trade_dd, reason, _comm, now, pos_id))
            return _pnl

        if side in ['LONG', 'SHORT']:
            # Fetch ALL open positions for this recorder/symbol (not just LIMIT 1)
            cursor.execute(f'''
                SELECT id, side, quantity, entry_price FROM paper_trades
                WHERE recorder_id = {ph} AND symbol = {ph} AND status = 'open'
                ORDER BY opened_at ASC
            ''', (recorder_id, symbol))
            all_open = cursor.fetchall()

            if all_open:
                # Separate same-direction and opposite-direction positions
                same_dir = [r for r in all_open if r[1] == side]
                opp_dir = [r for r in all_open if r[1] != side]

                # ADDED: DCA-off check ‚Äî close existing positions and clear lists so
                # existing DCA block below naturally skips (Rule 12, add-only approach)
                if same_dir:
                    cursor.execute(f'SELECT avg_down_enabled FROM recorders WHERE id = {ph}', (recorder_id,))
                    _dca_row = cursor.fetchone()
                    _is_dca_on = bool(_dca_row and _dca_row[0])
                    if not _is_dca_on:
                        try:
                            cursor.execute(f'''
                                SELECT dca_enabled FROM traders
                                WHERE recorder_id = {ph} AND enabled = {'TRUE' if use_postgres else '1'}
                                ORDER BY id LIMIT 1
                            ''', (recorder_id,))
                            _t_dca_check = cursor.fetchone()
                            if _t_dca_check and _t_dca_check[0]:
                                _is_dca_on = True
                        except Exception:
                            pass
                    if not _is_dca_on:
                        for pos in same_dir + opp_dir:
                            _pos_pnl = _close_position(pos[0], pos[1], pos[2], pos[3], price, 'new_entry')
                            print(f"üìù DCA OFF: Closed {pos[1]} #{pos[0]}, P&L: ${_pos_pnl:.2f}", flush=True)
                        print(f"üìù DCA OFF: Closed {len(same_dir)+len(opp_dir)} positions, opening fresh {side}", flush=True)
                        same_dir = []  # Clear so existing DCA block below skips
                        opp_dir = []

                # EXISTING CODE BELOW ‚Äî UNTOUCHED
                if same_dir:
                    # DCA into the FIRST same-direction position, close any extras as duplicates
                    primary = same_dir[0]
                    exist_id, exist_side, exist_qty, exist_entry = primary

                    # Close duplicate same-direction positions (stacking bug cleanup)
                    for dup in same_dir[1:]:
                        dup_pnl = _close_position(dup[0], dup[1], dup[2], dup[3], price, 'dedup')
                        print(f"üìù Closed duplicate {dup[1]} position #{dup[0]} (dedup), P&L: ${dup_pnl:.2f}", flush=True)

                    # Also close any opposite-direction positions (stale from before)
                    for opp in opp_dir:
                        opp_pnl = _close_position(opp[0], opp[1], opp[2], opp[3], price, 'signal')
                        print(f"üìù Closed stale opposite {opp[1]} position #{opp[0]}, P&L: ${opp_pnl:.2f}", flush=True)

                    # DCA: add to primary position (respect max_contracts_per_trade)
                    max_contracts = None
                    try:
                        cursor.execute(f'SELECT max_contracts_per_trade FROM recorders WHERE id = {ph}', (recorder_id,))
                        mc_row = cursor.fetchone()
                        if mc_row and mc_row[0] and int(mc_row[0]) > 0:
                            max_contracts = int(mc_row[0])
                    except Exception:
                        pass

                    new_qty = exist_qty + quantity
                    if max_contracts and new_qty > max_contracts:
                        print(f"üìù DCA capped: {new_qty} would exceed max_contracts={max_contracts}, skipping add", flush=True)
                        conn.commit()
                        conn.close()
                        return {'success': True, 'symbol': symbol, 'action': 'DCA_CAPPED', 'reason': f'qty {new_qty} exceeds max {max_contracts}'}

                    avg_entry = ((exist_entry * exist_qty) + (price * quantity)) / new_qty
                    tp_price, sl_price, tp_legs_list = _calc_tp_sl(avg_entry, side, total_qty=new_qty, multiplier=_paper_mult)
                    tp_legs_json = json.dumps(tp_legs_list) if tp_legs_list else None

                    cursor.execute(f'''
                        UPDATE paper_trades SET quantity = {ph}, entry_price = {ph}, tp_price = {ph}, sl_price = {ph}, tp_legs = {ph}
                        WHERE id = {ph}
                    ''', (new_qty, avg_entry, tp_price, sl_price, tp_legs_json, exist_id))

                    # Reset automatic DCA tracking so it recalculates from new avg entry
                    global _paper_dca_next_price
                    _paper_dca_next_price.pop(exist_id, None)

                    tp_str = f"TP={tp_price:.2f}" if tp_price else "TP=None"
                    sl_str = f"SL={sl_price:.2f}" if sl_price else "SL=None"
                    print(f"üìù DCA: Added {quantity} to {side} position | New qty: {new_qty} | Avg entry: ${avg_entry:.2f} | {tp_str} | {sl_str}", flush=True)

                    conn.commit()
                    return {'success': True, 'symbol': symbol, 'action': 'DCA', 'price': price, 'avg_entry': avg_entry, 'quantity': new_qty}

                else:
                    # Only opposite-direction positions exist
                    cursor.execute(f'SELECT avg_down_enabled FROM recorders WHERE id = {ph}', (recorder_id,))
                    rec_check = cursor.fetchone()
                    is_dca_strategy = bool(rec_check and rec_check[0])
                    # Also check trader-level dca_enabled (matches recorder_service.py:2346-2347)
                    if not is_dca_strategy:
                        try:
                            cursor.execute(f'''
                                SELECT dca_enabled FROM traders
                                WHERE recorder_id = {ph} AND enabled = {'TRUE' if use_postgres else '1'}
                                ORDER BY id LIMIT 1
                            ''', (recorder_id,))
                            _t_dca_row = cursor.fetchone()
                            if _t_dca_row and _t_dca_row[0]:
                                is_dca_strategy = True
                        except Exception:
                            pass

                    if is_dca_strategy:
                        # DCA strategies ignore opposite direction signals - wait for TP exit
                        print(f"üìù DCA Strategy: Ignoring opposite direction signal ({side}) - waiting for TP exit on {opp_dir[0][1]} position", flush=True)
                        conn.commit()
                        conn.close()
                        return {'success': True, 'symbol': symbol, 'action': 'IGNORED_DCA', 'reason': f'DCA strategy ignores {side} while {opp_dir[0][1]} position open'}

                    # Non-DCA: close ALL opposite positions and open new one
                    for opp in opp_dir:
                        opp_pnl = _close_position(opp[0], opp[1], opp[2], opp[3], price, 'signal')
                        print(f"üìù Closed {opp[1]} position #{opp[0]} (signal), P&L: ${opp_pnl:.2f}", flush=True)

            # Open new position with TP/SL from recorder settings
            tp_price, sl_price, tp_legs_list = _calc_tp_sl(price, side, total_qty=quantity, multiplier=_paper_mult)
            tp_legs_json = json.dumps(tp_legs_list) if tp_legs_list else None

            cursor.execute(f'''
                INSERT INTO paper_trades (recorder_id, symbol, side, quantity, entry_price, tp_price, sl_price, tp_legs, opened_at, status)
                VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, 'open')
            ''', (recorder_id, symbol, side, quantity, price, tp_price, sl_price, tp_legs_json, now))
            tp_str = f"TP={tp_price:.2f}" if tp_price else "TP=None"
            legs_str = f" | Legs: {len(tp_legs_list)}" if tp_legs_list and len(tp_legs_list) > 1 else ""
            sl_str = f"SL={sl_price:.2f}" if sl_price else "SL=None"
            print(f"üìù Opened {side} {quantity} {symbol} @ {price} | {tp_str}{legs_str} | {sl_str}", flush=True)

        else:  # CLOSE action
            # Find and close ALL open positions for this recorder/symbol
            cursor.execute(f'''
                SELECT id, side, quantity, entry_price FROM paper_trades
                WHERE recorder_id = {ph} AND symbol = {ph} AND status = 'open'
                ORDER BY opened_at ASC
            ''', (recorder_id, symbol))
            all_open = cursor.fetchall()

            for pos in all_open:
                pos_pnl = _close_position(pos[0], pos[1], pos[2], pos[3], price, 'signal')
                print(f"üìù Closed {pos[1]} position #{pos[0]} (signal), P&L: ${pos_pnl:.2f}", flush=True)

        conn.commit()
        return {'success': True, 'symbol': symbol, 'action': action, 'price': price}

    except Exception as e:
        print(f"‚ö†Ô∏è DB error in paper trade: {e}", flush=True)
        conn.rollback()
        raise
    finally:
        conn.close()


# ============================================================================
# PAPER TRADING TP/SL MONITOR - Uses live TradingView tick feed
# ============================================================================
_paper_monitor_running = False
_paper_dca_next_price = {}  # trade_id -> next DCA trigger price (for automatic price-based DCA)
_paper_trade_mae = {}  # trade_id -> worst unrealized P&L (most negative = deepest drawdown)
_paper_trail_state = {}  # trade_id -> {'best_price': float, 'trail_active': bool, 'be_triggered': bool}

# Commission per side per contract (round-turn = 2x). Keyed by symbol root.
# Default: $0.52/side for micros, $1.29/side for full-size. Override with recorder setting later.
_PAPER_COMMISSION_PER_SIDE = {
    'MNQ': 0.52, 'MES': 0.52, 'MYM': 0.52, 'M2K': 0.52, 'MGC': 0.52, 'MCL': 0.52, 'SIL': 0.52,
    'NQ': 1.29, 'ES': 1.29, 'YM': 1.29, 'RTY': 1.29, 'GC': 1.29, 'CL': 1.29, 'SI': 1.29,
}
_PAPER_COMMISSION_DEFAULT = 0.52  # fallback for unknown symbols

# Fill-through ticks: TP limit orders require price to trade THROUGH the level by N ticks.
# 0 = fill on touch (TradingView-style, optimistic), 1 = conservative/realistic.
# Only applies to TP (limit orders). SL (stop‚Üímarket) fills regardless.
_PAPER_FILL_THROUGH_TICKS = 1


def _calc_paper_commission(symbol: str, quantity: float) -> float:
    """Calculate round-turn commission for a paper trade close."""
    sym_root = extract_symbol_root(symbol) if symbol else ''
    per_side = _PAPER_COMMISSION_PER_SIDE.get(sym_root, _PAPER_COMMISSION_DEFAULT)
    return quantity * per_side * 2  # entry + exit = round-turn


def _is_cme_session_break() -> bool:
    """Check if we're in the CME daily maintenance window (no fills possible).
    Equity/metals/energy: 5:00 PM - 6:00 PM ET Mon-Fri, all day Sat, Sun before 6 PM."""
    from datetime import datetime, time
    from zoneinfo import ZoneInfo
    try:
        now_et = datetime.now(ZoneInfo('US/Eastern'))
        wd = now_et.weekday()
        if wd == 5:  # Saturday ‚Äî closed all day
            return True
        if wd == 6:  # Sunday ‚Äî closed until 6 PM ET
            return now_et.time() < time(18, 0)
        # Mon-Fri: maintenance 5:00 PM - 6:00 PM ET
        return time(17, 0) <= now_et.time() < time(18, 0)
    except Exception:
        return False  # Fail open ‚Äî don't skip checks if TZ fails


def _close_paper_trade_tpsl(trade_id: int, exit_price: float, exit_reason: str, recorder_id: int, side: str, entry_price: float, quantity: float, symbol: str):
    """Close a paper trade due to TP or SL hit"""
    from datetime import datetime
    import os

    database_url = os.environ.get('DATABASE_URL')
    use_postgres = bool(database_url)

    if use_postgres:
        import psycopg2
        conn = psycopg2.connect(database_url)
        ph = '%s'
    else:
        conn = sqlite3.connect('paper_trades.db')
        ph = '?'

    cursor = conn.cursor()
    now = datetime.now().isoformat()

    try:
        from tv_price_service import FUTURES_SPECS
        spec = FUTURES_SPECS.get(extract_symbol_root(symbol), {'point_value': 1.0})
        point_value = spec['point_value']

        if side == 'LONG':
            gross_pnl = (exit_price - entry_price) * point_value * quantity
        else:
            gross_pnl = (entry_price - exit_price) * point_value * quantity

        # Commission: round-turn per contract
        commission = _calc_paper_commission(symbol, quantity)
        pnl = gross_pnl - commission

        # Get cumulative P&L stats
        cursor.execute(f'SELECT COALESCE(SUM(pnl), 0) FROM paper_trades WHERE recorder_id = {ph} AND status = %s' if use_postgres else f'SELECT COALESCE(SUM(pnl), 0) FROM paper_trades WHERE recorder_id = {ph} AND status = ?', (recorder_id, 'closed'))
        prev_total = cursor.fetchone()[0] or 0
        cumulative = prev_total + pnl

        # Per-trade drawdown: max adverse excursion (how deep underwater the trade went)
        global _paper_trade_mae
        mae = _paper_trade_mae.pop(trade_id, 0.0)
        trade_drawdown = abs(mae) if mae < 0 else 0  # Store as positive dollar amount

        cursor.execute(f'''
            UPDATE paper_trades SET status = 'closed', exit_price = {ph}, pnl = {ph},
            cumulative_pnl = {ph}, drawdown = {ph}, exit_reason = {ph}, commission = {ph}, closed_at = {ph}
            WHERE id = {ph}
        ''', (exit_price, pnl, cumulative, trade_drawdown, exit_reason, commission, now, trade_id))
        conn.commit()

        # Clean up DCA, MAE, and trail tracking for closed trade
        global _paper_dca_next_price
        global _paper_trail_state
        _paper_dca_next_price.pop(trade_id, None)
        _paper_trade_mae.pop(trade_id, None)
        _paper_trail_state.pop(trade_id, None)

        dd_str = f" | DD: ${trade_drawdown:.2f}" if trade_drawdown > 0 else ""
        comm_str = f" | Comm: ${commission:.2f}" if commission > 0 else ""
        emoji = "üéØ" if exit_reason == 'tp' else "üõë"
        print(f"{emoji} Paper {side} closed by {exit_reason.upper()}: {symbol} @ {exit_price:.2f} | Net P&L: ${pnl:.2f} (gross: ${gross_pnl:.2f}){comm_str}{dd_str}", flush=True)

    except Exception as e:
        print(f"‚ö†Ô∏è Error closing paper trade {trade_id}: {e}", flush=True)
        conn.rollback()
    finally:
        conn.close()


def _get_live_price_for_symbol(symbol: str) -> float:
    """Get live price for a symbol from TradingView cache"""
    global _market_data_cache
    if not symbol:
        return None

    # Use extract_symbol_root for consistent parsing (handles COMEX:MGC1!, CME_MINI:NQ1!, etc.)
    symbol_root = extract_symbol_root(symbol)

    live_price = None
    if symbol_root and symbol_root in _market_data_cache:
        live_price = _market_data_cache[symbol_root].get('last')
    # Also try with the raw symbol uppercased
    if not live_price and symbol.upper() in _market_data_cache:
        live_price = _market_data_cache[symbol.upper()].get('last')

    # Micro/mini contract fallbacks ‚Äî same price as full-size contract
    if not live_price:
        PRICE_FALLBACKS = {
            'MGC': 'GC', 'MES': 'ES', 'MNQ': 'NQ', 'M2K': 'RTY', 'MYM': 'YM',
            'MCL': 'CL', 'SIL': 'SI', 'MGCG': 'GC', 'MGCJ': 'GC', 'MGCH': 'GC',
        }
        # Strip trailing contract month codes for lookup (e.g., MGCJ2026 -> MGC)
        lookup = symbol_root
        if lookup and len(lookup) > 3 and lookup[:3] in ('MGC', 'MES', 'MNQ', 'SIL', 'MCL'):
            lookup = lookup[:3]
        fallback = PRICE_FALLBACKS.get(lookup)
        if fallback and fallback in _market_data_cache:
            live_price = _market_data_cache[fallback].get('last')

    return live_price


def _calculate_unrealized_pnl(symbol: str, side: str, quantity: float, entry_price: float, live_price: float) -> float:
    """Calculate unrealized P&L for an open position"""
    from tv_price_service import FUTURES_SPECS

    # Get symbol root for specs lookup
    symbol_root = extract_symbol_root(symbol) if symbol else 'MNQ'
    spec = FUTURES_SPECS.get(symbol_root, {'point_value': 1.0})
    point_value = spec['point_value']

    if side == 'LONG':
        return (live_price - entry_price) * point_value * quantity
    else:
        return (entry_price - live_price) * point_value * quantity


def check_paper_max_daily_loss():
    """Check all open paper trades against max_daily_loss and auto-close if breached"""
    import os
    from datetime import datetime

    database_url = os.environ.get('DATABASE_URL')
    use_postgres = bool(database_url)

    if use_postgres:
        import psycopg2
        conn = psycopg2.connect(database_url)
        ph = '%s'
        today_filter = "DATE(closed_at) = CURRENT_DATE"
    else:
        conn = sqlite3.connect('paper_trades.db')
        ph = '?'
        today_filter = "DATE(closed_at) = DATE('now')"

    cursor = conn.cursor()

    try:
        # Get all open paper trades
        cursor.execute('''
            SELECT id, recorder_id, symbol, side, quantity, entry_price
            FROM paper_trades
            WHERE status = 'open'
        ''')
        open_trades = cursor.fetchall()

        if not open_trades:
            return

        # Group trades by recorder_id
        recorder_trades = {}
        for trade in open_trades:
            trade_id, recorder_id, symbol, side, quantity, entry_price = trade
            if recorder_id not in recorder_trades:
                recorder_trades[recorder_id] = []
            recorder_trades[recorder_id].append({
                'id': trade_id,
                'symbol': symbol,
                'side': side,
                'quantity': quantity,
                'entry_price': entry_price
            })

        # Check each recorder's max_daily_loss
        for recorder_id, trades in recorder_trades.items():
            # Get recorder's max_daily_loss setting
            cursor.execute(f'SELECT max_daily_loss, name FROM recorders WHERE id = {ph}', (recorder_id,))
            rec_row = cursor.fetchone()
            if not rec_row:
                continue

            max_daily_loss = float(rec_row[0] or 0)
            recorder_name = rec_row[1] if len(rec_row) > 1 else f"Recorder {recorder_id}"

            if max_daily_loss <= 0:
                continue  # No max loss set

            # Calculate today's realized P&L (closed trades)
            cursor.execute(f'''
                SELECT COALESCE(SUM(pnl), 0) FROM paper_trades
                WHERE recorder_id = {ph} AND status = 'closed' AND {today_filter}
            ''', (recorder_id,))
            realized_pnl = cursor.fetchone()[0] or 0

            # Calculate unrealized P&L for all open positions
            unrealized_pnl = 0
            for trade in trades:
                live_price = _get_live_price_for_symbol(trade['symbol'])
                if live_price:
                    unrealized_pnl += _calculate_unrealized_pnl(
                        trade['symbol'], trade['side'], trade['quantity'],
                        trade['entry_price'], live_price
                    )

            # Total daily P&L = realized + unrealized
            total_daily_pnl = realized_pnl + unrealized_pnl

            # Check if max daily loss breached
            if total_daily_pnl <= -max_daily_loss:
                print(f"üö® MAX DAILY LOSS BREACHED for [{recorder_name}]!", flush=True)
                print(f"   Realized: ${realized_pnl:.2f} | Unrealized: ${unrealized_pnl:.2f} | Total: ${total_daily_pnl:.2f} | Limit: -${max_daily_loss:.2f}", flush=True)

                # Close all open positions for this recorder
                for trade in trades:
                    live_price = _get_live_price_for_symbol(trade['symbol'])
                    if live_price:
                        _close_paper_trade_tpsl(
                            trade['id'], live_price, 'max_loss', recorder_id,
                            trade['side'], trade['entry_price'], trade['quantity'], trade['symbol']
                        )
                        print(f"   üíÄ Auto-closed {trade['side']} {trade['symbol']} @ {live_price:.2f}", flush=True)

    except Exception as e:
        print(f"‚ö†Ô∏è Error checking paper max daily loss: {e}", flush=True)
    finally:
        conn.close()


def check_paper_trades_tpsl():
    """Check all open paper trades against live prices for TP/SL hits AND max daily loss"""
    import os
    from datetime import datetime

    # Skip TP/SL checks during CME maintenance break (no real fills possible)
    if _is_cme_session_break():
        check_paper_max_daily_loss()  # Still enforce risk limits
        return

    database_url = os.environ.get('DATABASE_URL')
    use_postgres = bool(database_url)

    if use_postgres:
        import psycopg2
        conn = psycopg2.connect(database_url)
        ph = '%s'
    else:
        conn = sqlite3.connect('paper_trades.db')
        ph = '?'

    cursor = conn.cursor()

    try:
        # Get ALL open paper trades with recorder settings for trailing/break-even
        cursor.execute('''
            SELECT pt.id, pt.recorder_id, pt.symbol, pt.side, pt.quantity, pt.entry_price,
                   pt.tp_price, pt.sl_price, pt.tp_legs,
                   r.sl_type, r.trail_trigger, r.trail_freq,
                   r.break_even_enabled, r.break_even_ticks, r.break_even_offset,
                   r.sl_amount, r.sl_units
            FROM paper_trades pt
            LEFT JOIN recorders r ON pt.recorder_id = r.id
            WHERE pt.status = 'open'
        ''')
        open_trades = cursor.fetchall()

        if not open_trades:
            # Still check max daily loss even if no TP/SL trades
            check_paper_max_daily_loss()
            return

        # Get live prices from TradingView cache (use globals, not import)
        global _market_data_cache

        from tv_price_service import FUTURES_SPECS
        global _paper_trade_mae
        global _paper_dca_next_price
        global _paper_trail_state

        for trade in open_trades:
            trade_id, recorder_id, symbol, side, quantity, entry_price, tp_price, sl_price, tp_legs_raw, \
                sl_type, trail_trigger, trail_freq, be_enabled, be_ticks, be_offset, \
                r_sl_amount, r_sl_units = trade

            live_price = _get_live_price_for_symbol(symbol)

            if not live_price:
                continue  # No live price available

            # Track max adverse excursion (per-trade drawdown)
            sym_root = extract_symbol_root(symbol)
            spec = FUTURES_SPECS.get(sym_root, {'point_value': 1.0})
            pv = spec['point_value']
            if side == 'LONG':
                unrealized = (live_price - entry_price) * pv * quantity
            else:
                unrealized = (entry_price - live_price) * pv * quantity
            current_mae = _paper_trade_mae.get(trade_id, 0.0)
            if unrealized < current_mae:
                _paper_trade_mae[trade_id] = unrealized

            # Get bid/ask from cache for spread-aware fills (fallback to live_price/last)
            # Use 'or' pattern (Rule 17) ‚Äî .get() default is ignored if key exists with None value
            _cache_entry = _market_data_cache.get(sym_root, {})
            _bid = _cache_entry.get('bid') or live_price
            _ask = _cache_entry.get('ask') or live_price

            from recorder_service import get_tick_size as _gts
            _tp_tick_sz = _gts(symbol)
            _tp_offset = _PAPER_FILL_THROUGH_TICKS * _tp_tick_sz

            # --- Trailing stop + break-even simulation ---
            if sl_price and (sl_type == 'Trail' or be_enabled):
                # Initialize trail state for this trade if needed
                if trade_id not in _paper_trail_state:
                    _paper_trail_state[trade_id] = {
                        'best_price': entry_price,
                        'trail_active': False,
                        'be_triggered': False
                    }
                ts = _paper_trail_state[trade_id]
                sl_moved = False

                # Update best price seen (most favorable for our side)
                if side == 'LONG':
                    if live_price > ts['best_price']:
                        ts['best_price'] = live_price
                else:
                    if live_price < ts['best_price']:
                        ts['best_price'] = live_price

                # Break-even: snap SL to entry when price moves enough in our favor
                if be_enabled and be_ticks and not ts.get('be_triggered'):
                    be_ticks_val = float(be_ticks)
                    be_offset_val = float(be_offset) if be_offset else 0
                    if side == 'LONG':
                        profit_ticks = (ts['best_price'] - entry_price) / _tp_tick_sz
                        if profit_ticks >= be_ticks_val:
                            new_sl = entry_price + (be_offset_val * _tp_tick_sz)
                            new_sl = round(round(new_sl / _tp_tick_sz) * _tp_tick_sz, 10)
                            if new_sl > sl_price:
                                sl_price = new_sl
                                sl_moved = True
                            ts['be_triggered'] = True
                    else:
                        profit_ticks = (entry_price - ts['best_price']) / _tp_tick_sz
                        if profit_ticks >= be_ticks_val:
                            new_sl = entry_price - (be_offset_val * _tp_tick_sz)
                            new_sl = round(round(new_sl / _tp_tick_sz) * _tp_tick_sz, 10)
                            if new_sl < sl_price:
                                sl_price = new_sl
                                sl_moved = True
                            ts['be_triggered'] = True

                # Trailing stop: move SL to follow price by sl_amount distance
                if sl_type == 'Trail':
                    trail_trigger_val = float(trail_trigger) if trail_trigger else 0
                    trail_freq_val = float(trail_freq) if trail_freq else 0

                    # Check if trailing is activated
                    if not ts['trail_active']:
                        if side == 'LONG':
                            profit_ticks = (ts['best_price'] - entry_price) / _tp_tick_sz
                        else:
                            profit_ticks = (entry_price - ts['best_price']) / _tp_tick_sz
                        if trail_trigger_val == 0 or profit_ticks >= trail_trigger_val:
                            ts['trail_active'] = True

                    if ts['trail_active'] and r_sl_amount:
                        _sl_amt = float(r_sl_amount)
                        _sl_un = r_sl_units or 'Ticks'
                        _sl_dist = _sl_amt if _sl_un == 'Points' else _sl_amt * _tp_tick_sz
                        if side == 'LONG':
                            new_sl = ts['best_price'] - _sl_dist
                            new_sl = round(round(new_sl / _tp_tick_sz) * _tp_tick_sz, 10)
                            # Only move SL up, never down
                            if new_sl > sl_price:
                                # Apply trail_freq: only update if moved at least trail_freq ticks
                                if trail_freq_val == 0 or (new_sl - sl_price) / _tp_tick_sz >= trail_freq_val:
                                    sl_price = new_sl
                                    sl_moved = True
                        else:
                            new_sl = ts['best_price'] + _sl_dist
                            new_sl = round(round(new_sl / _tp_tick_sz) * _tp_tick_sz, 10)
                            # Only move SL down, never up
                            if new_sl < sl_price:
                                if trail_freq_val == 0 or (sl_price - new_sl) / _tp_tick_sz >= trail_freq_val:
                                    sl_price = new_sl
                                    sl_moved = True

                # Persist updated SL to DB
                if sl_moved:
                    cursor.execute(f'UPDATE paper_trades SET sl_price = {ph} WHERE id = {ph}', (sl_price, trade_id))
                    conn.commit()

            # --- Multi-leg TP support ---
            # If tp_legs exists, check each unfilled leg for partial closes
            # If tp_legs is NULL, fall back to single tp_price (backward compat)
            tp_legs = None
            if tp_legs_raw:
                try:
                    tp_legs = json.loads(tp_legs_raw) if isinstance(tp_legs_raw, str) else tp_legs_raw
                except:
                    tp_legs = None

            if tp_legs and len(tp_legs) > 1:
                # Multi-leg TP: check each unfilled leg independently
                # For partial fills, INSERT a new closed record per leg and reduce parent qty
                legs_modified = False
                current_qty = quantity
                now_ts = datetime.now().isoformat()
                for leg in tp_legs:
                    if leg.get('filled'):
                        continue
                    leg_price = leg.get('price', 0)
                    leg_qty = leg.get('qty', 0)
                    if not leg_price or not leg_qty or leg_qty <= 0:
                        continue
                    # Cap leg_qty at current remaining quantity
                    leg_qty = min(leg_qty, int(current_qty))
                    if leg_qty <= 0:
                        continue
                    # Check if this leg's TP is hit (same fill-through logic)
                    if (side == 'LONG' and _bid >= leg_price + _tp_offset) or (side == 'SHORT' and _ask <= leg_price - _tp_offset):
                        # Calculate P&L for this partial fill
                        if side == 'LONG':
                            _leg_gross = (leg_price - entry_price) * pv * leg_qty
                        else:
                            _leg_gross = (entry_price - leg_price) * pv * leg_qty
                        _leg_comm = _calc_paper_commission(symbol, leg_qty)
                        _leg_pnl = _leg_gross - _leg_comm
                        # Insert a new CLOSED record for this partial fill
                        cursor.execute(f'''
                            INSERT INTO paper_trades (recorder_id, symbol, side, quantity, entry_price,
                            exit_price, pnl, exit_reason, commission, opened_at, closed_at, status)
                            VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, 'closed')
                        ''', (recorder_id, symbol, side, leg_qty, entry_price,
                              leg_price, _leg_pnl, 'tp_leg', _leg_comm, now_ts, now_ts))
                        leg['filled'] = True
                        current_qty -= leg_qty
                        legs_modified = True
                        print(f"üéØ Paper TP leg filled: {leg_qty} of {side} {symbol} @ {leg_price:.2f} | P&L: ${_leg_pnl:.2f} | Remaining: {int(current_qty)}", flush=True)

                if legs_modified:
                    if current_qty <= 0:
                        # All legs filled ‚Äî close the parent trade
                        cursor.execute(f'''
                            UPDATE paper_trades SET status = 'closed', quantity = 0, tp_legs = {ph},
                            exit_reason = 'tp', closed_at = {ph}
                            WHERE id = {ph}
                        ''', (json.dumps(tp_legs), now_ts, trade_id))
                        # Clean up MAE, DCA, and trail tracking
                        _paper_trade_mae.pop(trade_id, None)
                        _paper_dca_next_price.pop(trade_id, None)
                        _paper_trail_state.pop(trade_id, None)
                    else:
                        # Update remaining quantity and leg state on parent
                        cursor.execute(f'''
                            UPDATE paper_trades SET quantity = {ph}, tp_legs = {ph}
                            WHERE id = {ph}
                        ''', (current_qty, json.dumps(tp_legs), trade_id))
                    conn.commit()
                    continue  # Skip single-TP check below

            # Single-leg TP (backward compat or single-TP recorder)
            # ONLY check single tp_price when there's NO multi-leg config
            # Otherwise multi-leg handles all TP exits above ‚Äî falling through here would
            # re-close the entire position at the first leg's price (Bug: hundreds of $ error)
            if tp_price and not (tp_legs and len(tp_legs) > 1):
                if (side == 'LONG' and _bid >= tp_price + _tp_offset) or (side == 'SHORT' and _ask <= tp_price - _tp_offset):
                    _close_paper_trade_tpsl(trade_id, tp_price, 'tp', recorder_id, side, entry_price, quantity, symbol)
                    continue

            # Check SL hit ‚Äî use bid for LONG (selling at bid), ask for SHORT (buying at ask)
            if sl_price:
                if (side == 'LONG' and _bid <= sl_price) or (side == 'SHORT' and _ask >= sl_price):
                    _fill_price = _bid if side == 'LONG' else _ask
                    _close_paper_trade_tpsl(trade_id, _fill_price, 'sl', recorder_id, side, entry_price, quantity, symbol)
                    continue

        # Also check max daily loss after TP/SL checks
        check_paper_max_daily_loss()

    except Exception as e:
        print(f"‚ö†Ô∏è Error checking paper TP/SL: {e}", flush=True)
    finally:
        conn.close()


def _check_paper_dca_adds():
    """Automatically add DCA contracts to open paper trades when price moves against position.
    Mirrors the real broker's DCA behavior: when price moves against by avg_down_point ticks,
    add add_position_size contracts, recalculate avg entry and TP/SL."""
    import os
    from datetime import datetime

    global _paper_dca_next_price

    database_url = os.environ.get('DATABASE_URL')
    use_postgres = bool(database_url)

    if use_postgres:
        import psycopg2
        conn = psycopg2.connect(database_url)
        ph = '%s'
        enabled_check = "r.avg_down_enabled = TRUE"
    else:
        conn = sqlite3.connect('paper_trades.db')
        ph = '?'
        enabled_check = "r.avg_down_enabled = 1"

    cursor = conn.cursor()

    try:
        # Get all open paper trades for DCA-enabled recorders
        cursor.execute(f'''
            SELECT pt.id, pt.recorder_id, pt.symbol, pt.side, pt.quantity, pt.entry_price,
                   r.avg_down_point, r.avg_down_units, r.add_position_size, r.max_contracts_per_trade,
                   r.tp_targets, r.sl_enabled, r.sl_amount, r.sl_units, r.trim_units
            FROM paper_trades pt
            JOIN recorders r ON pt.recorder_id = r.id
            WHERE pt.status = 'open' AND {enabled_check}
        ''')
        dca_trades = cursor.fetchall()

        if not dca_trades:
            return

        # Clean up stale DCA tracking entries (closed trades)
        active_ids = {t[0] for t in dca_trades}
        stale_ids = [k for k in _paper_dca_next_price if k not in active_ids]
        for sid in stale_ids:
            del _paper_dca_next_price[sid]

        from recorder_service import get_tick_size

        for trade in dca_trades:
            trade_id, rec_id, sym, side, qty, entry, dca_point, dca_units, add_size, max_contracts, tp_targets_raw, sl_en, sl_amt, sl_units, trim_un = trade

            if not dca_point or float(dca_point) <= 0:
                continue

            live_price = _get_live_price_for_symbol(sym)
            if not live_price:
                continue

            tick_sz = get_tick_size(sym)

            # Calculate DCA distance in price terms
            dca_ticks = float(dca_point)
            if dca_units == 'Points':
                dca_distance = dca_ticks
            else:  # Ticks (default)
                dca_distance = dca_ticks * tick_sz

            # Initialize next DCA trigger if not tracked yet
            if trade_id not in _paper_dca_next_price:
                if side == 'LONG':
                    _paper_dca_next_price[trade_id] = entry - dca_distance
                else:
                    _paper_dca_next_price[trade_id] = entry + dca_distance

            next_trigger = _paper_dca_next_price[trade_id]

            # Check if price has hit the DCA trigger
            if side == 'LONG' and live_price > next_trigger:
                continue  # Price hasn't dropped to trigger
            if side == 'SHORT' and live_price < next_trigger:
                continue  # Price hasn't risen to trigger

            # Check max_contracts cap
            add_qty = int(add_size) if add_size else 1
            new_qty = qty + add_qty
            if max_contracts and int(max_contracts) > 0 and new_qty > int(max_contracts):
                continue  # Capped

            # DCA triggered ‚Äî add contracts and update avg entry
            new_avg = ((entry * qty) + (live_price * add_qty)) / new_qty

            # Recalculate TP from new avg entry (first leg + all legs)
            new_tp = None
            new_tp_legs = None
            if tp_targets_raw:
                import json
                try:
                    tp_list = json.loads(tp_targets_raw) if isinstance(tp_targets_raw, str) else tp_targets_raw
                    if tp_list and len(tp_list) > 0:
                        # First leg ‚Üí backward-compat tp_price
                        tp_ticks = float(tp_list[0].get('ticks') or tp_list[0].get('value') or 0)
                        if tp_ticks > 0:
                            tp_off = tp_ticks * tick_sz
                            new_tp = new_avg + tp_off if side == 'LONG' else new_avg - tp_off
                            new_tp = round(round(new_tp / tick_sz) * tick_sz, 10)
                        # All legs ‚Äî recalculate prices and trim quantities for new total qty
                        new_tp_legs = []
                        remaining = int(new_qty)
                        for li, leg in enumerate(tp_list):
                            leg_ticks = float(leg.get('ticks') or leg.get('value') or 0)
                            leg_trim = float(leg.get('trim', 0))
                            if leg_ticks <= 0:
                                continue
                            leg_off = leg_ticks * tick_sz
                            leg_price = new_avg + leg_off if side == 'LONG' else new_avg - leg_off
                            leg_price = round(round(leg_price / tick_sz) * tick_sz, 10)
                            if li == len(tp_list) - 1:
                                leg_qty = remaining
                            elif trim_un == 'Percent':
                                leg_qty = max(1, int(round(new_qty * (leg_trim / 100.0))))
                            else:
                                leg_qty = max(1, int(round(leg_trim))) if leg_trim > 0 else 1
                                leg_qty = min(leg_qty, remaining)
                            if leg_qty <= 0:
                                continue
                            remaining -= leg_qty
                            new_tp_legs.append({'ticks': leg_ticks, 'price': leg_price, 'qty': leg_qty, 'filled': False})
                            if remaining <= 0:
                                break
                except:
                    pass
            new_tp_legs_json = json.dumps(new_tp_legs) if new_tp_legs else None

            # Recalculate SL from new avg entry
            new_sl = None
            if sl_en and sl_amt and float(sl_amt) > 0:
                sl_ticks = float(sl_amt)
                sl_off = sl_ticks if sl_units == 'Points' else sl_ticks * tick_sz
                new_sl = new_avg - sl_off if side == 'LONG' else new_avg + sl_off
                new_sl = round(round(new_sl / tick_sz) * tick_sz, 10)

            # Update the trade in DB
            cursor.execute(f'''
                UPDATE paper_trades SET quantity = {ph}, entry_price = {ph}, tp_price = {ph}, sl_price = {ph}, tp_legs = {ph}
                WHERE id = {ph}
            ''', (new_qty, new_avg, new_tp, new_sl, new_tp_legs_json, trade_id))
            conn.commit()

            # Set next DCA trigger from THIS trigger level (not from new avg)
            # This ensures DCA levels are evenly spaced from original entry
            if side == 'LONG':
                _paper_dca_next_price[trade_id] = next_trigger - dca_distance
            else:
                _paper_dca_next_price[trade_id] = next_trigger + dca_distance

            tp_str = f"TP={new_tp:.2f}" if new_tp else "TP=None"
            sl_str = f"SL={new_sl:.2f}" if new_sl else "SL=None"
            print(f"üìà Paper DCA: +{add_qty} to {side} {sym} @ {live_price:.2f} | Qty: {int(qty)} -> {int(new_qty)} | Avg: ${new_avg:.2f} | {tp_str} | {sl_str}", flush=True)

    except Exception as e:
        print(f"‚ö†Ô∏è Error checking paper DCA: {e}", flush=True)
    finally:
        conn.close()


def start_paper_tpsl_monitor():
    """Start background thread to monitor paper trades for TP/SL hits"""
    global _paper_monitor_running
    if _paper_monitor_running:
        return

    import threading
    import time

    def monitor_loop():
        global _paper_monitor_running
        _paper_monitor_running = True
        dca_counter = 0
        print("üéØ Paper trading TP/SL + DCA monitor started (TP/SL every 500ms, DCA every 5s)", flush=True)

        while _paper_monitor_running:
            try:
                check_paper_trades_tpsl()
                # DCA check every 5 seconds (10 * 500ms) ‚Äî DCA is slower timescale
                dca_counter += 1
                if dca_counter >= 10:
                    _check_paper_dca_adds()
                    dca_counter = 0
            except Exception as e:
                print(f"‚ö†Ô∏è Paper monitor error: {e}", flush=True)
            time.sleep(0.5)  # Check every 500ms for responsive TP/SL

    thread = threading.Thread(target=monitor_loop, daemon=True)
    thread.start()


# ============================================================================
# 100+ USER SCALABILITY MODULE (Feature-flagged, additive)
# ============================================================================
# Enable with: export SCALABILITY_UI_PUBLISHER=1
# See /scalability/ folder for documentation
try:
    from scalability.integration import (
        init_scalability,
        get_scalability_status,
        register_scalability_routes,
        register_scalability_socketio_handlers,
    )
    from scalability import FEATURES as SCALABILITY_FEATURES
    SCALABILITY_MODULE_AVAILABLE = True
except ImportError as e:
    SCALABILITY_MODULE_AVAILABLE = False
    SCALABILITY_FEATURES = {}
    print(f"‚ÑπÔ∏è Scalability module not loaded (optional): {e}")

# ============================================================================
# DISCORD NOTIFICATION SERVICE
# ============================================================================
# Requires: DISCORD_BOT_TOKEN environment variable
# Bot must be in same server as users OR have DM permissions

DISCORD_BOT_TOKEN = os.environ.get('DISCORD_BOT_TOKEN', '')
DISCORD_NOTIFICATIONS_ENABLED = bool(DISCORD_BOT_TOKEN)

if DISCORD_NOTIFICATIONS_ENABLED:
    print("‚úÖ Discord notifications enabled")
else:
    print("‚ÑπÔ∏è Discord notifications disabled (set DISCORD_BOT_TOKEN to enable)")

## Discord notification functions have been extracted to discord_notifications.py
## They are imported at the top of this file and initialized after app creation.

def track_paper_trade(recorder_id: int, symbol: str, action: str, quantity: float = 1, price: float = None):
    """
    Track paper trades using TradingView real-time prices.
    Called after webhook signals to track theoretical P&L.

    Args:
        recorder_id: Recorder ID
        symbol: Trading symbol (e.g., 'NQ', 'ES')
        action: 'LONG', 'SHORT', 'BUY', 'SELL', 'CLOSE'
        quantity: Number of contracts
        price: Entry/exit price (uses market price if not provided)
    """
    if not TV_PRICE_SERVICE_AVAILABLE:
        return None

    try:
        paper_engine = get_paper_engine()
        action_upper = action.upper()

        # Normalize symbol to root (e.g., CME_MINI:NQ1!1! -> NQ, MGCJ2026 -> MGC)
        tv_symbol = extract_symbol_root(symbol) or symbol.upper()

        # Determine action type
        if action_upper in ['CLOSE', 'EXIT', 'FLATTEN']:
            # Close all positions for this recorder/symbol
            result = paper_engine.close_position(recorder_id, tv_symbol, price)
            if result:
                logger.info(f"üìä Paper trade CLOSED: Recorder {recorder_id} {symbol} P&L: ${result.get('pnl', 0):.2f}")
            return result

        elif action_upper in ['LONG', 'BUY']:
            # Check if we have opposite position to close first
            existing = paper_engine.get_position(recorder_id, tv_symbol)
            if existing and existing.get('side') == 'SHORT':
                paper_engine.close_position(recorder_id, tv_symbol, price)

            result = paper_engine.open_position(recorder_id, tv_symbol, 'LONG', quantity, price)
            if result:
                logger.info(f"üìä Paper trade OPENED: Recorder {recorder_id} LONG {quantity} {symbol} @ ${result.get('entry_price', 0):.2f}")
            return result

        elif action_upper in ['SHORT', 'SELL']:
            # Check if we have opposite position to close first
            existing = paper_engine.get_position(recorder_id, tv_symbol)
            if existing and existing.get('side') == 'LONG':
                paper_engine.close_position(recorder_id, tv_symbol, price)

            result = paper_engine.open_position(recorder_id, tv_symbol, 'SHORT', quantity, price)
            if result:
                logger.info(f"üìä Paper trade OPENED: Recorder {recorder_id} SHORT {quantity} {symbol} @ ${result.get('entry_price', 0):.2f}")
            return result

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Paper trade tracking error: {e}")

    return None


## notify_tp_sl_hit, notify_error, notify_daily_summary, broadcast_announcement
## have been extracted to discord_notifications.py

# ============================================================================
# PUSH NOTIFICATION SERVICE (Browser Push)
# ============================================================================
# Requires: VAPID_PUBLIC_KEY, VAPID_PRIVATE_KEY environment variables
# Generate with: from py_vapid import Vapid; v = Vapid(); v.generate_keys()

VAPID_PUBLIC_KEY = os.environ.get('VAPID_PUBLIC_KEY', '')
VAPID_PRIVATE_KEY = os.environ.get('VAPID_PRIVATE_KEY', '')
VAPID_CLAIMS_EMAIL = os.environ.get('VAPID_CLAIMS_EMAIL', 'mailto:admin@justtrades.com')

PUSH_NOTIFICATIONS_ENABLED = bool(VAPID_PUBLIC_KEY and VAPID_PRIVATE_KEY)

if PUSH_NOTIFICATIONS_ENABLED:
    print("‚úÖ Push notifications enabled")
else:
    print("‚ÑπÔ∏è Push notifications disabled (set VAPID_PUBLIC_KEY and VAPID_PRIVATE_KEY to enable)")


def send_push_notification(user_id: int, title: str, body: str, url: str = None, tag: str = None) -> int:
    """
    Send push notification to all subscribed devices for a user.
    
    Args:
        user_id: Database user ID
        title: Notification title
        body: Notification body text
        url: URL to open when clicked (optional)
        tag: Notification tag for grouping (optional)
    
    Returns:
        Number of notifications sent successfully
    """
    if not PUSH_NOTIFICATIONS_ENABLED:
        return 0
    
    try:
        from pywebpush import webpush, WebPushException
        import json
        
        # Get user's push subscriptions
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                SELECT subscription_json FROM push_subscriptions 
                WHERE user_id = %s AND active = TRUE
            ''', (user_id,))
        else:
            cursor.execute('''
                SELECT subscription_json FROM push_subscriptions 
                WHERE user_id = ? AND active = 1
            ''', (user_id,))
        
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        
        if not rows:
            return 0
        
        payload = json.dumps({
            'title': title,
            'body': body,
            'icon': '/static/img/just_trades_logo.png',
            'badge': '/static/img/just_trades_logo.png',
            'tag': tag or 'just-trades',
            'data': {'url': url or '/dashboard'}
        })
        
        sent_count = 0
        for row in rows:
            try:
                subscription_json = row[0] if isinstance(row, tuple) else row.get('subscription_json')
                subscription_info = json.loads(subscription_json) if isinstance(subscription_json, str) else subscription_json
                
                webpush(
                    subscription_info=subscription_info,
                    data=payload,
                    vapid_private_key=VAPID_PRIVATE_KEY,
                    vapid_claims={'sub': VAPID_CLAIMS_EMAIL}
                )
                sent_count += 1
            except WebPushException as e:
                logger.warning(f"‚ö†Ô∏è Push notification failed: {e}")
                # If subscription is invalid, mark it as inactive
                if e.response and e.response.status_code in [404, 410]:
                    try:
                        mark_push_subscription_inactive(subscription_json)
                    except:
                        pass
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Push notification error: {e}")
        
        return sent_count
    except ImportError:
        logger.warning("‚ö†Ô∏è pywebpush not installed - push notifications disabled")
        return 0
    except Exception as e:
        logger.error(f"‚ùå Push notification error: {e}")
        return 0


def mark_push_subscription_inactive(subscription_json: str):
    """Mark a push subscription as inactive (expired/unsubscribed)."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('UPDATE push_subscriptions SET active = FALSE WHERE subscription_json = %s', (subscription_json,))
        else:
            cursor.execute('UPDATE push_subscriptions SET active = 0 WHERE subscription_json = ?', (subscription_json,))
        
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error marking subscription inactive: {e}")


def get_push_enabled_users(user_id: int = None) -> list:
    """Get users who have push notifications enabled."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if user_id:
            if is_using_postgres():
                cursor.execute('''
                    SELECT DISTINCT user_id FROM push_subscriptions 
                    WHERE user_id = %s AND active = TRUE
                ''', (user_id,))
            else:
                cursor.execute('''
                    SELECT DISTINCT user_id FROM push_subscriptions 
                    WHERE user_id = ? AND active = 1
                ''', (user_id,))
        else:
            if is_using_postgres():
                cursor.execute('SELECT DISTINCT user_id FROM push_subscriptions WHERE active = TRUE')
            else:
                cursor.execute('SELECT DISTINCT user_id FROM push_subscriptions WHERE active = 1')
        
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        
        return [row[0] for row in rows]
    except Exception as e:
        logger.error(f"Error getting push users: {e}")
        return []


def notify_via_push(user_id: int, title: str, body: str, url: str = None):
    """
    Send push notification to a user (convenience wrapper).
    Also sends Discord DM if enabled.
    """
    sent = 0
    if PUSH_NOTIFICATIONS_ENABLED:
        sent = send_push_notification(user_id, title, body, url)
    return sent


# High-performance signal queue for handling hundreds of signals per second
signal_queue = Queue(maxsize=10000)
BATCH_SIZE = 50  # Process signals in batches for faster DB writes
BATCH_TIMEOUT = 0.1  # Max wait time before processing partial batch (seconds)

# ============================================================================
# FAST WEBHOOK QUEUE - Accept webhooks INSTANTLY, process in PARALLEL
# ============================================================================
# CRITICAL: TradingView webhooks timeout after 5-10 seconds. If we do too much
# work in the webhook handler, signals get dropped. This queue accepts webhooks
# immediately (under 50ms) and processes them in parallel background workers.
_fast_webhook_queue = Queue(maxsize=10000)
_fast_webhook_enabled = True  # Fixed: workers now pass body directly, no Flask context needed
_fast_webhook_worker_count = 100  # Number of parallel workers for instant processing

def fast_webhook_worker(worker_id):
    """Background worker that processes raw webhooks from the fast queue.

    IMPORTANT: Does NOT use Flask's test_request_context (it was causing silent failures).
    Instead, calls process_webhook_with_data() directly with the raw body string.
    """
    logger.info(f"üöÄ Fast webhook worker #{worker_id} started")

    while True:
        try:
            # Get next webhook from queue (blocking with timeout)
            webhook_data = _fast_webhook_queue.get(timeout=1)

            token = webhook_data.get('token')
            body = webhook_data.get('body')
            received_at = webhook_data.get('received_at')
            signal_id = webhook_data.get('signal_id', f"sig_unknown_{uuid.uuid4().hex[:8]}")
            delay = time.time() - received_at

            # STEP 4: Worker picked up from queue
            track_signal_step(signal_id, 'STEP4_WORKER_PICKED', {
                'worker_id': worker_id,
                'delay_seconds': round(delay, 3),
                'queue_remaining': _fast_webhook_queue.qsize()
            })

            # Log that we're processing
            logger.info(f"‚ö° Worker #{worker_id} processing webhook: token={token[:8]}... signal={signal_id} (delay: {delay:.3f}s)")

            # Process the webhook with Flask app context (needed for jsonify, db, etc.)
            try:
                with app.app_context():
                    result = process_webhook_with_data(token, body, signal_id)
                    if result.get('success'):
                        # Check if broker execution was queued - if so, let broker worker complete the signal
                        if result.get('broker_queued'):
                            logger.info(f"‚úÖ Worker #{worker_id} queued webhook for broker: token={token[:8]}... signal={signal_id}")
                            # DON'T call complete_signal here - broker worker will do it after execution
                        else:
                            # No broker execution needed (e.g., close signals handled inline)
                            logger.info(f"‚úÖ Worker #{worker_id} completed webhook: token={token[:8]}... signal={signal_id}")
                            complete_signal(signal_id, 'complete')
                    else:
                        error_msg = result.get('error', 'unknown error')
                        logger.warning(f"‚ö†Ô∏è Worker #{worker_id} webhook returned: {error_msg}")
                        complete_signal(signal_id, 'failed', error_msg)
            except Exception as proc_err:
                logger.error(f"‚ùå Worker #{worker_id} error processing webhook: {proc_err}")
                import traceback
                logger.error(traceback.format_exc())
                complete_signal(signal_id, 'failed', str(proc_err))

            _fast_webhook_queue.task_done()

        except Empty:
            continue  # Queue empty, keep waiting
        except Exception as e:
            logger.error(f"Fast webhook worker #{worker_id} error: {e}")
            import traceback
            logger.error(traceback.format_exc())

# Worker threads will be started AFTER app is created (see start_fast_webhook_workers())
_fast_webhook_threads = []

def start_fast_webhook_workers():
    """Start the fast webhook worker threads. Must be called AFTER app is created."""
    global _fast_webhook_threads
    if _fast_webhook_threads:
        logger.info("Fast webhook workers already started")
        return

    for i in range(_fast_webhook_worker_count):
        t = threading.Thread(target=fast_webhook_worker, args=(i,), daemon=True, name=f"FastWebhookWorker-{i}")
        t.start()
        _fast_webhook_threads.append(t)
    logger.info(f"üöÄ Started {_fast_webhook_worker_count} parallel webhook workers for instant execution")

# ============================================================================
# EXTERNAL TRADING ENGINE MODE
# ============================================================================
# When EXTERNAL_TRADING_ENGINE=1, broker workers and monitoring threads
# run in a separate process (trading_engine.py). The web server only handles
# HTTP requests and pushes broker tasks to Redis.
_EXTERNAL_ENGINE = os.environ.get('EXTERNAL_TRADING_ENGINE') == '1'

# ============================================================================
# BROKER EXECUTION QUEUE - Trade Manager Style (Async, Non-Blocking)
# ============================================================================
# Webhook handler queues broker execution here, returns immediately.
# Background worker processes queue with retries.
# This ensures webhooks NEVER fail due to broker issues.

class RedisQueue:
    """Drop-in replacement for Python Queue backed by Redis.
    Used when EXTERNAL_TRADING_ENGINE is set to communicate between
    the web server process and the trading engine process."""

    def __init__(self, redis_url, key, maxsize=5000):
        import redis as _redis
        self._redis = _redis.from_url(redis_url, decode_responses=True)
        self._key = key
        self._maxsize = maxsize

    def put_nowait(self, item):
        if self.qsize() >= self._maxsize:
            raise Full()
        self._redis.rpush(self._key, json.dumps(item, default=str))

    def put(self, item, timeout=None):
        self.put_nowait(item)

    def get(self, timeout=1):
        result = self._redis.blpop(self._key, timeout=int(timeout))
        if result:
            return json.loads(result[1])
        raise Empty()

    def qsize(self):
        return self._redis.llen(self._key)

    def task_done(self):
        pass  # Redis doesn't need task_done tracking

    @property
    def maxsize(self):
        return self._maxsize

# Toggle: Use Redis queue when external trading engine is running, else in-memory
if _EXTERNAL_ENGINE:
    _redis_url = os.environ.get('REDIS_URL')
    if _redis_url:
        broker_execution_queue = RedisQueue(_redis_url, 'broker_tasks', maxsize=5000)
        logger.info("üîó Broker queue: Redis (external trading engine mode)")
    else:
        logger.error("‚ùå EXTERNAL_TRADING_ENGINE=1 but REDIS_URL not set! Falling back to in-memory queue.")
        broker_execution_queue = Queue(maxsize=5000)
else:
    broker_execution_queue = Queue(maxsize=5000)
    logger.info("üîó Broker queue: In-memory (single process mode)")
_broker_execution_worker_count = 100  # Number of parallel workers for HIVE MIND instant execution

# ============================================================================
# WEBHOOK HEALTH TRACKING - Detect stuck webhook processing
# ============================================================================
_webhook_last_received = 0  # Timestamp of last webhook received
_webhook_last_processed = 0  # Timestamp of last webhook successfully processed
_webhook_processing_count = 0  # Total webhooks processed since startup
_webhook_error_count = 0  # Total webhook errors since startup
_webhook_stuck_threshold = 300  # 5 minutes - if webhook received but not processed, it's stuck

# WEBHOOK ACTIVITY LOG - Track recent webhook processing for debugging
# ============================================================================
_webhook_activity_log = []  # List of recent webhook activities
_webhook_activity_max_size = 300  # Keep last 300 webhook activities (increased from 100)

# ============================================================================
# SIGNAL PIPELINE TRACKER - Track EVERY step for debugging dropped signals
# ============================================================================
# This tracks each signal through the entire pipeline so we can see exactly
# where signals get lost: TradingView ‚Üí Endpoint ‚Üí Queue ‚Üí Worker ‚Üí Broker
import uuid
import threading

_signal_pipeline = {}  # Key: signal_id -> {steps: [...], status: 'pending'|'complete'|'failed'}
_signal_pipeline_lock = threading.Lock()
_signal_pipeline_max_size = 500

def track_signal_step(signal_id: str, step: str, details: dict = None):
    """Track a signal's progress through the pipeline."""
    from datetime import datetime
    with _signal_pipeline_lock:
        if signal_id not in _signal_pipeline:
            _signal_pipeline[signal_id] = {
                'created': datetime.now().isoformat(),
                'steps': [],
                'status': 'pending',
                'last_update': datetime.now().isoformat()
            }
        _signal_pipeline[signal_id]['steps'].append({
            'step': step,
            'timestamp': datetime.now().isoformat(),
            'details': details or {}
        })
        _signal_pipeline[signal_id]['last_update'] = datetime.now().isoformat()

        # Cleanup old entries
        if len(_signal_pipeline) > _signal_pipeline_max_size:
            # Remove oldest entries
            sorted_ids = sorted(_signal_pipeline.keys(),
                key=lambda x: _signal_pipeline[x]['created'])
            for old_id in sorted_ids[:100]:
                del _signal_pipeline[old_id]

def complete_signal(signal_id: str, status: str = 'complete', error: str = None):
    """Mark a signal as complete or failed."""
    with _signal_pipeline_lock:
        if signal_id in _signal_pipeline:
            _signal_pipeline[signal_id]['status'] = status
            if error:
                _signal_pipeline[signal_id]['error'] = error

def get_signal_pipeline(limit: int = 50):
    """Get recent signals and their pipeline status."""
    with _signal_pipeline_lock:
        # Sort by creation time, newest first
        sorted_signals = sorted(
            _signal_pipeline.items(),
            key=lambda x: x[1]['created'],
            reverse=True
        )[:limit]
        return {sid: data for sid, data in sorted_signals}

def get_pending_signals():
    """Get signals that are stuck (pending for more than 30 seconds)."""
    from datetime import datetime, timedelta
    cutoff = (datetime.now() - timedelta(seconds=30)).isoformat()
    with _signal_pipeline_lock:
        pending = {}
        for sid, data in _signal_pipeline.items():
            if data['status'] == 'pending' and data['last_update'] < cutoff:
                pending[sid] = data
        return pending

# RAW WEBHOOK LOG - Track ALL incoming webhooks before any processing
_raw_webhook_log = []  # List of ALL raw webhook data
_raw_webhook_max_size = 500  # Keep last 500 raw webhooks (increased from 200)

def log_raw_webhook(token, body):
    """Log raw webhook data BEFORE any processing - helps debug filtering issues"""
    global _raw_webhook_log
    from datetime import datetime
    entry = {
        'timestamp': datetime.now().isoformat(),
        'token_prefix': token[:8] if token else 'N/A',
        'body_preview': body[:300] if body else 'empty',
        'body_length': len(body) if body else 0
    }
    _raw_webhook_log.append(entry)
    if len(_raw_webhook_log) > _raw_webhook_max_size:
        _raw_webhook_log = _raw_webhook_log[-_raw_webhook_max_size:]

def get_raw_webhook_log(limit=50):
    """Get recent raw webhook log"""
    return list(reversed(_raw_webhook_log[-limit:]))

def log_webhook_activity(recorder_name, action, symbol, status, error=None, broker_result=None, trade_id=None):
    """Log webhook activity for monitoring"""
    global _webhook_activity_log
    from datetime import datetime
    activity = {
        'timestamp': datetime.now().isoformat(),
        'recorder': recorder_name,
        'action': action,
        'symbol': symbol,
        'status': status,  # 'received', 'processing', 'success', 'failed', 'blocked'
        'error': error,
        'broker_result': broker_result,
        'trade_id': trade_id
    }
    _webhook_activity_log.append(activity)
    # Keep only last N entries
    if len(_webhook_activity_log) > _webhook_activity_max_size:
        _webhook_activity_log = _webhook_activity_log[-_webhook_activity_max_size:]

def get_webhook_activity_log(limit=50):
    """Get recent webhook activity log"""
    return list(reversed(_webhook_activity_log[-limit:]))

# WEBHOOK DEDUPLICATION - Prevent duplicate webhook processing
# ============================================================================
_webhook_dedup_cache = {}  # Key: "token:action:body_hash" -> timestamp
_webhook_dedup_window = 1  # Seconds - reject duplicate webhooks within this window (TV duplicates arrive within ms)
_webhook_dedup_max_size = 1000  # Max cache entries before cleanup

# ============================================================================
# üöÄ BROKER API QUEUE SYSTEM - Bulk requests, rate limit protection
# ============================================================================
# This system ensures we:
# 1. Process broker API calls ONE at a time (serial, not parallel)
# 2. Cache results to avoid duplicate calls
# 3. Batch requests for multiple accounts into fewer calls
# 4. Stay under rate limits even with many users

class BrokerAPIQueue:
    """
    Centralized queue for all broker API calls.
    Processes requests serially to avoid rate limiting.
    Caches results to reduce duplicate calls.
    """
    
    def __init__(self, cache_ttl: int = 10):
        self._queue = Queue()
        self._cache = {}  # {cache_key: {'data': ..., 'expires': timestamp}}
        self._cache_ttl = cache_ttl  # Cache time-to-live in seconds
        self._cache_lock = threading.Lock()
        self._processing = False
        self._last_call_time = 0
        self._min_call_interval = 0.5  # Minimum seconds between API calls
        self._rate_limit_until = 0  # Timestamp until which we're rate limited
        self._stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'api_calls': 0,
            'rate_limits': 0
        }
        
    def get_cached(self, cache_key: str):
        """Get cached data if not expired."""
        with self._cache_lock:
            if cache_key in self._cache:
                entry = self._cache[cache_key]
                if time.time() < entry['expires']:
                    self._stats['cache_hits'] += 1
                    return entry['data']
                else:
                    del self._cache[cache_key]
        return None
    
    def set_cached(self, cache_key: str, data, ttl: int = None):
        """Cache data with TTL."""
        with self._cache_lock:
            self._cache[cache_key] = {
                'data': data,
                'expires': time.time() + (ttl or self._cache_ttl)
            }
    
    def clear_cache(self, pattern: str = None):
        """Clear cache entries matching pattern (or all if None)."""
        with self._cache_lock:
            if pattern:
                keys_to_delete = [k for k in self._cache if pattern in k]
                for k in keys_to_delete:
                    del self._cache[k]
            else:
                self._cache.clear()
    
    def is_rate_limited(self) -> bool:
        """Check if we're in a rate limit cooldown."""
        return time.time() < self._rate_limit_until
    
    def set_rate_limited(self, cooldown_seconds: int = 60):
        """Set rate limit cooldown."""
        self._rate_limit_until = time.time() + cooldown_seconds
        self._stats['rate_limits'] += 1
    
    def wait_for_rate_limit(self):
        """Wait until rate limit clears, with respect to min interval."""
        # Wait for rate limit cooldown
        if self.is_rate_limited():
            wait_time = self._rate_limit_until - time.time()
            if wait_time > 0:
                time.sleep(wait_time)
        
        # Also respect minimum call interval
        elapsed = time.time() - self._last_call_time
        if elapsed < self._min_call_interval:
            time.sleep(self._min_call_interval - elapsed)
        
        self._last_call_time = time.time()
    
    def get_stats(self) -> dict:
        """Get queue statistics."""
        return {
            **self._stats,
            'cache_size': len(self._cache),
            'cache_hit_rate': (
                round(self._stats['cache_hits'] / max(1, self._stats['total_requests']) * 100, 1)
            )
        }

# Global broker API queue instance
broker_api_queue = BrokerAPIQueue(cache_ttl=10)

def bulk_fetch_account_data(account_ids: list = None) -> dict:
    """
    Fetch PnL and position data for ALL accounts in ONE batch.
    This is the key function for reducing API calls.
    
    Returns: {
        'accounts': {account_id: {pnl_data}},
        'positions': [position_list],
        'timestamp': fetch_time
    }
    """
    import requests
    
    cache_key = f"bulk_accounts_{','.join(map(str, sorted(account_ids or [])))}"
    
    # Check cache first
    cached = broker_api_queue.get_cached(cache_key)
    if cached:
        return cached
    
    broker_api_queue._stats['total_requests'] += 1
    
    # Wait for rate limit
    broker_api_queue.wait_for_rate_limit()
    
    result = {
        'accounts': {},
        'positions': [],
        'timestamp': time.time(),
        'errors': []
    }
    
    try:
        # Get database connection to fetch account tokens
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        if account_ids:
            placeholders = ','.join('?' * len(account_ids))
            cursor.execute(f'''
                SELECT id, name, tradovate_token, tradovate_accounts, environment, user_id
                FROM accounts 
                WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
                AND id IN ({placeholders})
            ''', account_ids)
        else:
            cursor.execute('''
                SELECT id, name, tradovate_token, tradovate_accounts, environment, user_id
                FROM accounts 
                WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
            ''')
        
        all_accounts = cursor.fetchall()
        conn.close()
        
        if not all_accounts:
            broker_api_queue.set_cached(cache_key, result)
            return result
        
        # Process each account (but rate-limit between them)
        for account in all_accounts:
            account_id = account['id']
            
            # Get valid token
            token = get_valid_tradovate_token(account_id)
            if not token:
                result['errors'].append(f"No valid token for account {account_id}")
                continue
            
            env = account['environment'] or 'demo'
            base_url = 'https://demo.tradovateapi.com/v1' if env == 'demo' else 'https://live.tradovateapi.com/v1'
            
            headers = {
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json'
            }
            
            # Parse subaccounts
            tradovate_accounts = []
            try:
                if account['tradovate_accounts']:
                    tradovate_accounts = json.loads(account['tradovate_accounts'])
            except:
                pass
            
            account_pnl = {
                'name': account['name'],
                'user_id': account['user_id'],
                'subaccounts': {}
            }
            
            # Fetch data for all subaccounts under this account
            for ta in tradovate_accounts:
                sub_id = ta.get('id')
                sub_name = ta.get('name', str(sub_id))
                # CRITICAL FIX: Use environment as source of truth for demo vs live
                env = (ta.get('environment') or 'demo').lower()
                is_demo = env != 'live'
                
                sub_base_url = 'https://demo.tradovateapi.com/v1' if is_demo else 'https://live.tradovateapi.com/v1'
                
                try:
                    # Brief pause between subaccount calls
                    time.sleep(0.1)
                    
                    # Fetch cash balance
                    cash_resp = requests.get(
                        f"{sub_base_url}/cashBalance/getCashBalanceSnapshot",
                        params={'accountId': sub_id},
                        headers=headers,
                        timeout=10
                    )
                    
                    if cash_resp.status_code == 429:
                        broker_api_queue.set_rate_limited(60)
                        result['errors'].append(f"Rate limited on account {sub_id}")
                        continue
                    
                    broker_api_queue._stats['api_calls'] += 1
                    
                    cash_data = cash_resp.json() if cash_resp.status_code == 200 else {}
                    
                    # Fetch positions
                    time.sleep(0.1)
                    pos_resp = requests.get(
                        f"{sub_base_url}/position/list",
                        headers=headers,
                        timeout=10
                    )
                    
                    broker_api_queue._stats['api_calls'] += 1
                    
                    positions = pos_resp.json() if pos_resp.status_code == 200 else []
                    
                    # Store subaccount data
                    account_pnl['subaccounts'][sub_id] = {
                        'name': sub_name,
                        'is_demo': is_demo,
                        'cash_balance': cash_data,
                        'realized_pnl': cash_data.get('realizedPnL', 0),
                        'unrealized_pnl': cash_data.get('openPnL', 0),
                        'total_pnl': cash_data.get('realizedPnL', 0) + cash_data.get('openPnL', 0)
                    }
                    
                    # Add positions to global list
                    for pos in positions:
                        if pos.get('netPos', 0) != 0:
                            result['positions'].append({
                                'account_id': account_id,
                                'subaccount_id': sub_id,
                                'subaccount_name': sub_name,
                                **pos
                            })
                    
                except requests.exceptions.Timeout:
                    result['errors'].append(f"Timeout on subaccount {sub_id}")
                except Exception as e:
                    result['errors'].append(f"Error on subaccount {sub_id}: {str(e)}")
            
            result['accounts'][account_id] = account_pnl
        
        # Cache the result
        broker_api_queue.set_cached(cache_key, result, ttl=10)
        
    except Exception as e:
        result['errors'].append(f"Bulk fetch error: {str(e)}")
    
    return result

def get_cached_account_pnl(account_id: int = None) -> dict:
    """
    Get account PnL from cache or trigger bulk fetch.
    This is the function other parts of the code should call.
    """
    # First, try to get from bulk cache
    bulk_data = broker_api_queue.get_cached("bulk_accounts_")
    
    if not bulk_data:
        # No cache, do a bulk fetch
        bulk_data = bulk_fetch_account_data()
    
    if account_id:
        return bulk_data.get('accounts', {}).get(account_id, {})
    
    return bulk_data

def get_cached_positions() -> list:
    """Get all positions from cache or trigger bulk fetch."""
    bulk_data = broker_api_queue.get_cached("bulk_accounts_")
    
    if not bulk_data:
        bulk_data = bulk_fetch_account_data()
    
    return bulk_data.get('positions', [])

# ============================================================================
# üõ°Ô∏è BULLETPROOF AUTH TRACKING - Track accounts that need OAuth re-authentication
# ============================================================================
_ACCOUNTS_NEED_REAUTH = set()  # Set of account IDs that need manual OAuth re-auth
_ACCOUNTS_NEED_REAUTH_LOCK = threading.Lock()

def mark_account_needs_reauth(account_id: int):
    """Mark an account as needing OAuth re-authentication."""
    with _ACCOUNTS_NEED_REAUTH_LOCK:
        _ACCOUNTS_NEED_REAUTH.add(account_id)

def clear_account_reauth(account_id: int):
    """Clear the re-auth flag for an account (after successful OAuth)."""
    with _ACCOUNTS_NEED_REAUTH_LOCK:
        _ACCOUNTS_NEED_REAUTH.discard(account_id)

def get_accounts_needing_reauth():
    """Get list of account IDs that need OAuth re-authentication."""
    with _ACCOUNTS_NEED_REAUTH_LOCK:
        return list(_ACCOUNTS_NEED_REAUTH)

def is_account_auth_valid(account_id: int) -> bool:
    """Check if an account's authentication is valid."""
    with _ACCOUNTS_NEED_REAUTH_LOCK:
        return account_id not in _ACCOUNTS_NEED_REAUTH

# WebSocket support for Tradovate market data
try:
    import websockets
    WEBSOCKETS_AVAILABLE = True
except ImportError:
    WEBSOCKETS_AVAILABLE = False
    # Logger not defined yet, will log later


# Load environment variables from .env file if it exists
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenv not installed, skip (optional dependency)
    pass

# ============================================================
# DATABASE CONNECTION - Supports both SQLite and PostgreSQL
# CONNECTION POOLING for fast parallel webhook processing
# ============================================================
DATABASE_URL = os.getenv('DATABASE_URL') or os.getenv('DATABASE_PRIVATE_URL') or os.getenv('DATABASE_PUBLIC_URL')
_using_postgres = False
_tables_initialized = False
_db_url = None
_pg_pool = None  # ThreadedConnectionPool for PostgreSQL

def is_using_postgres():
    """Check if we're actually using PostgreSQL"""
    return _using_postgres

def _init_db_once():
    """Initialize database once on startup with connection pool."""
    global _using_postgres, _tables_initialized, _db_url, _pg_pool

    if _tables_initialized:
        return

    if DATABASE_URL and DATABASE_URL.startswith('postgres'):
        try:
            import psycopg2
            import psycopg2.pool
            _db_url = DATABASE_URL.replace('postgres://', 'postgresql://', 1)

            # Test connection and create tables (with timeout to prevent hanging)
            conn = psycopg2.connect(_db_url, connect_timeout=10)
            conn.close()
            _using_postgres = True
            _init_postgres_tables()

            # Create connection pool for fast parallel processing
            # min=20 connections ready, max=200 for 5000+ user burst traffic
            try:
                _pg_pool = psycopg2.pool.ThreadedConnectionPool(
                    minconn=20,
                    maxconn=200,
                    dsn=_db_url,
                    connect_timeout=5
                )
                print("‚úÖ PostgreSQL connection pool created (20-100 connections)")
            except Exception as pool_err:
                print(f"‚ö†Ô∏è Pool creation failed, will use direct connections: {pool_err}")
                _pg_pool = None

            _tables_initialized = True
            print("‚úÖ PostgreSQL connected and tables initialized")
            return
        except Exception as e:
            print(f"‚ö†Ô∏è PostgreSQL init failed: {e}")
            _using_postgres = False

    # SQLite fallback
    _using_postgres = False
    conn = sqlite3.connect('just_trades.db', timeout=30)
    _init_sqlite_tables(conn)
    conn.close()
    _tables_initialized = True
    print("‚úÖ SQLite initialized")

def get_db_connection():
    """Get database connection from pool (fast) or create fresh (fallback)."""
    global _using_postgres, _db_url, _pg_pool

    # Initialize on first call
    _init_db_once()

    # PostgreSQL - use pool for speed, fallback to fresh connection
    if _using_postgres and _db_url:
        import psycopg2
        from psycopg2.extras import RealDictCursor

        # Try pool first (instant connection)
        if _pg_pool:
            try:
                conn = _pg_pool.getconn()
                conn.cursor_factory = RealDictCursor
                # Clear any aborted transaction state (no round-trip query needed)
                try:
                    conn.rollback()
                except Exception:
                    # Connection is truly dead ‚Äî discard it and get a fresh one
                    try:
                        _pg_pool.putconn(conn, close=True)
                    except:
                        pass
                    conn = psycopg2.connect(_db_url, connect_timeout=5)
                    conn.cursor_factory = RealDictCursor
                return PostgresConnectionWrapper(conn, _pg_pool)  # Returns to pool on close
            except Exception as pool_err:
                print(f"‚ö†Ô∏è Pool getconn failed, creating fresh connection: {pool_err}")

        # Fallback: fresh connection with retry logic
        max_retries = 3
        retry_delay = 0.5

        for attempt in range(max_retries):
            try:
                timeout = 10 if attempt == 0 else 5
                conn = psycopg2.connect(_db_url, connect_timeout=timeout)
                conn.cursor_factory = RealDictCursor
                try:
                    conn.rollback()
                except:
                    pass
                return PostgresConnectionWrapper(conn, None)  # No pool
            except Exception as e:
                if attempt < max_retries - 1:
                    import time
                    print(f"‚ö†Ô∏è PostgreSQL attempt {attempt + 1}/{max_retries} failed: {e}")
                    time.sleep(retry_delay)
                    retry_delay *= 2

        print(f"‚ùå PostgreSQL connection FAILED after {max_retries} attempts")
        raise Exception(f"Database connection failed after {max_retries} attempts")

    # SQLite fallback (only for local development when DATABASE_URL is not set)
    conn = sqlite3.connect('just_trades.db', timeout=30)
    conn.row_factory = sqlite3.Row
    return conn

class DictRow:
    """Row that supports both dict-style and index-style access."""
    def __init__(self, data, keys):
        self._data = data
        self._keys = keys
    
    def __getitem__(self, key):
        if isinstance(key, int):
            return self._data[self._keys[key]]
        return self._data[key]
    
    def __contains__(self, key):
        return key in self._data
    
    def get(self, key, default=None):
        return self._data.get(key, default)
    
    def keys(self):
        return self._data.keys()
    
    def values(self):
        return self._data.values()
    
    def items(self):
        return self._data.items()
    
    def __repr__(self):
        return repr(self._data)

class PostgresCursorWrapper:
    """Wrapper to make PostgreSQL cursor auto-convert SQLite ? to PostgreSQL %s."""
    def __init__(self, cursor):
        self._cursor = cursor
        self._keys = None
    
    def execute(self, sql, params=None):
        # Convert SQLite ? placeholders to PostgreSQL %s
        sql = sql.replace('?', '%s')
        if params:
            self._cursor.execute(sql, params)
        else:
            self._cursor.execute(sql)
        # Cache column names
        if self._cursor.description:
            self._keys = [desc[0] for desc in self._cursor.description]
        return self
    
    def _wrap_row(self, row):
        """Wrap a dict row to support both dict and index access."""
        if row is None:
            return None
        if isinstance(row, dict) and self._keys:
            return DictRow(row, self._keys)
        return row
    
    def fetchone(self):
        row = self._cursor.fetchone()
        return self._wrap_row(row)
    
    def fetchall(self):
        rows = self._cursor.fetchall()
        if rows and isinstance(rows[0], dict) and self._keys:
            return [DictRow(r, self._keys) for r in rows]
        return rows
    
    def fetchmany(self, size=None):
        rows = self._cursor.fetchmany(size) if size else self._cursor.fetchmany()
        if rows and isinstance(rows[0], dict) and self._keys:
            return [DictRow(r, self._keys) for r in rows]
        return rows
    
    @property
    def lastrowid(self):
        return None  # PostgreSQL uses RETURNING instead
    
    @property
    def rowcount(self):
        return self._cursor.rowcount
    
    @property
    def description(self):
        return self._cursor.description
    
    def close(self):
        self._cursor.close()
    
    def __iter__(self):
        for row in self._cursor:
            yield self._wrap_row(row)

class PostgresConnectionWrapper:
    """Wrapper to make PostgreSQL connection behave like SQLite."""
    def __init__(self, conn, pool):
        self._conn = conn
        self._pool = pool
        self._row_factory = None
        self._closed = False

    @property
    def row_factory(self):
        return self._row_factory

    @row_factory.setter
    def row_factory(self, value):
        self._row_factory = value

    def cursor(self):
        return PostgresCursorWrapper(self._conn.cursor())

    def execute(self, sql, params=None):
        sql = sql.replace('?', '%s')
        cursor = self._conn.cursor()
        if params:
            cursor.execute(sql, params)
        else:
            cursor.execute(sql)
        return PostgresCursorWrapper(cursor)

    def commit(self):
        self._conn.commit()

    def rollback(self):
        self._conn.rollback()

    def close(self):
        """Return connection to pool (or close direct connection)."""
        if self._closed:
            return
        self._closed = True
        try:
            # Always rollback before closing to clear any partial transaction
            try:
                self._conn.rollback()
            except:
                pass
            if self._pool:
                self._pool.putconn(self._conn)
            else:
                self._conn.close()
        except Exception as e:
            print(f"‚ö†Ô∏è Error closing connection: {e}")

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()
    
    def __del__(self):
        """Ensure connection is returned even if close() wasn't called."""
        if not self._closed:
            self.close()

def _init_sqlite_tables(conn):
    """Initialize ALL SQLite tables - mirrors PostgreSQL schema."""
    cursor = conn.cursor()
    
    print("üìä Creating SQLite tables (complete schema)...")
    
    # Users table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT NOT NULL UNIQUE,
            email TEXT NOT NULL UNIQUE,
            password_hash TEXT NOT NULL,
            discord_user_id TEXT UNIQUE,
            discord_access_token TEXT,
            discord_dms_enabled INTEGER DEFAULT 0,
            session_id TEXT,
            last_login TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
            display_name TEXT,
            is_admin INTEGER DEFAULT 0,
            is_active INTEGER DEFAULT 1,
            settings_json TEXT DEFAULT '{}'
        )
    ''')
    
    # Announcements/Bulletins table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS announcements (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            message TEXT NOT NULL,
            type TEXT DEFAULT 'info',
            is_active INTEGER DEFAULT 1,
            created_by INTEGER REFERENCES users(id),
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            expires_at TEXT,
            priority INTEGER DEFAULT 0
        )
    ''')
    
    # Accounts table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS accounts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER REFERENCES users(id),
            name TEXT NOT NULL,
            broker TEXT DEFAULT 'Tradovate',
            auth_type TEXT DEFAULT 'oauth',
            username TEXT,
            password TEXT,
            account_id TEXT,
            api_key TEXT,
            api_secret TEXT,
            api_endpoint TEXT,
            environment TEXT DEFAULT 'demo',
            client_id TEXT,
            client_secret TEXT,
            tradovate_token TEXT,
            tradovate_refresh_token TEXT,
            token_expires_at TEXT,
            max_contracts INTEGER,
            multiplier REAL DEFAULT 1.0,
            enabled INTEGER DEFAULT 1,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
            subaccounts TEXT,
            tradovate_accounts TEXT,
            md_access_token TEXT,
            tradingview_session TEXT,
            has_tradingview_addon INTEGER DEFAULT 0,
            device_id TEXT
        )
    ''')
    
    # Recorders table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorders (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER REFERENCES users(id),
            name TEXT NOT NULL,
            strategy_type TEXT DEFAULT 'Futures',
            symbol TEXT,
            demo_account_id TEXT,
            account_id INTEGER,
            is_private INTEGER DEFAULT 0,
            initial_position_size INTEGER DEFAULT 2,
            add_position_size INTEGER DEFAULT 2,
            tp_units TEXT DEFAULT 'Ticks',
            trim_units TEXT DEFAULT 'Contracts',
            tp_targets TEXT DEFAULT '[]',
            sl_enabled INTEGER DEFAULT 0,
            sl_amount REAL DEFAULT 0,
            sl_units TEXT DEFAULT 'Ticks',
            sl_type TEXT DEFAULT 'Fixed',
            break_even_enabled INTEGER DEFAULT 0,
            break_even_ticks INTEGER DEFAULT 10,
            avg_down_enabled INTEGER DEFAULT 0,
            avg_down_amount INTEGER DEFAULT 1,
            avg_down_point REAL DEFAULT 0,
            avg_down_units TEXT DEFAULT 'Ticks',
            add_delay INTEGER DEFAULT 1,
            max_contracts_per_trade INTEGER DEFAULT 0,
            option_premium_filter REAL DEFAULT 0,
            direction_filter TEXT,
            time_filter_1_enabled INTEGER DEFAULT 0,
            time_filter_1_start TEXT DEFAULT '',
            time_filter_1_stop TEXT DEFAULT '',
            time_filter_2_enabled INTEGER DEFAULT 0,
            time_filter_2_start TEXT DEFAULT '',
            time_filter_2_stop TEXT DEFAULT '',
            signal_cooldown INTEGER DEFAULT 0,
            max_signals_per_session INTEGER DEFAULT 0,
            max_daily_loss REAL DEFAULT 0,
            auto_flat_after_cutoff INTEGER DEFAULT 0,
            notes TEXT,
            recording_enabled INTEGER DEFAULT 1,
            is_recording INTEGER DEFAULT 0,
            webhook_token TEXT,
            signal_count INTEGER DEFAULT 0,
            ticker TEXT,
            position_size INTEGER DEFAULT 1,
            tp_enabled INTEGER DEFAULT 1,
            trailing_sl INTEGER DEFAULT 0,
            enabled INTEGER DEFAULT 1,
            inverse_signals INTEGER DEFAULT 0,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Traders table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS traders (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER REFERENCES users(id),
            recorder_id INTEGER,
            account_id INTEGER,
            subaccount_id INTEGER,
            subaccount_name TEXT,
            is_demo INTEGER DEFAULT 1,
            enabled INTEGER DEFAULT 1,
            initial_position_size INTEGER,
            add_position_size INTEGER,
            tp_targets TEXT,
            sl_enabled INTEGER,
            sl_amount REAL,
            sl_units TEXT,
            max_daily_loss REAL,
            enabled_accounts TEXT,
            max_contracts INTEGER DEFAULT 0,
            custom_ticker TEXT,
            multiplier REAL DEFAULT 1.0,
            risk_percent REAL,
            name TEXT,
            time_filter_1_enabled INTEGER DEFAULT 0,
            time_filter_1_start TEXT DEFAULT '',
            time_filter_1_stop TEXT DEFAULT '',
            time_filter_2_enabled INTEGER DEFAULT 0,
            time_filter_2_start TEXT DEFAULT '',
            time_filter_2_stop TEXT DEFAULT '',
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Recorded trades table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorded_trades (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER REFERENCES users(id),
            recorder_id INTEGER NOT NULL,
            signal_id INTEGER,
            ticker TEXT NOT NULL,
            action TEXT NOT NULL,
            side TEXT NOT NULL,
            entry_price REAL,
            entry_time TEXT,
            exit_price REAL,
            exit_time TEXT,
            quantity INTEGER DEFAULT 1,
            pnl REAL DEFAULT 0,
            pnl_ticks REAL DEFAULT 0,
            fees REAL DEFAULT 0,
            status TEXT DEFAULT 'open',
            exit_reason TEXT,
            notes TEXT,
            tp_price REAL,
            sl_price REAL,
            tp_ticks REAL,
            sl_ticks REAL,
            max_favorable REAL DEFAULT 0,
            max_adverse REAL DEFAULT 0,
            tp_order_id TEXT,
            sl_order_id TEXT,
            broker_order_id TEXT,
            broker_strategy_id TEXT,
            broker_fill_price REAL,
            broker_managed_tp_sl INTEGER DEFAULT 0,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Recorded signals table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorded_signals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            recorder_id INTEGER NOT NULL,
            signal_type TEXT,
            ticker TEXT,
            action TEXT,
            price REAL,
            raw_signal TEXT,
            processed INTEGER DEFAULT 0,
            result TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Recorder positions table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorder_positions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            recorder_id INTEGER,
            ticker TEXT,
            side TEXT,
            total_quantity INTEGER DEFAULT 0,
            fills TEXT,
            avg_entry_price REAL,
            realized_pnl REAL DEFAULT 0,
            unrealized_pnl REAL DEFAULT 0,
            worst_unrealized_pnl REAL DEFAULT 0,
            exit_price REAL,
            exit_time TEXT,
            status TEXT DEFAULT 'open',
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            closed_at TEXT,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Signals table (legacy)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS signals (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            recorder_id INTEGER,
            raw_data TEXT,
            action TEXT,
            ticker TEXT,
            price REAL,
            processed INTEGER DEFAULT 0,
            result TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Open positions table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS open_positions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            account_id INTEGER NOT NULL,
            subaccount_id TEXT,
            account_name TEXT,
            symbol TEXT NOT NULL,
            quantity REAL NOT NULL,
            avg_price REAL NOT NULL,
            current_price REAL DEFAULT 0.0,
            unrealized_pnl REAL DEFAULT 0.0,
            order_id TEXT,
            strategy_name TEXT,
            direction TEXT,
            open_time TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(account_id, subaccount_id, symbol)
        )
    ''')
    
    # Strategies table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS strategies (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER REFERENCES users(id),
            account_id INTEGER,
            demo_account_id INTEGER,
            name TEXT NOT NULL,
            symbol TEXT,
            strat_type TEXT,
            days_to_expiry INTEGER,
            strike_offset REAL,
            position_size INTEGER,
            position_add INTEGER,
            take_profit REAL,
            stop_loss REAL,
            trim REAL,
            tpsl_units TEXT,
            directional_strategy TEXT,
            recording_enabled INTEGER DEFAULT 1,
            positional_settings TEXT,
            delay_seconds INTEGER,
            max_contracts INTEGER,
            premium_filter INTEGER,
            direction_filter TEXT,
            time_filter_enabled INTEGER,
            time_filter_start TEXT,
            time_filter_end TEXT,
            entry_delay INTEGER,
            signal_cooldown INTEGER,
            max_signals_per_session INTEGER,
            max_daily_loss REAL,
            auto_flat INTEGER,
            active INTEGER DEFAULT 1,
            enabled INTEGER DEFAULT 1,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
            notes TEXT,
            account_routing TEXT,
            is_public INTEGER DEFAULT 0,
            created_by_username TEXT,
            inverse_signals INTEGER DEFAULT 0
        )
    ''')
    
    # Webhooks table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS webhooks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT NOT NULL,
            method TEXT DEFAULT 'POST',
            headers TEXT,
            body TEXT,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Strategy PnL history
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS strategy_pnl_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            strategy_id INTEGER,
            strategy_name TEXT,
            pnl REAL,
            drawdown REAL DEFAULT 0.0,
            timestamp TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # OAuth states table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS oauth_states (
            state TEXT PRIMARY KEY,
            account_id INTEGER NOT NULL,
            created_at TEXT NOT NULL,
            redirect_uri TEXT
        )
    ''')
    
    # Watchlist items
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS watchlist_items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT NOT NULL UNIQUE,
            company_name TEXT,
            cik TEXT,
            exchange TEXT,
            currency TEXT DEFAULT 'USD',
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Digest runs
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS digest_runs (
            run_id INTEGER PRIMARY KEY AUTOINCREMENT,
            started_at TEXT DEFAULT CURRENT_TIMESTAMP,
            finished_at TEXT,
            run_type TEXT,
            status TEXT DEFAULT 'running',
            error TEXT
        )
    ''')
    
    # News items
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news_items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url_hash TEXT NOT NULL UNIQUE,
            ticker TEXT NOT NULL,
            source TEXT NOT NULL,
            headline TEXT NOT NULL,
            summary TEXT,
            url TEXT,
            published_at TEXT,
            inserted_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Ratings snapshots
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS ratings_snapshots (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT NOT NULL,
            provider TEXT NOT NULL,
            raw_rating TEXT,
            normalized_bucket TEXT,
            as_of TEXT DEFAULT CURRENT_TIMESTAMP,
            inserted_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Politician trades
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS politician_trades (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source TEXT,
            chamber TEXT,
            politician TEXT NOT NULL,
            filed_at TEXT,
            txn_date TEXT,
            issuer TEXT,
            ticker_guess TEXT,
            action TEXT,
            amount_range TEXT,
            url TEXT,
            inserted_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Support tickets for chat support system
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS support_tickets (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id INTEGER REFERENCES users(id),
            user_email TEXT,
            user_name TEXT,
            subject TEXT,
            status TEXT DEFAULT 'open',
            priority TEXT DEFAULT 'normal',
            created_at TEXT DEFAULT CURRENT_TIMESTAMP,
            updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
            closed_at TEXT
        )
    ''')
    
    # Support messages (chat messages within tickets)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS support_messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticket_id INTEGER REFERENCES support_tickets(id),
            sender_type TEXT NOT NULL,
            sender_id INTEGER,
            sender_name TEXT,
            message TEXT NOT NULL,
            created_at TEXT DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    print("‚úÖ SQLite tables created (complete schema)!")


def _init_postgres_tables():
    """Initialize ALL PostgreSQL tables to match SQLite schema."""
    import psycopg2
    db_url = DATABASE_URL.replace('postgres://', 'postgresql://', 1)
    conn = psycopg2.connect(db_url, connect_timeout=10)
    cursor = conn.cursor()
    
    print("üìä Creating PostgreSQL tables (complete schema)...")
    
    # Users table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id SERIAL PRIMARY KEY,
            username VARCHAR(100) NOT NULL UNIQUE,
            email VARCHAR(255) NOT NULL UNIQUE,
            password_hash VARCHAR(255) NOT NULL,
            discord_user_id VARCHAR(50) UNIQUE,
            discord_access_token TEXT,
            discord_dms_enabled BOOLEAN DEFAULT FALSE,
            session_id VARCHAR(255),
            last_login TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            display_name TEXT,
            is_admin BOOLEAN DEFAULT FALSE,
            is_active BOOLEAN DEFAULT TRUE,
            settings_json TEXT DEFAULT '{}'
        )
    ''')
    
    # Announcements/Bulletins table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS announcements (
            id SERIAL PRIMARY KEY,
            title VARCHAR(255) NOT NULL,
            message TEXT NOT NULL,
            type VARCHAR(20) DEFAULT 'info',
            is_active BOOLEAN DEFAULT TRUE,
            created_by INTEGER REFERENCES users(id),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            expires_at TIMESTAMP,
            priority INTEGER DEFAULT 0
        )
    ''')
    
    # Accounts table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS accounts (
            id SERIAL PRIMARY KEY,
            user_id INTEGER REFERENCES users(id),
            name VARCHAR(100) NOT NULL,
            broker VARCHAR(50) DEFAULT 'Tradovate',
            auth_type VARCHAR(20) DEFAULT 'oauth',
            username VARCHAR(100),
            password TEXT,
            account_id VARCHAR(50),
            api_key VARCHAR(255),
            api_secret TEXT,
            api_endpoint VARCHAR(255),
            environment VARCHAR(20) DEFAULT 'demo',
            client_id VARCHAR(255),
            client_secret TEXT,
            tradovate_token TEXT,
            tradovate_refresh_token TEXT,
            token_expires_at TIMESTAMP,
            max_contracts INTEGER,
            multiplier REAL DEFAULT 1.0,
            enabled BOOLEAN DEFAULT TRUE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            subaccounts TEXT,
            tradovate_accounts TEXT,
            md_access_token TEXT,
            tradingview_session TEXT,
            has_tradingview_addon BOOLEAN DEFAULT FALSE,
            device_id VARCHAR(255)
        )
    ''')
    
    # Recorders table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorders (
            id SERIAL PRIMARY KEY,
            user_id INTEGER REFERENCES users(id),
            name TEXT NOT NULL,
            strategy_type TEXT DEFAULT 'Futures',
            symbol TEXT,
            demo_account_id TEXT,
            account_id INTEGER,
            is_private BOOLEAN DEFAULT FALSE,
            initial_position_size INTEGER DEFAULT 2,
            add_position_size INTEGER DEFAULT 2,
            tp_units TEXT DEFAULT 'Ticks',
            trim_units TEXT DEFAULT 'Contracts',
            tp_targets TEXT DEFAULT '[]',
            sl_enabled BOOLEAN DEFAULT FALSE,
            sl_amount REAL DEFAULT 0,
            sl_units TEXT DEFAULT 'Ticks',
            sl_type TEXT DEFAULT 'Fixed',
            break_even_enabled BOOLEAN DEFAULT FALSE,
            break_even_ticks INTEGER DEFAULT 10,
            avg_down_enabled BOOLEAN DEFAULT FALSE,
            avg_down_amount INTEGER DEFAULT 1,
            avg_down_point REAL DEFAULT 0,
            avg_down_units TEXT DEFAULT 'Ticks',
            add_delay INTEGER DEFAULT 1,
            max_contracts_per_trade INTEGER DEFAULT 0,
            option_premium_filter REAL DEFAULT 0,
            direction_filter TEXT,
            time_filter_1_enabled BOOLEAN DEFAULT FALSE,
            time_filter_1_start TEXT DEFAULT '',
            time_filter_1_stop TEXT DEFAULT '',
            time_filter_2_enabled BOOLEAN DEFAULT FALSE,
            time_filter_2_start TEXT DEFAULT '',
            time_filter_2_stop TEXT DEFAULT '',
            signal_cooldown INTEGER DEFAULT 0,
            max_signals_per_session INTEGER DEFAULT 0,
            max_daily_loss REAL DEFAULT 0,
            auto_flat_after_cutoff BOOLEAN DEFAULT FALSE,
            notes TEXT,
            recording_enabled BOOLEAN DEFAULT TRUE,
            is_recording BOOLEAN DEFAULT FALSE,
            webhook_token TEXT,
            signal_count INTEGER DEFAULT 0,
            ticker TEXT,
            position_size INTEGER DEFAULT 1,
            tp_enabled BOOLEAN DEFAULT TRUE,
            trailing_sl BOOLEAN DEFAULT FALSE,
            enabled BOOLEAN DEFAULT TRUE,
            inverse_signals BOOLEAN DEFAULT FALSE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Traders table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS traders (
            id SERIAL PRIMARY KEY,
            user_id INTEGER REFERENCES users(id),
            recorder_id INTEGER,
            account_id INTEGER,
            subaccount_id INTEGER,
            subaccount_name VARCHAR(255),
            is_demo BOOLEAN DEFAULT TRUE,
            enabled BOOLEAN DEFAULT TRUE,
            initial_position_size INTEGER,
            add_position_size INTEGER,
            tp_targets TEXT,
            sl_enabled BOOLEAN,
            sl_amount REAL,
            sl_units TEXT,
            max_daily_loss REAL,
            enabled_accounts TEXT,
            max_contracts INTEGER DEFAULT 0,
            custom_ticker VARCHAR(50),
            multiplier REAL DEFAULT 1.0,
            risk_percent REAL,
            name VARCHAR(255),
            time_filter_1_enabled BOOLEAN DEFAULT FALSE,
            time_filter_1_start TEXT DEFAULT '',
            time_filter_1_stop TEXT DEFAULT '',
            time_filter_2_enabled BOOLEAN DEFAULT FALSE,
            time_filter_2_start TEXT DEFAULT '',
            time_filter_2_stop TEXT DEFAULT '',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Recorded trades table - full schema
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorded_trades (
            id SERIAL PRIMARY KEY,
            user_id INTEGER REFERENCES users(id),
            recorder_id INTEGER NOT NULL,
            signal_id INTEGER,
            ticker TEXT NOT NULL,
            action TEXT NOT NULL,
            side TEXT NOT NULL,
            entry_price REAL,
            entry_time TIMESTAMP,
            exit_price REAL,
            exit_time TIMESTAMP,
            quantity INTEGER DEFAULT 1,
            pnl REAL DEFAULT 0,
            pnl_ticks REAL DEFAULT 0,
            fees REAL DEFAULT 0,
            status TEXT DEFAULT 'open',
            exit_reason TEXT,
            notes TEXT,
            tp_price REAL,
            sl_price REAL,
            tp_ticks REAL,
            sl_ticks REAL,
            max_favorable REAL DEFAULT 0,
            max_adverse REAL DEFAULT 0,
            tp_order_id VARCHAR(100),
            sl_order_id VARCHAR(100),
            broker_order_id VARCHAR(100),
            broker_strategy_id VARCHAR(100),
            broker_fill_price REAL,
            broker_managed_tp_sl BOOLEAN DEFAULT FALSE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Recorded signals table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorded_signals (
            id SERIAL PRIMARY KEY,
            recorder_id INTEGER NOT NULL,
            signal_type TEXT,
            ticker TEXT,
            action TEXT,
            price REAL,
            raw_signal TEXT,
            processed BOOLEAN DEFAULT FALSE,
            result TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Recorder positions table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorder_positions (
            id SERIAL PRIMARY KEY,
            recorder_id INTEGER,
            ticker VARCHAR(50),
            side VARCHAR(10),
            total_quantity INTEGER DEFAULT 0,
            fills TEXT,
            avg_entry_price REAL,
            realized_pnl REAL DEFAULT 0,
            unrealized_pnl REAL DEFAULT 0,
            worst_unrealized_pnl REAL DEFAULT 0,
            exit_price REAL,
            exit_time TIMESTAMP,
            status VARCHAR(20) DEFAULT 'open',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            closed_at TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Signals table (legacy)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS signals (
            id SERIAL PRIMARY KEY,
            recorder_id INTEGER,
            raw_data TEXT,
            action VARCHAR(20),
            ticker VARCHAR(50),
            price REAL,
            processed BOOLEAN DEFAULT FALSE,
            result TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Open positions table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS open_positions (
            id SERIAL PRIMARY KEY,
            account_id INTEGER NOT NULL,
            subaccount_id TEXT,
            account_name TEXT,
            symbol TEXT NOT NULL,
            quantity REAL NOT NULL,
            avg_price REAL NOT NULL,
            current_price REAL DEFAULT 0.0,
            unrealized_pnl REAL DEFAULT 0.0,
            order_id TEXT,
            strategy_name TEXT,
            direction TEXT,
            open_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(account_id, subaccount_id, symbol)
        )
    ''')
    
    # Strategies table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS strategies (
            id SERIAL PRIMARY KEY,
            user_id INTEGER REFERENCES users(id),
            account_id INTEGER,
            demo_account_id INTEGER,
            name VARCHAR(100) NOT NULL,
            symbol VARCHAR(20),
            strat_type VARCHAR(50),
            days_to_expiry INTEGER,
            strike_offset REAL,
            position_size INTEGER,
            position_add INTEGER,
            take_profit REAL,
            stop_loss REAL,
            trim REAL,
            tpsl_units VARCHAR(20),
            directional_strategy VARCHAR(50),
            recording_enabled BOOLEAN DEFAULT TRUE,
            positional_settings TEXT,
            delay_seconds INTEGER,
            max_contracts INTEGER,
            premium_filter BOOLEAN,
            direction_filter VARCHAR(20),
            time_filter_enabled BOOLEAN,
            time_filter_start VARCHAR(10),
            time_filter_end VARCHAR(10),
            entry_delay INTEGER,
            signal_cooldown INTEGER,
            max_signals_per_session INTEGER,
            max_daily_loss REAL,
            auto_flat BOOLEAN,
            active BOOLEAN DEFAULT TRUE,
            enabled BOOLEAN DEFAULT TRUE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            notes TEXT,
            account_routing TEXT,
            is_public BOOLEAN DEFAULT FALSE,
            created_by_username VARCHAR(100),
            inverse_signals BOOLEAN DEFAULT FALSE
        )
    ''')
    
    # Webhooks table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS webhooks (
            id SERIAL PRIMARY KEY,
            url TEXT NOT NULL,
            method TEXT DEFAULT 'POST',
            headers TEXT,
            body TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Strategy PnL history
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS strategy_pnl_history (
            id SERIAL PRIMARY KEY,
            strategy_id INTEGER,
            strategy_name TEXT,
            pnl REAL,
            drawdown REAL DEFAULT 0.0,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # OAuth states table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS oauth_states (
            state TEXT PRIMARY KEY,
            account_id INTEGER NOT NULL,
            created_at TEXT NOT NULL,
            redirect_uri TEXT
        )
    ''')
    
    # Watchlist items
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS watchlist_items (
            id SERIAL PRIMARY KEY,
            ticker TEXT NOT NULL UNIQUE,
            company_name TEXT,
            cik TEXT,
            exchange TEXT,
            currency TEXT DEFAULT 'USD',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Digest runs
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS digest_runs (
            run_id SERIAL PRIMARY KEY,
            started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            finished_at TIMESTAMP,
            run_type TEXT,
            status TEXT DEFAULT 'running',
            error TEXT
        )
    ''')
    
    # News items
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS news_items (
            id SERIAL PRIMARY KEY,
            url_hash TEXT NOT NULL UNIQUE,
            ticker TEXT NOT NULL,
            source TEXT NOT NULL,
            headline TEXT NOT NULL,
            summary TEXT,
            url TEXT,
            published_at TIMESTAMP,
            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Ratings snapshots
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS ratings_snapshots (
            id SERIAL PRIMARY KEY,
            ticker TEXT NOT NULL,
            provider TEXT NOT NULL,
            raw_rating TEXT,
            normalized_bucket TEXT,
            as_of TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Politician trades
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS politician_trades (
            id SERIAL PRIMARY KEY,
            source TEXT,
            chamber TEXT,
            politician TEXT NOT NULL,
            filed_at TIMESTAMP,
            txn_date TIMESTAMP,
            issuer TEXT,
            ticker_guess TEXT,
            action TEXT,
            amount_range TEXT,
            url TEXT,
            inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Support tickets for chat support system
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS support_tickets (
            id SERIAL PRIMARY KEY,
            user_id INTEGER REFERENCES users(id),
            user_email TEXT,
            user_name TEXT,
            subject TEXT,
            status TEXT DEFAULT 'open',
            priority TEXT DEFAULT 'normal',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            closed_at TIMESTAMP
        )
    ''')
    
    # Support messages (chat messages within tickets)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS support_messages (
            id SERIAL PRIMARY KEY,
            ticket_id INTEGER REFERENCES support_tickets(id),
            sender_type TEXT NOT NULL,
            sender_id INTEGER,
            sender_name TEXT,
            message TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Add inverse_signals column to recorders table (Jan 2026 migration)
    try:
        cursor.execute("ALTER TABLE recorders ADD COLUMN IF NOT EXISTS inverse_signals BOOLEAN DEFAULT FALSE")
        print("‚úÖ Added inverse_signals column to recorders table")
    except:
        pass  # Column already exists
    
    # Add inverse_signals column to strategies table (Jan 2026 migration)
    try:
        cursor.execute("ALTER TABLE strategies ADD COLUMN IF NOT EXISTS inverse_signals BOOLEAN DEFAULT FALSE")
        print("‚úÖ Added inverse_signals column to strategies table")
    except:
        pass  # Column already exists
    
    conn.commit()
    cursor.close()
    conn.close()
    print("‚úÖ PostgreSQL tables created (complete schema)!")

app = Flask(__name__)
logger = logging.getLogger(__name__)

# ============================================================================
# SIMPLE HEALTH CHECK - Responds immediately, no DB or external checks
# This ensures Railway health check passes even during slow startup
# ============================================================================
@app.route('/ping')
def ping():
    """Ultra-simple health check - always returns OK immediately"""
    return 'OK', 200

@app.route('/health')
def health_simple():
    """Simple health check for Railway - no DB queries to avoid timeouts"""
    brevo_ok = False
    try:
        import brevo_python
        brevo_ok = True
    except ImportError:
        pass
    return jsonify({
        'status': 'healthy',
        'service': 'just-trades',
        'message': 'Server is running',
        'brevo_installed': brevo_ok,
        'whop_configured': bool(os.environ.get('WHOP_API_KEY')),
        'brevo_configured': bool(os.environ.get('BREVO_API_KEY')),
    })

# ============================================================================
# GLOBAL ERROR HANDLER - Hide tracebacks in production
# ============================================================================
_is_production = bool(os.environ.get('RAILWAY_ENVIRONMENT') or os.environ.get('DATABASE_URL'))

@app.errorhandler(404)
def not_found_error(error):
    if request.path.startswith('/api/'):
        return jsonify({'error': 'Not found'}), 404
    return '<h1>404 ‚Äî Page Not Found</h1><p>The requested page does not exist.</p>', 404

@app.errorhandler(500)
def internal_error(error):
    import traceback
    tb = traceback.format_exc()
    print(f"‚ùå 500 ERROR: {error}")
    print(f"‚ùå TRACEBACK: {tb}")
    if _is_production:
        if request.path.startswith('/api/'):
            return jsonify({'error': 'Internal server error'}), 500
        return '<h1>500 ‚Äî Internal Server Error</h1><p>Something went wrong. Please try again later.</p>', 500
    return f"""
    <h1>500 Internal Server Error</h1>
    <h2>Error: {error}</h2>
    <h3>Traceback:</h3>
    <pre>{tb}</pre>
    """, 500

@app.errorhandler(Exception)
def handle_exception(e):
    import traceback
    tb = traceback.format_exc()
    print(f"‚ùå EXCEPTION: {e}")
    print(f"‚ùå TRACEBACK: {tb}")
    if _is_production:
        if request.path.startswith('/api/'):
            return jsonify({'error': 'Internal server error'}), 500
        return '<h1>Error</h1><p>Something went wrong. Please try again later.</p>', 500
    return f"""
    <h1>Error</h1>
    <h2>{type(e).__name__}: {e}</h2>
    <h3>Traceback:</h3>
    <pre>{tb}</pre>
    """, 500

# ============================================================================
# GLOBAL API SECURITY GATE ‚Äî Requires authentication for ALL /api/ routes
# ============================================================================
# Whitelist of /api/ paths that MUST remain public (no auth possible)
_API_PUBLIC_PREFIXES = (
    '/api/oauth/callback',      # Tradovate OAuth redirect ‚Äî external system calls this
    '/api/public/',             # Explicitly public endpoints (stats, etc.)
    '/api/trading-engine/',     # Internal health check (no sensitive data)
)

@app.before_request
def _global_api_auth_gate():
    """Enforce authentication on ALL /api/ routes unless whitelisted."""
    path = request.path

    # Only guard /api/ routes
    if not path.startswith('/api/'):
        return None

    # Allow whitelisted public paths
    for prefix in _API_PUBLIC_PREFIXES:
        if path.startswith(prefix):
            return None

    # Allow if valid ADMIN_API_KEY is provided (for curl/monitoring)
    api_key = request.headers.get('X-Admin-Key') or request.args.get('admin_key')
    expected_key = os.environ.get('ADMIN_API_KEY')
    if expected_key and api_key == expected_key:
        return None

    # Allow if user is logged in
    if USER_AUTH_AVAILABLE and is_logged_in():
        return None

    # Block everything else
    return jsonify({'error': 'Authentication required'}), 401

# ============================================================================
# RATE LIMITING ‚Äî Protect /login and /register from brute force
# ============================================================================
_rate_limit_store = {}  # {ip: [timestamp, timestamp, ...]}
_rate_limit_lock = threading.Lock()
_RATE_LIMIT_WINDOW = 300  # 5 minutes
_RATE_LIMIT_MAX_LOGIN = 10  # max 10 login attempts per 5 min
_RATE_LIMIT_MAX_REGISTER = 5  # max 5 register attempts per 5 min

def _check_rate_limit(key, max_attempts):
    """Returns True if rate limited (too many attempts)."""
    now = time.time()
    with _rate_limit_lock:
        if key not in _rate_limit_store:
            _rate_limit_store[key] = []
        # Prune old entries outside the window
        _rate_limit_store[key] = [t for t in _rate_limit_store[key] if now - t < _RATE_LIMIT_WINDOW]
        if len(_rate_limit_store[key]) >= max_attempts:
            return True
        _rate_limit_store[key].append(now)
        return False

@app.before_request
def _rate_limit_auth_routes():
    """Rate limit login and register POST requests."""
    if request.method != 'POST':
        return None
    path = request.path
    ip = request.headers.get('X-Forwarded-For', request.remote_addr or '').split(',')[0].strip()
    if path == '/login':
        if _check_rate_limit(f'login:{ip}', _RATE_LIMIT_MAX_LOGIN):
            logger.warning(f"RATE LIMITED: {ip} on /login")
            flash('Too many login attempts. Please wait 5 minutes.', 'error')
            return render_template('login.html'), 429
    elif path == '/register':
        if _check_rate_limit(f'register:{ip}', _RATE_LIMIT_MAX_REGISTER):
            logger.warning(f"RATE LIMITED: {ip} on /register")
            flash('Too many registration attempts. Please wait 5 minutes.', 'error')
            return render_template('register.html'), 429
    return None

# ============================================================================
# CSRF PROTECTION ‚Äî Block cross-origin POST requests
# ============================================================================
_CSRF_SAFE_METHODS = frozenset(['GET', 'HEAD', 'OPTIONS'])
_CSRF_EXEMPT_PREFIXES = (
    '/webhook/',            # TradingView webhook signals (external POST)
    '/webhooks/',           # Whop webhook signals (external POST)
    '/api/oauth/callback',  # Tradovate OAuth redirect
    '/api/trading-engine/', # Internal health check
)

@app.before_request
def _csrf_origin_check():
    """Reject cross-origin POST/PUT/DELETE requests (CSRF protection)."""
    if request.method in _CSRF_SAFE_METHODS:
        return None
    # Exempt paths (webhooks, OAuth callbacks)
    for prefix in _CSRF_EXEMPT_PREFIXES:
        if request.path.startswith(prefix):
            return None
    # API calls with admin key are exempt (curl/monitoring)
    api_key = request.headers.get('X-Admin-Key')
    if api_key and api_key == os.environ.get('ADMIN_API_KEY'):
        return None
    # Check Origin header first, then Referer
    origin = request.headers.get('Origin')
    referer = request.headers.get('Referer')
    if origin:
        allowed_origins = [
            'https://www.justtrades.app',
            'https://justtrades.app',
            'http://localhost:5000',
            'http://127.0.0.1:5000',
        ]
        if origin not in allowed_origins:
            logger.warning(f"CSRF BLOCKED: Origin={origin} Path={request.path}")
            if request.path.startswith('/api/'):
                return jsonify({'error': 'Cross-origin request blocked'}), 403
            flash('Request blocked for security reasons.', 'error')
            return redirect(request.path), 403
    elif referer:
        from urllib.parse import urlparse
        ref_host = urlparse(referer).netloc
        allowed_hosts = ['www.justtrades.app', 'justtrades.app', 'localhost:5000', '127.0.0.1:5000']
        if ref_host not in allowed_hosts:
            logger.warning(f"CSRF BLOCKED: Referer={referer} Path={request.path}")
            if request.path.startswith('/api/'):
                return jsonify({'error': 'Cross-origin request blocked'}), 403
            flash('Request blocked for security reasons.', 'error')
            return redirect(request.path), 403
    # If neither Origin nor Referer is present, allow (some browsers strip these)
    return None

# ============================================================================
# SESSION CONFIGURATION - Required for User Authentication
# ============================================================================
# Use environment variable for production, or generate a secure random key
import secrets
app.secret_key = os.environ.get('FLASK_SECRET_KEY') or secrets.token_hex(32)
app.config['SESSION_TYPE'] = 'filesystem'
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(days=7)  # Sessions last 7 days
app.config['SESSION_COOKIE_SECURE'] = bool(os.environ.get('RAILWAY_PUBLIC_DOMAIN') or os.environ.get('FLASK_ENV') == 'production')
app.config['SESSION_COOKIE_HTTPONLY'] = True
app.config['SESSION_COOKIE_SAMESITE'] = 'Lax'

# Register auth context processor to make current_user available in all templates
if USER_AUTH_AVAILABLE:
    app.context_processor(auth_context_processor)

# ============================================================================
# INITIALIZE DISCORD MODULE
# ============================================================================
_auth_db_fn = None
if USER_AUTH_AVAILABLE:
    try:
        from user_auth import get_auth_db_connection as _auth_db_fn
    except ImportError:
        pass

init_discord_notifications(
    bot_token=DISCORD_BOT_TOKEN,
    get_db_connection=get_db_connection,
    is_using_postgres=is_using_postgres,
    get_chicago_time=get_chicago_time,
    send_push_notification=send_push_notification if PUSH_NOTIFICATIONS_ENABLED else None,
    push_notifications_enabled=PUSH_NOTIFICATIONS_ENABLED,
    user_auth_available=USER_AUTH_AVAILABLE,
    get_auth_db_connection=_auth_db_fn
)

init_discord_routes(
    user_auth_available=USER_AUTH_AVAILABLE,
    is_logged_in=is_logged_in if USER_AUTH_AVAILABLE else (lambda: False),
    get_current_user=get_current_user if USER_AUTH_AVAILABLE else (lambda: None),
    get_auth_db_connection=_auth_db_fn,
    discord_notifications_enabled=DISCORD_NOTIFICATIONS_ENABLED,
    send_discord_dm_fn=send_discord_dm,
    broadcast_announcement_fn=broadcast_announcement,
)
app.register_blueprint(discord_bp)

# Account activation routes (Whop purchase -> website access)
try:
    from account_activation import activation_bp
    app.register_blueprint(activation_bp)
    logger.info("Account activation routes registered")
except ImportError:
    logger.debug("Account activation module not available")

# Template filter for date formatting
@app.template_filter('format_datetime')
def format_datetime_filter(value):
    """Format datetime string for display in Chicago time."""
    if not value:
        return None
    try:
        from datetime import datetime
        # Handle different date formats (SQLite TEXT, PostgreSQL TIMESTAMP)
        value_str = str(value).strip()
        
        # Try ISO format first (PostgreSQL with T separator)
        if 'T' in value_str:
            # Remove timezone if present for parsing
            clean_value = value_str.split('+')[0].split('-')[0] if '+' in value_str or value_str.count('-') > 2 else value_str
            clean_value = clean_value.replace('Z', '').strip()
            try:
                dt = datetime.fromisoformat(clean_value)
            except:
                # Fallback: try parsing without microseconds
                dt = datetime.strptime(clean_value.split('.')[0], '%Y-%m-%dT%H:%M:%S')
        else:
            # SQLite format: 'YYYY-MM-DD HH:MM:SS' or 'YYYY-MM-DD'
            clean_value = value_str.split('.')[0]  # Remove microseconds if present
            if len(clean_value) > 10:  # Has time component
                dt = datetime.strptime(clean_value, '%Y-%m-%d %H:%M:%S')
            else:  # Date only
                dt = datetime.strptime(clean_value, '%Y-%m-%d')
                return dt.strftime('%b %d, %Y')
        
        # Convert to Chicago time for display
        if dt.tzinfo is None:
            # Assume UTC if no timezone, convert to Chicago
            dt = dt.replace(tzinfo=ZoneInfo('UTC') if 'ZoneInfo' in dir() else pytz.UTC)
        try:
            dt_chicago = dt.astimezone(CHICAGO_TZ)
            return dt_chicago.strftime('%b %d, %Y %I:%M %p CT')
        except:
            return dt.strftime('%b %d, %Y %I:%M %p')
    except Exception as e:
        # Return simplified version if parsing fails
        return str(value).split('T')[0] if 'T' in str(value) else str(value).split(' ')[0]

# Register subscription context processor
if SUBSCRIPTION_SYSTEM_AVAILABLE:
    @app.context_processor
    def subscription_context():
        """Make subscription status available in all templates."""
        from flask import session
        user_id = session.get('user_id')
        if user_id:
            try:
                from subscription_models import get_user_subscription
                platform_sub = get_user_subscription(user_id, plan_type='platform')
                return {
                    'subscription_status': get_feature_status(user_id),
                    'user_plan_tier': get_user_plan_tier(user_id),
                    'has_platform_subscription': platform_sub is not None,
                    'platform_subscription': platform_sub,
                }
            except:
                pass
        return {
            'subscription_status': {'has_subscription': False, 'tier': 'none'},
            'user_plan_tier': 'none',
            'has_platform_subscription': False,
            'platform_subscription': None,
        }

# Initialize SocketIO for WebSocket support (like Trade Manager)
# When EXTERNAL_TRADING_ENGINE is set, use Redis message_queue so trading_engine.py
# can emit events to browser clients via this web server's SocketIO.
_sio_message_queue = None
if _EXTERNAL_ENGINE:
    _sio_redis_url = os.environ.get('REDIS_URL')
    if _sio_redis_url:
        _sio_message_queue = _sio_redis_url
        logger.info("üîó SocketIO: Using Redis message_queue for cross-process emit")

# Use 'eventlet' or 'gevent' if available, otherwise fall back to threading
try:
    import eventlet
    eventlet.monkey_patch()
    socketio = SocketIO(app, cors_allowed_origins="*", async_mode='eventlet', message_queue=_sio_message_queue)
    logger.info("SocketIO using eventlet async mode")
except ImportError:
    try:
        import gevent
        from gevent import monkey
        monkey.patch_all()
        socketio = SocketIO(app, cors_allowed_origins="*", async_mode='gevent', message_queue=_sio_message_queue)
        logger.info("SocketIO using gevent async mode")
    except ImportError:
        socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading', message_queue=_sio_message_queue)
        logger.info("SocketIO using threading async mode (fallback)")

logging.getLogger('engineio.server').setLevel(logging.WARNING)
logging.getLogger('socketio.server').setLevel(logging.WARNING)

# ============================================================================
# INITIALIZE 100+ USER SCALABILITY MODULE (if available and enabled)
# ============================================================================
if SCALABILITY_MODULE_AVAILABLE:
    try:
        # Check if any scalability features are enabled
        any_enabled = any(SCALABILITY_FEATURES.values())
        if any_enabled:
            logger.info("üöÄ Scalability module detected with enabled features - initializing...")
            init_result = init_scalability(socketio, get_db_connection, auto_start=True)
            register_scalability_routes(app)
            register_scalability_socketio_handlers(socketio)
            logger.info(f"‚úÖ Scalability module initialized: {init_result.get('components', {})}")
        else:
            logger.info("‚ÑπÔ∏è Scalability module available but no features enabled")
            logger.info("   Enable with: export SCALABILITY_UI_PUBLISHER=1")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Scalability module initialization failed (non-fatal): {e}")

# ‚ö†Ô∏è SECURITY WARNING: API credentials should be stored in environment variables or secure config
# These are default credentials - prefer storing in .env file or environment variables
# Example: TRADOVATE_API_CID=8720, TRADOVATE_API_SECRET=your-secret
TRADOVATE_API_CID = int(os.getenv('TRADOVATE_API_CID', '8720'))
TRADOVATE_API_SECRET = os.getenv('TRADOVATE_API_SECRET', 'e76ee8d1-d168-4252-a59e-f11a8b0cdae4')

# Contract multipliers for PnL calculation
CONTRACT_MULTIPLIERS = {
    'MES': 5.0,    # Micro E-mini S&P 500: $5 per point
    'MNQ': 2.0,    # Micro E-mini Nasdaq: $2 per point
    'ES': 50.0,    # E-mini S&P 500: $50 per point
    'NQ': 20.0,    # E-mini Nasdaq: $20 per point
    'MYM': 5.0,    # Micro E-mini Dow: $5 per point
    'YM': 5.0,     # E-mini Dow: $5 per point
    'M2K': 5.0,    # Micro E-mini Russell 2000: $5 per point
    'RTY': 50.0,   # E-mini Russell 2000: $50 per point
}

def get_contract_multiplier(symbol: str) -> float:
    """Get contract multiplier for a symbol"""
    symbol_upper = symbol.upper().strip()
    
    # Try to match known base symbols (2-3 characters)
    # Check 3-char symbols first (MES, MNQ, M2K, etc.)
    if symbol_upper[:3] in CONTRACT_MULTIPLIERS:
        return CONTRACT_MULTIPLIERS[symbol_upper[:3]]
    
    # Check 2-char symbols (ES, NQ, YM, etc.)
    if symbol_upper[:2] in CONTRACT_MULTIPLIERS:
        return CONTRACT_MULTIPLIERS[symbol_upper[:2]]
    
    # Fallback: remove month codes and numbers
    # Month codes: F, G, H, J, K, M, N, Q, U, V, X, Z
    base_symbol = re.sub(r'[0-9!]+', '', symbol_upper)  # Remove numbers and !
    base_symbol = re.sub(r'[FGHJKMNQUVXZ]$', '', base_symbol)  # Remove trailing month code
    
    return CONTRACT_MULTIPLIERS.get(base_symbol, 1.0)

def get_market_price_simple(symbol: str) -> Optional[float]:
    """
    Get current market price for futures. Tries multiple sources:
    1. Yahoo Finance (most reliable for futures)
    2. TradingView scanner API (fallback)
    """
    try:
        if not symbol:
            return None

        # Extract root symbol
        clean_symbol = symbol.upper().replace('!', '').replace('1', '')
        root = extract_symbol_root(clean_symbol)

        # Map to Yahoo Finance futures symbols (e.g., MNQ=F)
        yahoo_futures_map = {
            'MNQ': 'MNQ=F',  # Micro E-mini Nasdaq
            'MES': 'MES=F',  # Micro E-mini S&P
            'M2K': 'M2K=F',  # Micro E-mini Russell
            'MYM': 'MYM=F',  # Micro E-mini Dow
            'NQ': 'NQ=F',    # E-mini Nasdaq
            'ES': 'ES=F',    # E-mini S&P
            'RTY': 'RTY=F',  # E-mini Russell
            'YM': 'YM=F',    # E-mini Dow
            'CL': 'CL=F',    # Crude Oil
            'GC': 'GC=F',    # Gold
            'MGC': 'MGC=F',  # Micro Gold
            'MCL': 'MCL=F',  # Micro Crude
            'SI': 'SI=F',    # Silver
            'HG': 'HG=F',    # Copper
        }

        yahoo_symbol = yahoo_futures_map.get(root)
        if yahoo_symbol:
            # Try Yahoo Finance first - most reliable
            try:
                url = f"https://query1.finance.yahoo.com/v8/finance/chart/{yahoo_symbol}"
                params = {'interval': '1m', 'range': '1d'}
                headers = {'User-Agent': 'JustTrades/1.0'}

                response = requests.get(url, params=params, headers=headers, timeout=5)

                if response.status_code == 200:
                    data = response.json()
                    result = data.get('chart', {}).get('result', [])

                    if result:
                        meta = result[0].get('meta', {})
                        price = meta.get('regularMarketPrice')

                        if price:
                            logger.debug(f"üìà Yahoo Finance price for {root}: {price}")
                            return float(price)
            except Exception as yahoo_err:
                logger.debug(f"Yahoo Finance error for {root}: {yahoo_err}")

        # Fallback: TradingView scanner API
        tv_symbol_map = {
            'MNQ': 'CME_MINI:MNQ1!',
            'MES': 'CME_MINI:MES1!',
            'MYM': 'CBOT_MINI:MYM1!',
            'M2K': 'CME_MINI:M2K1!',
            'NQ': 'CME:NQ1!',
            'ES': 'CME:ES1!',
            'YM': 'CBOT:YM1!',
            'RTY': 'CME:RTY1!',
            'CL': 'NYMEX:CL1!',
            'GC': 'COMEX:GC1!',
            'MGC': 'COMEX:MGC1!',
            'SI': 'COMEX:SI1!',
        }

        tv_symbol = tv_symbol_map.get(root, f'CME:{clean_symbol}')

        url = "https://scanner.tradingview.com/futures/scan"
        payload = {
            "symbols": {"tickers": [tv_symbol]},
            "columns": ["close", "change", "high", "low", "volume"]
        }

        response = requests.post(url, json=payload, timeout=5)

        if response.status_code == 200:
            data = response.json()
            if data.get('data') and len(data['data']) > 0:
                price = data['data'][0].get('d', [None])[0]
                if price:
                    logger.debug(f"üìà TradingView price for {root}: {price}")
                    return float(price)

        logger.debug(f"Could not get price for {symbol} from any source")
        return None

    except Exception as e:
        logger.warning(f"Error getting market price for {symbol}: {e}")
        return None


# Cache for price data to avoid excessive API calls
_price_cache = {}
_price_cache_ttl = 10  # seconds - reduced for better drawdown tracking when WebSocket unavailable


def get_cached_price(symbol: str) -> Optional[float]:
    """Get price with caching to avoid rate limits"""
    global _price_cache
    
    if not symbol:
        return None
    
    root = extract_symbol_root(symbol)
    cache_key = root
    
    # Check cache first
    if cache_key in _price_cache:
        cached_price, cached_time = _price_cache[cache_key]
        if time.time() - cached_time < _price_cache_ttl:
            return cached_price
    
    # Fetch new price
    price = get_market_price_simple(symbol)
    
    if price:
        _price_cache[cache_key] = (price, time.time())
    
    return price

SYMBOL_FALLBACK_MAP = {
    'MNQ': 'MNQZ5',
    'MES': 'MESZ5',
    'ES': 'ESZ5',
    'NQ': 'NQZ5',
    'CL': 'CLZ5',
    'GC': 'GCZ5',
    'MCL': 'MCLZ5'
}
SYMBOL_CONVERSION_CACHE: dict[tuple[str, bool], tuple[str, datetime]] = {}
SYMBOL_CACHE_TTL = timedelta(hours=1)

TICK_INFO = {
    # === INDEX FUTURES ===
    'MES': {'tick_size': 0.25, 'tick_value': 1.25},
    'ES': {'tick_size': 0.25, 'tick_value': 12.5},
    'MNQ': {'tick_size': 0.25, 'tick_value': 0.5},
    'NQ': {'tick_size': 0.25, 'tick_value': 5.0},
    'MYM': {'tick_size': 1.0, 'tick_value': 0.5},
    'YM': {'tick_size': 1.0, 'tick_value': 5.0},
    'M2K': {'tick_size': 0.1, 'tick_value': 0.5},
    'RTY': {'tick_size': 0.1, 'tick_value': 5.0},

    # === METALS ===
    'GC': {'tick_size': 0.1, 'tick_value': 10.0},
    'MGC': {'tick_size': 0.1, 'tick_value': 1.0},
    'SI': {'tick_size': 0.005, 'tick_value': 25.0},
    'SIL': {'tick_size': 0.005, 'tick_value': 5.0},
    'HG': {'tick_size': 0.0005, 'tick_value': 12.5},
    'PL': {'tick_size': 0.1, 'tick_value': 5.0},

    # === ENERGIES ===
    'CL': {'tick_size': 0.01, 'tick_value': 10.0},
    'MCL': {'tick_size': 0.01, 'tick_value': 1.0},
    'NG': {'tick_size': 0.001, 'tick_value': 10.0},
    'HO': {'tick_size': 0.0001, 'tick_value': 4.2},
    'RB': {'tick_size': 0.0001, 'tick_value': 4.2},

    # === TREASURIES ===
    'ZB': {'tick_size': 0.03125, 'tick_value': 31.25},
    'ZN': {'tick_size': 0.015625, 'tick_value': 15.625},
    'ZF': {'tick_size': 0.0078125, 'tick_value': 7.8125},
    'ZT': {'tick_size': 0.0078125, 'tick_value': 15.625},

    # === CURRENCIES ===
    '6E': {'tick_size': 0.00005, 'tick_value': 6.25},
    '6J': {'tick_size': 0.0000005, 'tick_value': 6.25},
    '6B': {'tick_size': 0.0001, 'tick_value': 6.25},
    '6A': {'tick_size': 0.0001, 'tick_value': 10.0},
    '6C': {'tick_size': 0.00005, 'tick_value': 5.0},
    '6S': {'tick_size': 0.0001, 'tick_value': 12.5},
    '6N': {'tick_size': 0.0001, 'tick_value': 10.0},
    '6M': {'tick_size': 0.00001, 'tick_value': 5.0},
    'DX': {'tick_size': 0.005, 'tick_value': 5.0},

    # === CRYPTO ===
    'BTC': {'tick_size': 5.0, 'tick_value': 25.0},
    'MBT': {'tick_size': 5.0, 'tick_value': 0.5},
    'ETH': {'tick_size': 0.25, 'tick_value': 12.5},
    'MET': {'tick_size': 0.25, 'tick_value': 0.025},

    # === GRAINS ===
    'ZC': {'tick_size': 0.25, 'tick_value': 12.5},
    'ZS': {'tick_size': 0.25, 'tick_value': 12.5},
    'ZW': {'tick_size': 0.25, 'tick_value': 12.5},
    'ZM': {'tick_size': 0.1, 'tick_value': 10.0},
    'ZL': {'tick_size': 0.01, 'tick_value': 6.0},

    # === SOFTS ===
    'KC': {'tick_size': 0.05, 'tick_value': 18.75},
    'CT': {'tick_size': 0.01, 'tick_value': 5.0},
    'SB': {'tick_size': 0.01, 'tick_value': 11.2},
}


def convert_tradingview_to_tradovate_symbol(symbol: str, access_token: str | None = None, demo: bool = True) -> str:
    """
    Convert TradingView symbol (MNQ1!) to Tradovate front-month symbol (MNQZ5).
    Calculates the current front month contract based on today's date.
    """
    if not symbol:
        return symbol
    clean = symbol.strip().upper()
    # Already Tradovate format (no ! suffix)
    if '!' not in clean:
        return clean
    match = re.match(r'^([A-Z]+)\d*!$', clean)
    if not match:
        return clean.replace('!', '')
    root = match.group(1)
    cache_key = (root, demo)
    cached = SYMBOL_CONVERSION_CACHE.get(cache_key)
    if cached:
        value, expires = cached
        if datetime.utcnow() < expires:
            return value
    
    # Calculate front month contract based on current date
    now = datetime.utcnow()
    current_month = now.month
    current_year = now.year % 10  # Last digit of year (e.g., 2025 -> 5)
    
    # Month codes: F=Jan, G=Feb, H=Mar, J=Apr, K=May, M=Jun, N=Jul, Q=Aug, U=Sep, V=Oct, X=Nov, Z=Dec
    month_codes = ['F', 'G', 'H', 'J', 'K', 'M', 'N', 'Q', 'U', 'V', 'X', 'Z']
    
    # For quarterly contracts (MNQ, MES, ES, NQ, etc.), use quarterly months: H(Mar), M(Jun), U(Sep), Z(Dec)
    quarterly_contracts = ['MNQ', 'MES', 'ES', 'NQ', 'YM', 'RTY', 'M2K', 'CL', 'MCL', 'GC', 'MGC']
    
    if root in quarterly_contracts:
        # Quarterly contracts: Find next quarterly expiration
        quarterly_months = [2, 5, 8, 11]  # Mar, Jun, Sep, Dec (0-indexed: 2, 5, 8, 11)
        quarterly_codes = ['H', 'M', 'U', 'Z']
        
        # Find the next quarterly month
        for i, qm in enumerate(quarterly_months):
            if current_month <= qm + 1:  # Add 1 month buffer for rollover
                month_code = quarterly_codes[i]
                year = current_year
                break
        else:
            # Past December, use March of next year
            month_code = 'H'
            year = (current_year + 1) % 10
        
        converted = f"{root}{month_code}{year}"
    else:
        # Monthly contracts: Use current month or next month
        if current_month == 12:
            month_code = 'F'  # January
            year = (current_year + 1) % 10
        else:
            month_code = month_codes[current_month]  # Current month
            year = current_year
        
        converted = f"{root}{month_code}{year}"
    
    logger.info(f"üìÖ Converted {symbol} ‚Üí {converted} (front month for {now.strftime('%Y-%m')})")
    SYMBOL_CONVERSION_CACHE[cache_key] = (converted, datetime.utcnow() + SYMBOL_CACHE_TTL)
    return converted


def extract_symbol_root(symbol: str) -> str:
    if not symbol:
        return ''
    clean = symbol.upper()
    # Strip exchange prefix (e.g., "COMEX:MGC1!" ‚Üí "MGC1!")
    if ':' in clean:
        clean = clean.split(':')[-1]
    clean = clean.replace('1!', '').replace('!', '')

    # Check for known symbols FIRST (matches recorder_service.py approach)
    KNOWN_ROOTS = ['MNQ', 'MES', 'M2K', 'MYM', 'MCL', 'MGC', 'NQ', 'ES', 'YM', 'RTY', 'CL', 'GC', 'SI', 'HG', 'PL', 'NG', 'ZB', 'ZN', 'ZF', 'ZT', 'ZC', 'ZS', 'ZW', 'ZL', 'KC', 'SB', 'CT', 'HE', 'LE', 'SIL']
    for known in sorted(KNOWN_ROOTS, key=len, reverse=True):  # Longest first (MNQ before NQ)
        if clean.startswith(known):
            return known

    # Fallback: remove trailing month code + digits
    clean = re.sub(r'[0-9]+', '', clean)
    clean = re.sub(r'[FGHJKMNQUVXZ]$', '', clean)
    return clean if clean else symbol.upper()


def get_tick_info(symbol: str) -> dict:
    root = extract_symbol_root(symbol)
    return TICK_INFO.get(root, {'tick_size': 0.25, 'tick_value': 1.0})


async def wait_for_position_fill(tradovate, account_id: int, symbol: str, expected_side: str, timeout: float = 10.0):
    """
    Wait for position to appear after order fill.
    Increased timeout to 10 seconds and added better logging.
    """
    expected_side = expected_side.lower()
    symbol_upper = symbol.upper()
    deadline = time.time() + timeout
    attempt = 0
    
    logger.info(f"üîç Waiting for position fill: symbol={symbol_upper}, side={expected_side}, account_id={account_id}")
    
    while time.time() < deadline:
        attempt += 1
        try:
            positions = await tradovate.get_positions(account_id)
            logger.debug(f"  Attempt {attempt}: Found {len(positions)} positions")
            
            for pos in positions:
                pos_symbol = str(pos.get('symbol', '')).upper()
                net_pos = pos.get('netPos') or 0
                
                # Match by root symbol (e.g., "MNQ" matches "MNQZ5", "MNQ1!", etc.)
                symbol_root = symbol_upper[:3] if len(symbol_upper) >= 3 else symbol_upper
                pos_root = pos_symbol[:3] if len(pos_symbol) >= 3 else pos_symbol
                
                if symbol_root == pos_root or symbol_upper in pos_symbol or pos_symbol in symbol_upper:
                    logger.debug(f"  Symbol match: {pos_symbol} (netPos={net_pos})")
                    if (expected_side == 'buy' and net_pos > 0) or (expected_side == 'sell' and net_pos < 0):
                        logger.info(f"‚úÖ Position found: {pos_symbol} {net_pos} @ {pos.get('netPrice', 'N/A')}")
                        return pos
        except Exception as e:
            logger.warning(f"  Error checking positions (attempt {attempt}): {e}")
        
        await asyncio.sleep(0.5)  # Check every 0.5 seconds
    
    logger.warning(f"‚è±Ô∏è Timeout waiting for position: {symbol_upper} (checked {attempt} times)")
    return None


def clamp_price(price: float, tick_size: float) -> float:
    if price is None:
        return None
    ticks = round(price / tick_size)
    clamped = ticks * tick_size
    decimals = max(2, len(str(tick_size).rstrip('0').split('.')[-1]))
    return round(clamped, decimals)


async def apply_risk_orders(tradovate, account_spec: str, account_id: int, symbol: str, entry_side: str, quantity: int, risk_config: dict):
    """
    Apply risk management orders (TP/SL) as OCO (One-Cancels-Other) using Tradovate's order strategy.
    When TP hits, SL is automatically cancelled (and vice versa).
    """
    logger.info(f"üéØ apply_risk_orders called: symbol={symbol}, side={entry_side}, qty={quantity}")
    logger.info(f"üéØ Risk config: {risk_config}")
    
    if not risk_config or not quantity:
        logger.info(f"üéØ No risk config or quantity=0, skipping bracket orders")
        return
    symbol_upper = symbol.upper()
    logger.info(f"üîç Waiting for position fill to calculate TP/SL prices...")
    fill = await wait_for_position_fill(tradovate, account_id, symbol_upper, entry_side)
    if not fill:
        logger.error(f"‚ùå Unable to locate filled position for {symbol_upper} to apply brackets")
        logger.error(f"   This means TP/SL orders will NOT be placed!")
        logger.error(f"   Possible causes: Position not visible yet, symbol mismatch, or timeout")
        return
    entry_price = fill.get('netPrice') or fill.get('price') or fill.get('avgPrice')
    if not entry_price:
        logger.error(f"‚ùå No entry price found for {symbol_upper}; skipping bracket creation")
        logger.error(f"   Position data: {fill}")
        return
    
    logger.info(f"‚úÖ Position found: Entry price = {entry_price}, will calculate TP/SL from here")
    tick_info = get_tick_info(symbol_upper)
    tick_size = tick_info['tick_size']
    is_long = entry_side.lower() == 'buy'
    exit_action = 'Sell' if is_long else 'Buy'

    # Get risk settings
    take_profit_list = risk_config.get('take_profit') or []
    stop_cfg = risk_config.get('stop_loss')
    trail_cfg = risk_config.get('trail')
    
    # Get tick values
    tp_ticks = None
    sl_ticks = None
    
    if take_profit_list:
        first_tp = take_profit_list[0]
        tp_ticks = first_tp.get('gain_ticks')
    
    if stop_cfg:
        sl_ticks = stop_cfg.get('loss_ticks')
    
    # Calculate absolute prices for OCO exit orders
    tp_price = None
    sl_price = None
    
    if tp_ticks:
        tp_offset = tick_size * tp_ticks
        tp_price = entry_price + tp_offset if is_long else entry_price - tp_offset
        tp_price = clamp_price(tp_price, tick_size)
    
    if sl_ticks:
        sl_offset = tick_size * sl_ticks
        sl_price = entry_price - sl_offset if is_long else entry_price + sl_offset
        sl_price = clamp_price(sl_price, tick_size)
    
    # Track order IDs for break-even and trailing stop integration
    tp_order_id = None
    sl_order_id = None
    
    # Detect multi-TP: need individual orders instead of single OCO
    has_multi_tp = len(take_profit_list) > 1

    # Multi-TP path: Place TP Level 1 with correct trim quantity, SL separately (no OCO)
    if has_multi_tp and tp_price:
        trim_units = risk_config.get('trim_units', 'Percent')
        first_tp_trim = take_profit_list[0].get('trim_percent', 100)
        if trim_units == 'Contracts':
            first_tp_qty = min(int(first_tp_trim), quantity) if first_tp_trim else quantity
        else:
            first_tp_qty = int(round(quantity * (first_tp_trim / 100.0))) if first_tp_trim else quantity
        first_tp_qty = max(first_tp_qty, 1)  # At least 1 contract

        # Place TP Level 1
        logger.info(f"üìä Multi-TP: Placing TP Level 1 @ {tp_price}, Qty: {first_tp_qty}/{quantity}")
        tp_order_data = tradovate.create_limit_order(account_spec, symbol_upper, exit_action, first_tp_qty, tp_price, account_id)
        tp_result = await tradovate.place_order(tp_order_data)
        if tp_result and tp_result.get('success'):
            tp_order_id = tp_result.get('orderId') or tp_result.get('data', {}).get('orderId')

        # Place SL for full position
        if sl_price:
            logger.info(f"üìä Multi-TP: Placing SL @ {sl_price}, Qty: {quantity}")
            sl_order_data = tradovate.create_stop_order(account_spec, symbol_upper, exit_action, quantity, sl_price, account_id)
            sl_result = await tradovate.place_order(sl_order_data)
            if sl_result and sl_result.get('success'):
                sl_order_id = sl_result.get('orderId') or sl_result.get('data', {}).get('orderId')

    # Single TP: If we have BOTH TP and SL, try to place as OCO order strategy
    elif tp_price and sl_price:
        logger.info(f"üìä Placing OCO exit orders: TP @ {tp_price}, SL @ {sl_price}, Qty: {quantity}")
        logger.info(f"   Entry: {entry_price}, TP ticks: {tp_ticks}, SL ticks: {sl_ticks}")
        
        # Use the new OCO exit method
        result = await tradovate.place_exit_oco(
            account_id=account_id,
            account_spec=account_spec,
            symbol=symbol_upper,
            exit_side=exit_action,
            quantity=quantity,
            take_profit_price=tp_price,
            stop_loss_price=sl_price
        )
        
        if result and result.get('success'):
            logger.info(f"‚úÖ OCO exit orders placed successfully")
            
            # Register the pair for custom OCO monitoring (if they were placed as individual orders)
            tp_order_id = result.get('tp_order_id')
            sl_order_id = result.get('sl_order_id')
            
            if tp_order_id and sl_order_id:
                register_oco_pair(tp_order_id, sl_order_id, account_id, symbol_upper)
        else:
            logger.warning(f"‚ö†Ô∏è OCO exit failed, orders may have been placed individually: {result}")
            # Fallback: Place orders individually and register as OCO pair
            # This ensures OCO behavior even if Tradovate's OCO strategy fails
            if tp_price:
                logger.info(f"üìä Placing TP individually @ {tp_price}, Qty: {quantity}")
                tp_order_data = tradovate.create_limit_order(account_spec, symbol_upper, exit_action, quantity, tp_price, account_id)
                tp_result = await tradovate.place_order(tp_order_data)
                if tp_result and tp_result.get('success'):
                    tp_order_id = tp_result.get('orderId') or tp_result.get('data', {}).get('orderId')
            
            if sl_price:
                logger.info(f"üìä Placing SL individually @ {sl_price}, Qty: {quantity}")
                sl_order_data = tradovate.create_stop_order(account_spec, symbol_upper, exit_action, quantity, sl_price, account_id)
                sl_result = await tradovate.place_order(sl_order_data)
                if sl_result and sl_result.get('success'):
                    sl_order_id = sl_result.get('orderId') or sl_result.get('data', {}).get('orderId')
            
            # Register as OCO pair if both were placed
            if tp_order_id and sl_order_id:
                register_oco_pair(tp_order_id, sl_order_id, account_id, symbol_upper)
                logger.info(f"üîó OCO pair registered (fallback): TP={tp_order_id} <-> SL={sl_order_id}")
    
    # If only TP (no SL)
    elif tp_price:
        logger.info(f"üìä Placing Take Profit only @ {tp_price}, Qty: {quantity}")
        tp_order_data = tradovate.create_limit_order(account_spec, symbol_upper, exit_action, quantity, tp_price, account_id)
        tp_result = await tradovate.place_order(tp_order_data)
        if tp_result and tp_result.get('success'):
            tp_order_id = tp_result.get('orderId') or tp_result.get('data', {}).get('orderId')
    
    # If only SL (no TP)
    elif sl_price:
        logger.info(f"üìä Placing Stop Loss only @ {sl_price}, Qty: {quantity}")
        sl_order_data = tradovate.create_stop_order(account_spec, symbol_upper, exit_action, quantity, sl_price, account_id)
        sl_result = await tradovate.place_order(sl_order_data)
        if sl_result and sl_result.get('success'):
            sl_order_id = sl_result.get('orderId') or sl_result.get('data', {}).get('orderId')
    
    # Handle trailing stop (can be used with or instead of fixed SL)
    if trail_cfg and trail_cfg.get('offset_ticks'):
        trail_ticks = trail_cfg.get('offset_ticks')
        trail_offset = tick_size * trail_ticks
        
        # Calculate initial stop price (entry - offset for long, entry + offset for short)
        if is_long:
            initial_stop_price = entry_price - trail_offset
        else:
            initial_stop_price = entry_price + trail_offset
        initial_stop_price = clamp_price(initial_stop_price, tick_size)
        
        logger.info(f"üìä Placing Trailing Stop: offset={trail_offset} ({trail_ticks} ticks), initial stop={initial_stop_price}")
        trail_order = tradovate.create_trailing_stop_order(
            account_spec, symbol_upper, exit_action, quantity, 
            float(trail_offset), account_id, initial_stop_price
        )
        trail_result = await tradovate.place_order(trail_order)
        
        if trail_result and trail_result.get('success'):
            trail_order_id = trail_result.get('orderId') or trail_result.get('data', {}).get('orderId')
            logger.info(f"‚úÖ Trailing Stop placed: Order ID={trail_order_id}")
            
            # CRITICAL: Register trailing stop with TP for OCO (one cancels the other)
            if tp_order_id and trail_order_id:
                register_oco_pair(tp_order_id, trail_order_id, account_id, symbol_upper)
                logger.info(f"üîó Trailing Stop and TP registered as OCO: Trail={trail_order_id} <-> TP={tp_order_id}")
            
            # If we placed a trailing stop AND a fixed SL, register them for OCO (they're alternatives)
            if sl_order_id and trail_order_id:
                # The trailing stop and fixed SL are alternatives - register as OCO
                register_oco_pair(trail_order_id, sl_order_id, account_id, symbol_upper)
                logger.info(f"üîó Trailing Stop and SL registered as OCO: Trail={trail_order_id} <-> SL={sl_order_id}")
        else:
            error_msg = trail_result.get('error', 'Unknown error') if trail_result else 'No response'
            logger.warning(f"‚ö†Ô∏è Failed to place trailing stop: {error_msg}")
    
    # Handle break-even (monitor position and move SL to entry when profitable)
    break_even_cfg = risk_config.get('break_even')
    if break_even_cfg and break_even_cfg.get('activation_ticks'):
        be_ticks = break_even_cfg.get('activation_ticks')
        logger.info(f"üìä Break-even enabled: Will move SL to entry after {be_ticks} ticks profit")
        
        # Register for break-even monitoring
        register_break_even_monitor(
            account_id=account_id,
            symbol=symbol_upper,
            entry_price=entry_price,
            is_long=is_long,
            activation_ticks=be_ticks,
            tick_size=tick_size,
            sl_order_id=sl_order_id,  # We'll modify this order
            quantity=quantity,
            account_spec=account_spec
        )
    
    # Handle multiple TP levels (if any beyond the first)
    last_tp_order_id = tp_order_id  # Track the last TP placed (starts with TP1)
    all_tp_order_ids = [tp_order_id] if tp_order_id else []  # Track all TPs for SL cleanup
    if len(take_profit_list) > 1:
        trim_units = risk_config.get('trim_units', 'Percent')
        logger.info(f"üìä Processing {len(take_profit_list) - 1} additional TP levels (trim_units={trim_units})")
        first_tp_trim = take_profit_list[0].get('trim_percent', 100)
        if trim_units == 'Contracts':
            first_tp_qty = min(int(first_tp_trim), quantity) if first_tp_trim else quantity
        else:
            first_tp_qty = int(round(quantity * (first_tp_trim / 100.0))) if first_tp_trim else quantity
        remaining_qty = quantity - first_tp_qty

        for idx, tp in enumerate(take_profit_list[1:], start=1):
            ticks = tp.get('gain_ticks')
            trim_value = tp.get('trim_percent', 0)

            if trim_units == 'Contracts':
                level_qty = min(int(trim_value), remaining_qty) if trim_value else 0
            else:
                level_qty = int(round(quantity * (trim_value / 100.0))) if trim_value else 0
            if idx == len(take_profit_list) - 1 and level_qty == 0:
                level_qty = remaining_qty  # Last level gets remaining

            level_qty = min(max(level_qty, 0), remaining_qty)
            if level_qty <= 0:
                continue

            remaining_qty -= level_qty

            if ticks:
                tp_offset = tick_size * ticks
                level_price = entry_price + tp_offset if is_long else entry_price - tp_offset
                level_price = clamp_price(level_price, tick_size)

                logger.info(f"  TP Level {idx + 1}: Price={level_price}, Qty={level_qty}")
                tp_order = tradovate.create_limit_order(account_spec, symbol_upper, exit_action, level_qty, level_price, account_id)
                level_result = await tradovate.place_order(tp_order)
                if level_result and level_result.get('success'):
                    level_order_id = level_result.get('orderId') or level_result.get('data', {}).get('orderId')
                    if level_order_id:
                        last_tp_order_id = level_order_id
                        all_tp_order_ids.append(level_order_id)

        # Register LAST TP + SL as OCO pair (last TP fills = position fully closed = cancel SL)
        # Only the last TP triggers SL cancellation; intermediate TPs do NOT cancel SL
        if last_tp_order_id and sl_order_id:
            register_oco_pair(last_tp_order_id, sl_order_id, account_id, symbol_upper)
            logger.info(f"üîó Multi-TP OCO: Last TP={last_tp_order_id} <-> SL={sl_order_id} (SL cancels when last TP fills)")
            # Store all TP IDs so SL fill can cancel them all via orphan cleanup
            if len(all_tp_order_ids) > 1:
                logger.info(f"üîó Multi-TP: All TP orders tracked: {all_tp_order_ids}")


def normalize_symbol(symbol: str) -> str:
    """Normalize symbol for comparison (remove !, handle different formats)"""
    if not symbol:
        return ''
    normalized = symbol.upper().strip()
    # Remove trailing ! (TradingView format)
    normalized = normalized.rstrip('!')
    return normalized

async def cancel_open_orders(tradovate, account_id: int, symbol: str | None = None, cancel_all: bool = False):
    cancelled = 0
    try:
        # Try getting all orders first (includes order strategies), then filter by account
        # This ensures we get bracket orders, OCO orders, etc. that might not show up in account-specific endpoint
        all_orders = await tradovate.get_orders(None) or []  # Get all orders
        orders = [o for o in all_orders if str(o.get('accountId', '')) == str(account_id)]
        
        # If we got no orders from /order/list, fallback to account-specific endpoint
        if not orders:
            logger.info(f"No orders found via /order/list, trying account-specific endpoint")
            orders = await tradovate.get_orders(str(account_id)) or []
        logger.info(f"Retrieved {len(orders)} orders for account {account_id}, filtering for symbol: {symbol}, cancel_all: {cancel_all}")
        
        # Log all orders for debugging
        if orders:
            logger.info(f"=== ALL ORDERS RETRIEVED ===")
            for idx, order in enumerate(orders[:10]):  # Log first 10 orders
                logger.info(f"Order #{idx+1}: id={order.get('id')}, status={order.get('status')}, ordStatus={order.get('ordStatus')}, "
                           f"symbol={order.get('symbol')}, contractId={order.get('contractId')}, "
                           f"orderType={order.get('orderType')}, orderQty={order.get('orderQty')}, "
                           f"action={order.get('action')}, strategyId={order.get('orderStrategyId')}, "
                           f"keys={list(order.keys())[:15]}")
            if len(orders) > 10:
                logger.info(f"... and {len(orders) - 10} more orders")
            logger.info(f"=== END ORDER LIST ===")
        
        # Statuses that represent active/resting orders that can be cancelled
        # According to Tradovate docs, statuses are: Working, Filled, Canceled, Rejected, Expired
        # Also check: PendingNew, PendingReplace, PendingCancel, Stopped, Suspended
        # We check both lowercase and capitalized versions for robustness
        cancellable_statuses = {
            'working', 'pending', 'queued', 'accepted', 'new', 
            'pendingnew', 'pendingreplace', 'pendingcancel',
            'stopped', 'suspended',
            # Capitalized versions (Tradovate standard format)
            'Working', 'Pending', 'Queued', 'Accepted', 'New',
            'PendingNew', 'PendingReplace', 'PendingCancel',
            'Stopped', 'Suspended'
        }
        
        # Normalize target symbol if provided
        target_symbol_normalized = None
        target_contract_ids = set()
        if symbol:
            target_symbol_normalized = normalize_symbol(symbol)
            logger.info(f"Target symbol normalized: {target_symbol_normalized}")
        
        # Resolve target symbol to contractId(s) if we have a symbol to match
        if symbol and target_symbol_normalized:
            try:
                # Get positions to find matching contractIds
                positions = await tradovate.get_positions(account_id)
                for pos in positions:
                    pos_symbol = str(pos.get('symbol') or '').upper()
                    if normalize_symbol(pos_symbol) == target_symbol_normalized:
                        contract_id = pos.get('contractId')
                        if contract_id:
                            target_contract_ids.add(contract_id)
                            logger.info(f"Found matching contractId {contract_id} for symbol {target_symbol_normalized}")
            except Exception as e:
                logger.warning(f"Error resolving contractIds for symbol matching: {e}")
        
        for order in orders:
            if not order:
                continue
            
            # Check both 'status' and 'ordStatus' fields (Tradovate uses ordStatus per docs)
            # Also check 'action' which sometimes indicates buy/sell (meaning order is active)
            status = order.get('ordStatus') or order.get('status') or ''
            status_lower = status.lower() if status else ''
            order_id = order.get('id')
            order_type = order.get('orderType') or order.get('order_type') or 'Unknown'
            order_strategy_id = order.get('orderStrategyId')  # For bracket/OCO orders
            order_action = order.get('action')  # Buy/Sell indicates active order
            
            # Non-cancellable final statuses (order is already complete)
            non_cancellable = {'filled', 'canceled', 'cancelled', 'rejected', 'expired', 'complete', 'completed'}
            
            # Skip if status indicates order is already done
            if status_lower and status_lower in non_cancellable:
                logger.debug(f"Skipping order {order_id} - status '{status}' is final (not cancellable)")
                continue
            
            # If status is empty but order has action (Buy/Sell), it's likely an active order
            if not status and not order_action:
                # If no status and no action, check if it has position-related fields (might be position data, not order)
                if 'netPos' in order:
                    logger.debug(f"Skipping order {order_id} - appears to be position data, not order")
                    continue
            
            # Log what we're about to try to cancel
            logger.info(f"Order {order_id} may be active: status='{status}', ordStatus='{order.get('ordStatus')}', action={order_action}, strategyId={order_strategy_id}")
            
            # Get symbol from order - could be direct symbol field or need to resolve from contractId
            order_symbol = str(order.get('symbol') or '').upper()
            order_contract_id = order.get('contractId')
            
            # Resolve contractId to symbol if we don't have symbol
            if not order_symbol and order_contract_id:
                try:
                    resolved_symbol = await tradovate._get_contract_symbol(order_contract_id)
                    if resolved_symbol:
                        order_symbol = resolved_symbol.upper()
                        order['symbol'] = resolved_symbol  # Cache it for future use
                        logger.debug(f"Resolved contractId {order_contract_id} to symbol {order_symbol}")
                except Exception as e:
                    logger.debug(f"Could not resolve contractId {order_contract_id}: {e}")
            
            # Filter by symbol if provided - try multiple matching strategies
            should_cancel = True
            if cancel_all:
                # Cancel all cancellable orders regardless of symbol
                should_cancel = True
                logger.info(f"Cancel-all mode: Will cancel order {order_id} ({order_symbol or f'contractId:{order_contract_id}' or 'no symbol'}, {order_type}, status: {status})")
            elif symbol:
                should_cancel = False
                
                # Strategy 1: Exact symbol match (after normalization)
                if order_symbol:
                    order_symbol_normalized = normalize_symbol(order_symbol)
                    if order_symbol_normalized == target_symbol_normalized:
                        should_cancel = True
                        logger.info(f"Order {order_id} matches by normalized symbol: {order_symbol} -> {order_symbol_normalized}")
                
                # Strategy 2: ContractId match
                if not should_cancel and order_contract_id and order_contract_id in target_contract_ids:
                    should_cancel = True
                    logger.info(f"Order {order_id} matches by contractId: {order_contract_id}")
                
                # Strategy 3: Partial symbol match (in case of format differences)
                if not should_cancel and order_symbol:
                    # Try matching base symbol (e.g., "ES" in "ESM1" or "ES1!")
                    order_base = normalize_symbol(order_symbol)
                    target_base = target_symbol_normalized
                    # Extract base symbol (remove month codes and numbers)
                    order_base_only = re.sub(r'\d+[A-Z]*$', '', order_base)
                    target_base_only = re.sub(r'\d+[A-Z]*$', '', target_base)
                    if order_base_only and target_base_only and order_base_only == target_base_only:
                        # If base matches and one contains the other, it's likely a match
                        if target_base_only in order_base or order_base_only in target_base:
                            should_cancel = True
                            logger.info(f"Order {order_id} matches by base symbol: {order_base} vs {target_base}")
                
                if not should_cancel:
                    logger.debug(f"Skipping order {order_id} ({order_symbol or 'no symbol'}) - doesn't match {symbol}")
                    continue
            
            # Attempt to cancel the order
            logger.info(f"Attempting to cancel order {order_id} ({order_symbol or f'contractId:{order_contract_id}' or 'no symbol'}, {order_type}, status: {status})")
            if await tradovate.cancel_order(order_id):
                cancelled += 1
                logger.info(f"‚úÖ Successfully cancelled order {order_id} ({order_symbol or 'no symbol'})")
            else:
                logger.warning(f"‚ùå Failed to cancel order {order_id} ({order_symbol or 'no symbol'})")
                
    except Exception as e:
        logger.error(f"Unable to cancel open orders for {symbol or 'account'}: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    logger.info(f"Total cancelled: {cancelled} orders for symbol {symbol or 'all'}")
    return cancelled


def get_tick_size(symbol):
    """Get tick size for a given symbol"""
    if not symbol:
        return 0.25  # Default
    
    symbol_upper = symbol.upper()
    
    # Micro futures
    if 'MNQ' in symbol_upper:
        return 0.25
    elif 'MES' in symbol_upper:
        return 0.25
    elif 'MYM' in symbol_upper:
        return 1.0
    elif 'M2K' in symbol_upper:
        return 0.10
    elif 'MCL' in symbol_upper:
        return 0.01
    elif 'MGC' in symbol_upper:
        return 0.10
    # E-mini futures
    elif 'NQ' in symbol_upper:
        return 0.25
    elif 'ES' in symbol_upper:
        return 0.25
    elif 'YM' in symbol_upper:
        return 1.0
    elif 'RTY' in symbol_upper:
        return 0.10
    # Commodities
    elif 'CL' in symbol_upper:
        return 0.01
    elif 'GC' in symbol_upper:
        return 0.10
    elif 'SI' in symbol_upper:
        return 0.005
    elif 'NG' in symbol_upper:
        return 0.001
    # Currencies
    elif '6E' in symbol_upper or 'EUR' in symbol_upper:
        return 0.00005
    elif '6J' in symbol_upper or 'JPY' in symbol_upper:
        return 0.0000005
    elif '6B' in symbol_upper or 'GBP' in symbol_upper:
        return 0.0001
    # Default
    else:
        return 0.25


def get_tick_value(symbol):
    """Get tick value (dollar value per tick) for a given symbol"""
    if not symbol:
        return 0.50  # Default for micro futures
    
    symbol_upper = symbol.upper()
    
    # Micro futures (smaller tick values)
    if 'MNQ' in symbol_upper:
        return 0.50   # $0.50 per tick (0.25 points)
    elif 'MES' in symbol_upper:
        return 1.25   # $1.25 per tick (0.25 points)
    elif 'MYM' in symbol_upper:
        return 0.50   # $0.50 per tick (1 point)
    elif 'M2K' in symbol_upper:
        return 0.50   # $0.50 per tick (0.10 points)
    elif 'MCL' in symbol_upper:
        return 1.00   # $1.00 per tick (0.01)
    elif 'MGC' in symbol_upper:
        return 1.00   # $1.00 per tick (0.10)
    # E-mini futures (larger tick values)
    elif 'NQ' in symbol_upper:
        return 5.00   # $5.00 per tick (0.25 points)
    elif 'ES' in symbol_upper:
        return 12.50  # $12.50 per tick (0.25 points)
    elif 'YM' in symbol_upper:
        return 5.00   # $5.00 per tick (1 point)
    elif 'RTY' in symbol_upper:
        return 5.00   # $5.00 per tick (0.10 points)
    # Commodities
    elif 'CL' in symbol_upper:
        return 10.00  # $10.00 per tick (0.01)
    elif 'GC' in symbol_upper:
        return 10.00  # $10.00 per tick (0.10)
    elif 'SI' in symbol_upper:
        return 25.00  # $25.00 per tick
    elif 'NG' in symbol_upper:
        return 10.00  # $10.00 per tick
    # Currencies
    elif '6E' in symbol_upper or 'EUR' in symbol_upper:
        return 6.25   # $6.25 per tick
    elif '6J' in symbol_upper or 'JPY' in symbol_upper:
        return 6.25
    elif '6B' in symbol_upper or 'GBP' in symbol_upper:
        return 6.25
    # Default
    else:
        return 0.50


def init_db():
    """Initialize webhook and strategy tables - supports both SQLite and PostgreSQL"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Check if using PostgreSQL
    is_postgres = is_using_postgres()
    
    if is_postgres:
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS webhooks (
                id SERIAL PRIMARY KEY,
                url TEXT NOT NULL,
                method TEXT NOT NULL,
                headers TEXT,
                body TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS strategy_pnl_history (
                id SERIAL PRIMARY KEY,
                strategy_id INTEGER,
                strategy_name TEXT,
                pnl REAL,
                drawdown REAL DEFAULT 0.0,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
    else:
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS webhooks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT NOT NULL,
                method TEXT NOT NULL,
                headers TEXT,
                body TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS strategy_pnl_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                strategy_id INTEGER,
                strategy_name TEXT,
                pnl REAL,
                drawdown REAL DEFAULT 0.0,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_strategy_pnl_timestamp 
            ON strategy_pnl_history(strategy_id, timestamp)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_strategy_pnl_date 
            ON strategy_pnl_history(DATE(timestamp))
        ''')
    conn.commit()
    conn.close()
    
    # Initialize just_trades.db with positions table (like Trade Manager)
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS open_positions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            account_id INTEGER NOT NULL,
            subaccount_id TEXT,
            account_name TEXT,
            symbol TEXT NOT NULL,
            quantity REAL NOT NULL,
            avg_price REAL NOT NULL,
            current_price REAL DEFAULT 0.0,
            unrealized_pnl REAL DEFAULT 0.0,
            order_id TEXT,
            strategy_name TEXT,
            direction TEXT,  -- 'BUY' or 'SELL'
            open_time DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(account_id, subaccount_id, symbol)
        )
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_open_positions_account 
        ON open_positions(account_id, subaccount_id)
    ''')
    
    # Recorders table - stores recorder configurations
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorders (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            strategy_type TEXT DEFAULT 'Futures',
            symbol TEXT,
            demo_account_id TEXT,
            account_id INTEGER,
            -- Positional Settings
            initial_position_size INTEGER DEFAULT 2,
            add_position_size INTEGER DEFAULT 2,
            -- TP Settings
            tp_units TEXT DEFAULT 'Ticks',
            trim_units TEXT DEFAULT 'Contracts',
            tp_targets TEXT DEFAULT '[]',
            -- SL Settings
            sl_enabled INTEGER DEFAULT 0,
            sl_amount REAL DEFAULT 0,
            sl_units TEXT DEFAULT 'Ticks',
            sl_type TEXT DEFAULT 'Fixed',
            -- Averaging Down
            avg_down_enabled INTEGER DEFAULT 0,
            avg_down_amount INTEGER DEFAULT 1,
            avg_down_point REAL DEFAULT 0,
            avg_down_units TEXT DEFAULT 'Ticks',
            -- Filter Settings
            add_delay INTEGER DEFAULT 1,
            max_contracts_per_trade INTEGER DEFAULT 0,
            option_premium_filter REAL DEFAULT 0,
            direction_filter TEXT,
            -- Time Filters
            time_filter_1_start TEXT DEFAULT '',
            time_filter_1_stop TEXT DEFAULT '',
            time_filter_2_start TEXT DEFAULT '',
            time_filter_2_stop TEXT DEFAULT '',
            -- Execution Controls
            signal_cooldown INTEGER DEFAULT 0,
            max_signals_per_session INTEGER DEFAULT 0,
            max_daily_loss REAL DEFAULT 0,
            auto_flat_after_cutoff INTEGER DEFAULT 0,
            -- Miscellaneous
            notes TEXT,
            -- Recording Status
            recording_enabled INTEGER DEFAULT 1,
            is_recording INTEGER DEFAULT 0,
            -- Webhook
            webhook_token TEXT,
            signal_count INTEGER DEFAULT 0,
            -- Timestamps
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_recorders_name 
        ON recorders(name)
    ''')
    
    # Recorded trades table - stores individual trade executions from signals
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorded_trades (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            recorder_id INTEGER NOT NULL,
            signal_id INTEGER,
            ticker TEXT NOT NULL,
            action TEXT NOT NULL,
            side TEXT NOT NULL,
            entry_price REAL,
            entry_time DATETIME,
            exit_price REAL,
            exit_time DATETIME,
            quantity INTEGER DEFAULT 1,
            pnl REAL DEFAULT 0,
            pnl_ticks REAL DEFAULT 0,
            fees REAL DEFAULT 0,
            status TEXT DEFAULT 'open',
            exit_reason TEXT,
            notes TEXT,
            -- TP/SL tracking
            tp_price REAL,
            sl_price REAL,
            tp_ticks REAL,
            sl_ticks REAL,
            max_favorable REAL DEFAULT 0,
            max_adverse REAL DEFAULT 0,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (recorder_id) REFERENCES recorders(id),
            FOREIGN KEY (signal_id) REFERENCES recorded_signals(id)
        )
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_recorded_trades_recorder 
        ON recorded_trades(recorder_id)
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_recorded_trades_status 
        ON recorded_trades(status)
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_recorded_trades_entry_time 
        ON recorded_trades(entry_time)
    ''')
    
    # Recorder positions table - combines DCA entries into single position for drawdown tracking
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS recorder_positions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            recorder_id INTEGER NOT NULL,
            ticker TEXT NOT NULL,
            side TEXT NOT NULL,
            total_quantity INTEGER DEFAULT 0,
            avg_entry_price REAL,
            entries TEXT,
            current_price REAL,
            unrealized_pnl REAL DEFAULT 0,
            worst_unrealized_pnl REAL DEFAULT 0,
            best_unrealized_pnl REAL DEFAULT 0,
            exit_price REAL,
            realized_pnl REAL,
            status TEXT DEFAULT 'open',
            opened_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            closed_at DATETIME,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (recorder_id) REFERENCES recorders(id)
        )
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_recorder_positions_recorder 
        ON recorder_positions(recorder_id)
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_recorder_positions_status 
        ON recorder_positions(status)
    ''')
    
    # Traders table - links recorders to accounts for live trading
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS traders (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            recorder_id INTEGER NOT NULL,
            account_id INTEGER NOT NULL,
            enabled INTEGER DEFAULT 1,
            -- Risk settings (copied from recorder, can be customized per trader)
            initial_position_size INTEGER DEFAULT 2,
            add_position_size INTEGER DEFAULT 2,
            tp_targets TEXT DEFAULT '[]',
            sl_enabled INTEGER DEFAULT 0,
            sl_amount REAL DEFAULT 0,
            sl_units TEXT DEFAULT 'Ticks',
            max_daily_loss REAL DEFAULT 500,
            -- Subaccount info
            subaccount_id INTEGER,
            subaccount_name TEXT,
            is_demo INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (recorder_id) REFERENCES recorders(id) ON DELETE CASCADE,
            FOREIGN KEY (account_id) REFERENCES accounts(id) ON DELETE CASCADE,
            UNIQUE(recorder_id, account_id, subaccount_id)
        )
    ''')
    
    # Add risk settings columns to existing traders table (for existing databases)
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS initial_position_size INTEGER DEFAULT 2')
    except:
        pass  # Column already exists
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS add_position_size INTEGER DEFAULT 2')
    except:
        pass
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS tp_targets TEXT DEFAULT "[]"')
    except:
        pass
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS sl_enabled INTEGER DEFAULT 0')
    except:
        pass
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS sl_amount REAL DEFAULT 0')
    except:
        pass
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS sl_units TEXT DEFAULT "Ticks"')
    except:
        pass
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS max_daily_loss REAL DEFAULT 500')
    except:
        pass
    
    # Add sl_type, break-even, avg-down, tp/trim units to traders (Jan 2026 fix)
    # PostgreSQL needs single quotes for string defaults, SQLite uses double quotes
    if is_postgres:
        text_columns = [
            ('sl_type', "TEXT DEFAULT 'Fixed'"),
            ('avg_down_units', "TEXT DEFAULT 'Ticks'"),
            ('tp_units', "TEXT DEFAULT 'Ticks'"),
            ('trim_units', "TEXT DEFAULT 'Contracts'"),
        ]
    else:
        text_columns = [
            ('sl_type', 'TEXT DEFAULT "Fixed"'),
            ('avg_down_units', 'TEXT DEFAULT "Ticks"'),
            ('tp_units', 'TEXT DEFAULT "Ticks"'),
            ('trim_units', 'TEXT DEFAULT "Contracts"'),
        ]
    
    # Non-text columns (same syntax for both)
    other_columns = [
        ('break_even_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ('break_even_ticks', 'INTEGER DEFAULT 10'),
        ('avg_down_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ('avg_down_amount', 'INTEGER DEFAULT 0'),
        ('avg_down_point', 'REAL DEFAULT 0'),
    ]
    
    for col_name, col_type in text_columns + other_columns:
        try:
            cursor.execute(f'ALTER TABLE traders ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
        except:
            pass  # Column already exists
    
    # Add time filter columns to traders table (Dec 2025)
    if is_postgres:
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_1_enabled BOOLEAN DEFAULT FALSE")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_1_start TEXT DEFAULT ''")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_1_stop TEXT DEFAULT ''")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_2_enabled BOOLEAN DEFAULT FALSE")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_2_start TEXT DEFAULT ''")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_2_stop TEXT DEFAULT ''")
        except:
            pass
    else:
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_1_enabled INTEGER DEFAULT 0")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_1_start TEXT DEFAULT ''")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_1_stop TEXT DEFAULT ''")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_2_enabled INTEGER DEFAULT 0")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_2_start TEXT DEFAULT ''")
        except:
            pass
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS time_filter_2_stop TEXT DEFAULT ''")
        except:
            pass

    # Jan 2026: Add new webhook feature columns to traders
    # trail_trigger - Profit ticks before trailing stop starts (0 = immediate)
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS trail_trigger INTEGER DEFAULT 0')
    except:
        pass

    # trail_freq - How often to update trailing stop (0 = every tick)
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS trail_freq INTEGER DEFAULT 0')
    except:
        pass

    # break_even_offset - How many ticks of profit to lock in (0 = true breakeven)
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS break_even_offset INTEGER DEFAULT 0')
    except:
        pass

    # Jan 2026: Add signal delay columns to traders table
    # add_delay - Take every Nth signal (1 = all, 2 = every other, 3 = every 3rd, etc.)
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS add_delay INTEGER DEFAULT 1')
        logger.info("‚úÖ Added add_delay column to traders table")
    except:
        pass  # Column already exists

    # signal_count - Track how many signals this trader has received (for Nth signal filtering)
    try:
        cursor.execute('ALTER TABLE traders ADD COLUMN IF NOT EXISTS signal_count INTEGER DEFAULT 0')
        logger.info("‚úÖ Added signal_count column to traders table")
    except:
        pass  # Column already exists

    # dca_enabled - When ON, trader is in DCA mode (always treat same-direction as add to position)
    # This is a PROTECTED setting - controls DCA logic directly without code detection
    if is_postgres:
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS dca_enabled BOOLEAN DEFAULT FALSE")
            logger.info("‚úÖ Added dca_enabled column to traders table (PostgreSQL)")
        except:
            pass  # Column already exists
    else:
        try:
            cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS dca_enabled INTEGER DEFAULT 0")
            logger.info("‚úÖ Added dca_enabled column to traders table (SQLite)")
        except:
            pass  # Column already exists

    # Per-trader execution tracking (Feb 2026) ‚Äî isolates cooldown/max_signals per trader
    try:
        cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS last_trade_time TEXT")
    except:
        pass
    try:
        cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS today_signal_count INTEGER DEFAULT 0")
    except:
        pass
    try:
        cursor.execute("ALTER TABLE traders ADD COLUMN IF NOT EXISTS today_signal_date TEXT DEFAULT ''")
    except:
        pass

    # Add inverse_signals column to recorders table (Jan 2026)
    if is_postgres:
        try:
            cursor.execute("ALTER TABLE recorders ADD COLUMN IF NOT EXISTS inverse_signals BOOLEAN DEFAULT FALSE")
            logger.info("‚úÖ Added inverse_signals column to recorders table (PostgreSQL)")
        except:
            pass  # Column already exists
    else:
        try:
            cursor.execute("ALTER TABLE recorders ADD COLUMN IF NOT EXISTS inverse_signals INTEGER DEFAULT 0")
            logger.info("‚úÖ Added inverse_signals column to recorders table (SQLite)")
        except:
            pass  # Column already exists
    
    # Add inverse_signals column to strategies table (Jan 2026)
    if is_postgres:
        try:
            cursor.execute("ALTER TABLE strategies ADD COLUMN IF NOT EXISTS inverse_signals BOOLEAN DEFAULT FALSE")
            logger.info("‚úÖ Added inverse_signals column to strategies table (PostgreSQL)")
        except:
            pass  # Column already exists
    else:
        try:
            cursor.execute("ALTER TABLE strategies ADD COLUMN IF NOT EXISTS inverse_signals INTEGER DEFAULT 0")
            logger.info("‚úÖ Added inverse_signals column to strategies table (SQLite)")
        except:
            pass  # Column already exists
    
    # Add shared strategy columns to existing strategies table (for existing databases)
    try:
        cursor.execute('ALTER TABLE strategies ADD COLUMN IF NOT EXISTS is_public INTEGER DEFAULT 0')
    except:
        pass  # Column already exists
    try:
        cursor.execute('ALTER TABLE strategies ADD COLUMN IF NOT EXISTS created_by_username TEXT')
    except:
        pass  # Column already exists
    
    # Add time filter enabled columns to recorders table (Dec 2025)
    # Use BOOLEAN for PostgreSQL, INTEGER for SQLite
    if is_postgres:
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS time_filter_1_enabled BOOLEAN DEFAULT FALSE')
        except:
            pass  # Column already exists
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS time_filter_2_enabled BOOLEAN DEFAULT FALSE')
        except:
            pass  # Column already exists
    else:
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS time_filter_1_enabled INTEGER DEFAULT 0')
        except:
            pass  # Column already exists
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS time_filter_2_enabled INTEGER DEFAULT 0')
        except:
            pass  # Column already exists
    
    # Add is_private column to recorders table (Dec 2025)
    # Defaults to FALSE so all existing recorders are public
    if is_postgres:
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS is_private BOOLEAN DEFAULT FALSE')
        except:
            pass  # Column already exists
    else:
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS is_private INTEGER DEFAULT 0')
        except:
            pass  # Column already exists

    # Add required_tier column to recorders table (Feb 2026)
    # Allows admin to restrict a recorder to premium/elite tier users
    if is_postgres:
        try:
            cursor.execute("ALTER TABLE recorders ADD COLUMN IF NOT EXISTS required_tier VARCHAR(20) DEFAULT 'public'")
        except:
            pass  # Column already exists
    else:
        try:
            cursor.execute("ALTER TABLE recorders ADD COLUMN IF NOT EXISTS required_tier TEXT DEFAULT 'public'")
        except:
            pass  # Column already exists

    # Jan 2026: Add new webhook feature columns to recorders
    # same_direction_ignore - Block duplicate signals in same direction
    if is_postgres:
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS same_direction_ignore BOOLEAN DEFAULT FALSE')
        except:
            pass
    else:
        try:
            cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS same_direction_ignore INTEGER DEFAULT 0')
        except:
            pass

    # break_even_offset - How many ticks of profit to lock in (0 = true breakeven)
    try:
        cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS break_even_offset INTEGER DEFAULT 0')
    except:
        pass

    # trail_trigger - Profit ticks before trailing stop starts (0 = immediate)
    try:
        cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS trail_trigger INTEGER DEFAULT 0')
    except:
        pass

    # trail_freq - How often to update trailing stop (0 = every tick)
    try:
        cursor.execute('ALTER TABLE recorders ADD COLUMN IF NOT EXISTS trail_freq INTEGER DEFAULT 0')
    except:
        pass

    # Jan 2026: Add unique constraint on traders to prevent duplicates
    # First run cleanup, then add constraint
    try:
        if is_postgres:
            # For PostgreSQL, create a unique index that handles NULLs properly
            cursor.execute('''
                CREATE UNIQUE INDEX IF NOT EXISTS idx_traders_unique_combo
                ON traders(recorder_id, account_id, subaccount_id)
                WHERE subaccount_id IS NOT NULL
            ''')
        else:
            # SQLite doesn't support WHERE clause in CREATE UNIQUE INDEX, so use a regular unique index
            cursor.execute('''
                CREATE UNIQUE INDEX IF NOT EXISTS idx_traders_unique_combo
                ON traders(recorder_id, account_id, subaccount_id)
            ''')
    except Exception as e:
        logger.warning(f"Could not create unique index on traders (may have duplicates): {e}")

    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_traders_recorder
        ON traders(recorder_id)
    ''')
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_traders_account
        ON traders(account_id)
    ''')

    # Affiliate applications table
    if is_postgres:
        try:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS affiliate_applications (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(255) NOT NULL,
                    email VARCHAR(255) NOT NULL,
                    website TEXT,
                    social_handle VARCHAR(255),
                    audience_description TEXT NOT NULL,
                    trading_experience TEXT,
                    status VARCHAR(20) DEFAULT 'pending',
                    affiliate_code VARCHAR(20) UNIQUE,
                    admin_notes TEXT,
                    reviewed_by INTEGER REFERENCES users(id),
                    reviewed_at TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    whop_link_platform_basic TEXT,
                    whop_link_platform_premium TEXT,
                    whop_link_platform_elite TEXT,
                    whop_link_discord_basic TEXT,
                    whop_link_discord_premium TEXT
                )
            ''')
        except:
            pass  # Table already exists
    else:
        try:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS affiliate_applications (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    email TEXT NOT NULL,
                    website TEXT,
                    social_handle TEXT,
                    audience_description TEXT NOT NULL,
                    trading_experience TEXT,
                    status TEXT DEFAULT 'pending',
                    affiliate_code TEXT UNIQUE,
                    admin_notes TEXT,
                    reviewed_by INTEGER REFERENCES users(id),
                    reviewed_at TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    whop_link_platform_basic TEXT,
                    whop_link_platform_premium TEXT,
                    whop_link_platform_elite TEXT,
                    whop_link_discord_basic TEXT,
                    whop_link_discord_premium TEXT
                )
            ''')
        except:
            pass  # Table already exists

    # Add affiliate_code column if table was created before it was added
    try:
        cursor.execute('ALTER TABLE affiliate_applications ADD COLUMN IF NOT EXISTS affiliate_code VARCHAR(20) UNIQUE' if is_postgres else 'ALTER TABLE affiliate_applications ADD COLUMN IF NOT EXISTS affiliate_code TEXT UNIQUE')
    except:
        pass  # Column already exists

    # Add referred_by column to users table for affiliate referral tracking
    try:
        cursor.execute('ALTER TABLE users ADD COLUMN IF NOT EXISTS referred_by VARCHAR(20)' if is_postgres else 'ALTER TABLE users ADD COLUMN IF NOT EXISTS referred_by TEXT')
    except:
        pass  # Column already exists

    conn.commit()
    conn.close()

def fetch_and_store_tradovate_accounts(account_id: int, access_token: str, base_url: str = "https://demo.tradovateapi.com") -> dict:
    """
    Fetch Tradovate accounts/subaccounts for the given account_id and MERGE with existing data.
    This preserves accounts from environments the token can't access.
    Returns a dict with success flag and parsed subaccounts.
    """
    try:
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }

        # First, get EXISTING data from the database to preserve accounts we can't access
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f"SELECT tradovate_accounts, subaccounts FROM accounts WHERE id = {ph}", (account_id,))
        existing_row = cursor.fetchone()
        conn.close()
        
        existing_accounts = []
        existing_subaccounts = []
        if existing_row:
            try:
                if existing_row['tradovate_accounts']:
                    existing_accounts = json.loads(existing_row['tradovate_accounts'])
            except:
                pass
            try:
                if existing_row['subaccounts']:
                    existing_subaccounts = json.loads(existing_row['subaccounts'])
            except:
                pass

        # Always attempt both demo and live endpoints so we capture all accounts
        base_urls = []
        if base_url:
            base_urls.append(base_url.rstrip('/'))
        for candidate in ("https://demo.tradovateapi.com", "https://live.tradovateapi.com"):
            if candidate not in base_urls:
                base_urls.append(candidate)

        # Track which environments we successfully fetched
        fetched_environments = set()
        new_accounts = []
        new_subaccounts = []
        success = False
        
        for candidate_base in base_urls:
            try:
                response = requests.get(f"{candidate_base}/v1/account/list", headers=headers, timeout=15)
            except Exception as req_err:
                logger.warning(f"Error fetching Tradovate accounts from {candidate_base}: {req_err}")
                continue

            if response.status_code != 200:
                logger.warning(f"Could not fetch from {candidate_base}: {response.status_code} (token may not have access)")
                continue

            success = True
            environment = "demo" if "demo." in candidate_base else "live"
            fetched_environments.add(environment)
            logger.info(f"Successfully fetched accounts from {environment} environment")
            
            accounts_payload = response.json() or []
            for account in accounts_payload:
                account_copy = dict(account) if isinstance(account, dict) else {}
                account_copy["environment"] = environment
                account_copy["is_demo"] = environment == "demo"
                new_accounts.append(account_copy)

                parent_name = account_copy.get('name') or account_copy.get('accountName', 'Tradovate')
                for sub in account_copy.get('subAccounts', []) or []:
                    tags = sub.get('tags') or []
                    if isinstance(tags, str):
                        tags = [tags]
                    name = sub.get('name') or ''
                    is_demo = True if environment == "demo" else False
                    new_subaccounts.append({
                        "id": sub.get('id'),
                        "name": name,
                        "parent": parent_name,
                        "tags": tags,
                        "active": sub.get('active', True),
                        "environment": environment,
                        "is_demo": is_demo
                    })

        if not success:
            # If we couldn't fetch from any endpoint, keep existing data
            if existing_accounts:
                logger.warning(f"Could not refresh accounts, keeping existing {len(existing_accounts)} accounts")
                return {"success": True, "subaccounts": existing_subaccounts, "message": "Using cached data"}
            return {"success": False, "error": "Failed to fetch Tradovate accounts from demo or live endpoints"}

        # MERGE: Keep existing accounts from environments we couldn't fetch
        combined_accounts = list(new_accounts)
        combined_subaccounts = list(new_subaccounts)
        
        for existing_acc in existing_accounts:
            existing_env = existing_acc.get('environment') or ('demo' if existing_acc.get('is_demo') else 'live')
            if existing_env not in fetched_environments:
                # We couldn't access this environment, so keep the existing data
                logger.info(f"Preserving existing {existing_env} account: {existing_acc.get('name')}")
                combined_accounts.append(existing_acc)
        
        for existing_sub in existing_subaccounts:
            existing_env = existing_sub.get('environment') or ('demo' if existing_sub.get('is_demo') else 'live')
            if existing_env not in fetched_environments:
                combined_subaccounts.append(existing_sub)

        # Log what we're storing
        demo_count = sum(1 for a in combined_accounts if a.get('is_demo'))
        live_count = sum(1 for a in combined_accounts if not a.get('is_demo'))
        logger.info(f"Storing {demo_count} demo + {live_count} live accounts for account {account_id}")

        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE accounts
            SET tradovate_accounts = ?, subaccounts = ?, updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (json.dumps(combined_accounts), json.dumps(combined_subaccounts), account_id))
        conn.commit()
        conn.close()
        logger.info(f"Stored {len(combined_subaccounts)} Tradovate subaccounts for account {account_id}")
        return {"success": True, "subaccounts": combined_subaccounts}
    except Exception as e:
        logger.error(f"Error storing Tradovate accounts: {e}")
        return {"success": False, "error": str(e)}

# Handle missing source map files gracefully (browsers request these automatically)
# Source maps are optional and only used for debugging, so we return 204 No Content
@app.route('/static/js/<path:filename>')
def handle_static_js(filename):
    """Handle static JS files and source maps"""
    from flask import send_from_directory, Response
    import os
    
    # If it's a source map request, return 204 (they're optional)
    if filename.endswith('.map'):
        return Response(status=204)  # No Content - source maps are optional
    
    # Otherwise try to serve the actual file
    static_dir = os.path.join(os.path.dirname(__file__), 'static', 'js')
    file_path = os.path.join(static_dir, filename)
    if os.path.exists(file_path):
        return send_from_directory(static_dir, filename)
    
    # File doesn't exist
    return Response(status=404)

@app.route('/static/<path:filename>')
def handle_static_file(filename):
    """Handle static files and source maps"""
    from flask import send_from_directory, Response
    import os
    
    # If it's a source map request, return 204 (they're optional)
    if filename.endswith('.map'):
        return Response(status=204)  # No Content - source maps are optional
    
    # Otherwise try to serve the actual file
    static_dir = os.path.join(os.path.dirname(__file__), 'static')
    file_path = os.path.join(static_dir, filename)
    if os.path.exists(file_path):
        return send_from_directory(static_dir, filename)
    
    # File doesn't exist
    return Response(status=404)

@app.route('/')
def index():
    """Root route - redirect to dashboard if logged in, otherwise show landing page."""
    if USER_AUTH_AVAILABLE and is_logged_in():
        return redirect(url_for('dashboard'))
    return redirect(url_for('pricing'))

# ============================================================================
# DATABASE MIGRATION ENDPOINT
# ============================================================================

@app.route('/api/fix-trader-sizes/<int:recorder_id>', methods=['POST', 'GET'])
@admin_or_api_key_required
def fix_trader_sizes(recorder_id):
    """Reset all traders for a recorder to use NULL position sizes (fall back to recorder defaults)."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_pg = is_using_postgres()
        ph = '%s' if is_pg else '?'
        cursor.execute(f'UPDATE traders SET initial_position_size = NULL, add_position_size = NULL WHERE recorder_id = {ph}', (recorder_id,))
        affected = cursor.rowcount
        conn.commit()
        conn.close()
        return jsonify({'success': True, 'affected': affected, 'message': f'Reset {affected} traders to use recorder defaults'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/admin/trial-abuse/unblock-all', methods=['POST', 'GET'])
@admin_or_api_key_required
def admin_unblock_all_trial_abuse():
    """Emergency: unblock all falsely flagged users"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("UPDATE trial_fingerprints SET is_blocked = FALSE, block_reason = 'Reset - false positive', trial_count = 1")
        conn.commit()
        conn.close()
        return jsonify({'success': True, 'message': 'All users unblocked'})
    except Exception as e:
        return jsonify({'success': True, 'message': f'Table may not exist yet: {e}'})


@app.route('/api/reset-db-pool', methods=['POST', 'GET'])
@admin_or_api_key_required
def reset_db_pool():
    """Flush and recreate the PostgreSQL connection pool to clear poisoned connections."""
    global _pg_pool
    import psycopg2.pool
    try:
        if _pg_pool:
            _pg_pool.closeall()
            _pg_pool = None
        if _using_postgres and _db_url:
            _pg_pool = psycopg2.pool.ThreadedConnectionPool(
                minconn=20, maxconn=200, dsn=_db_url, connect_timeout=5
            )
            return jsonify({'success': True, 'message': 'DB pool reset ‚Äî 20 fresh connections ready'})
        return jsonify({'success': False, 'message': 'Not using PostgreSQL'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/run-migrations', methods=['POST', 'GET'])
@admin_or_api_key_required
def run_migrations():
    """Run pending database migrations to add missing columns.

    Query params:
        reset_paper_trades=1 - Also delete all paper trades (fresh start)
    """
    results = []

    # Check if we should reset paper trades
    reset_paper = request.args.get('reset_paper_trades') == '1'
    if reset_paper:
        try:
            import os
            database_url = os.environ.get('DATABASE_URL')
            if database_url:
                import psycopg2
                pconn = psycopg2.connect(database_url)
                pcur = pconn.cursor()
                pcur.execute('SELECT COUNT(*) FROM paper_trades')
                count = pcur.fetchone()[0]
                pcur.execute('DELETE FROM paper_trades')
                pconn.commit()
                pconn.close()
                results.append(f"üóëÔ∏è RESET: Deleted {count} paper trades")
        except Exception as e:
            results.append(f"‚ùå Paper trades reset failed: {str(e)[:100]}")

    conn = get_db_connection()
    cursor = conn.cursor()
    is_postgres = is_using_postgres()

    # List of columns to ensure exist
    migrations = [
        ('recorders', 'same_direction_ignore', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ('recorders', 'break_even_offset', 'INTEGER DEFAULT 0'),
        ('recorders', 'trail_trigger', 'INTEGER DEFAULT 0'),
        ('recorders', 'trail_freq', 'INTEGER DEFAULT 0'),
        ('recorders', 'inverse_signals', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ('accounts', 'is_connected', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ('accounts', 'projectx_username', 'TEXT'),
        ('accounts', 'projectx_api_key', 'TEXT'),
        ('accounts', 'projectx_prop_firm', 'TEXT'),
        ('accounts', 'projectx_account_id', 'TEXT'),
        # Trader-level risk settings
        ('traders', 'add_delay', 'INTEGER DEFAULT 1'),
        ('traders', 'signal_count', 'INTEGER DEFAULT 0'),
        ('traders', 'break_even_offset', 'INTEGER DEFAULT 0'),
        ('traders', 'trail_trigger', 'INTEGER DEFAULT 0'),
        ('traders', 'trail_freq', 'INTEGER DEFAULT 0'),
        ('traders', 'signal_cooldown', 'INTEGER DEFAULT 0'),
        ('traders', 'max_signals_per_session', 'INTEGER DEFAULT 0'),
        ('traders', 'auto_flat_after_cutoff', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ('traders', 'inverse_signals', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        # break_even settings for traders
        ('traders', 'break_even_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ('traders', 'break_even_ticks', 'INTEGER DEFAULT 10'),
        # Premium strategy flag for Just Trades Showcase
        ('recorders', 'is_premium', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        # Signal blocking - instant reject while position is open (broker-managed exits)
        ('recorders', 'signal_blocking', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        # Affiliate Whop checkout links
        ('affiliate_applications', 'whop_link_platform_basic', 'TEXT'),
        ('affiliate_applications', 'whop_link_platform_premium', 'TEXT'),
        ('affiliate_applications', 'whop_link_platform_elite', 'TEXT'),
        ('affiliate_applications', 'whop_link_discord_basic', 'TEXT'),
        ('affiliate_applications', 'whop_link_discord_premium', 'TEXT'),
        # DCA enabled flag for traders (Feb 2026)
        ('traders', 'dca_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
    ]

    for table, column, col_type in migrations:
        try:
            cursor.execute(f'ALTER TABLE {table} ADD COLUMN IF NOT EXISTS {column} {col_type}')
            conn.commit()
            results.append(f"‚úÖ Added {table}.{column}")
        except Exception as e:
            conn.rollback()
            if 'already exists' in str(e).lower() or 'duplicate column' in str(e).lower():
                results.append(f"‚è≠Ô∏è {table}.{column} already exists")
            else:
                results.append(f"‚ùå {table}.{column}: {str(e)[:100]}")

    # Fix NULL values for traders.add_delay and traders.signal_count
    try:
        cursor.execute('UPDATE traders SET add_delay = 1 WHERE add_delay IS NULL')
        updated_delay = cursor.rowcount
        cursor.execute('UPDATE traders SET signal_count = 0 WHERE signal_count IS NULL')
        updated_count = cursor.rowcount
        conn.commit()
        if updated_delay > 0 or updated_count > 0:
            results.append(f"‚úÖ Fixed NULL values: add_delay={updated_delay} rows, signal_count={updated_count} rows")
    except Exception as e:
        conn.rollback()
        results.append(f"‚ö†Ô∏è Could not fix NULL values: {str(e)[:100]}")

    # Enable DCA on ALL traders linked to JADVIX recorders (name contains 'JADVIX')
    # JADVIX strats require DCA ON for all users ‚Äî this fixes traders created before the
    # avg_down_enabled‚Üídca_enabled field name mismatch was fixed
    try:
        if is_postgres:
            cursor.execute('''
                UPDATE traders SET dca_enabled = TRUE
                WHERE recorder_id IN (SELECT id FROM recorders WHERE UPPER(name) LIKE '%JADVIX%')
                AND (dca_enabled IS NULL OR dca_enabled = FALSE)
            ''')
        else:
            cursor.execute('''
                UPDATE traders SET dca_enabled = 1
                WHERE recorder_id IN (SELECT id FROM recorders WHERE UPPER(name) LIKE '%JADVIX%')
                AND (dca_enabled IS NULL OR dca_enabled = 0)
            ''')
        jadvix_updated = cursor.rowcount
        conn.commit()
        if jadvix_updated > 0:
            results.append(f"‚úÖ JADVIX DCA fix: enabled dca_enabled on {jadvix_updated} traders")
        else:
            results.append(f"‚è≠Ô∏è JADVIX DCA: all traders already have dca_enabled=TRUE")
    except Exception as e:
        conn.rollback()
        results.append(f"‚ö†Ô∏è JADVIX DCA fix failed: {str(e)[:100]}")

    # Fix max_contracts DEFAULT 10 ‚Üí 0 (unlimited) for all traders
    # DEFAULT 10 was wrong ‚Äî silently capped every trader at 10 contracts
    # Users who never touched max_contracts got blocked on DCA past 10
    try:
        cursor.execute('UPDATE traders SET max_contracts = 0 WHERE max_contracts = 10')
        mc_updated = cursor.rowcount
        conn.commit()
        if mc_updated > 0:
            results.append(f"‚úÖ max_contracts fix: set {mc_updated} traders from 10 ‚Üí 0 (unlimited)")
        else:
            results.append(f"‚è≠Ô∏è max_contracts: no traders had the old default (10)")
    except Exception as e:
        conn.rollback()
        results.append(f"‚ö†Ô∏è max_contracts fix failed: {str(e)[:100]}")

    conn.close()
    return jsonify({'success': True, 'migrations': results})

@app.route('/api/whop/status', methods=['GET'])
def whop_status():
    """Check Whop integration status."""
    try:
        from whop_integration import get_configuration_status, WHOP_API_KEY, WHOP_WEBHOOK_SECRET
        status = get_configuration_status()

        # Test API connection if key is configured
        api_test = None
        if status['api_key_configured']:
            try:
                import requests
                headers = {
                    'Authorization': f'Bearer {WHOP_API_KEY}',
                    'Accept': 'application/json',
                }
                # Use /memberships endpoint (with limit) since /me doesn't exist in v2
                resp = requests.get('https://api.whop.com/api/v2/memberships?per_page=1', headers=headers, timeout=10)
                if resp.status_code == 200:
                    data = resp.json()
                    count = len(data.get('data', []))
                    api_test = {'success': True, 'message': f'API connected - {data.get("pagination", {}).get("total_count", count)} memberships found'}
                else:
                    api_test = {'success': False, 'message': f'API returned {resp.status_code}: {resp.text[:100]}'}
            except Exception as e:
                api_test = {'success': False, 'message': str(e)}

        return jsonify({
            'success': True,
            'whop_configured': status['api_key_configured'],
            'webhook_secret_configured': status['webhook_secret_configured'],
            'api_test': api_test,
            'product_map': status['product_map'],
            'webhook_url': 'https://www.justtrades.app/webhooks/whop'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# ============================================================================
# USER AUTHENTICATION ROUTES
# ============================================================================

@app.route('/pricing')
def pricing():
    """Public pricing page. Accepts ?ref=CODE for affiliate link tracking."""
    ref_code = request.args.get('ref', '').strip()
    affiliate_links = {}
    if ref_code:
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            is_postgres = is_using_postgres()
            placeholder = '%s' if is_postgres else '?'
            cursor.execute(
                f"SELECT whop_link_platform_basic, whop_link_platform_premium, whop_link_platform_elite, "
                f"whop_link_discord_basic, whop_link_discord_premium "
                f"FROM affiliate_applications WHERE affiliate_code = {placeholder} AND status = 'approved'",
                (ref_code,)
            )
            row = cursor.fetchone()
            if row:
                row_dict = dict(row)
                affiliate_links = {k: v for k, v in row_dict.items() if v}
            cursor.close()
            conn.close()
        except Exception as e:
            logger.warning(f"Pricing affiliate lookup error: {e}")
    return render_template('pricing.html', affiliate_links=affiliate_links, ref_code=ref_code)


@app.route('/terms')
def terms():
    """Terms of Service page."""
    return render_template('terms.html')


@app.route('/privacy')
def privacy():
    """Privacy Policy page."""
    return render_template('privacy.html')


@app.route('/risk-disclosure')
def risk_disclosure():
    """Risk Disclosure page."""
    return render_template('risk_disclosure.html')


# ============================================================================
# BLOG ROUTES
# ============================================================================

BLOG_SLUG_MAP = {
    'tradingview-to-tradovate-automation': 'blog_post_tradingview.html',
    'best-automation-apex-trader-funding-accounts': 'blog_post_apex.html',
    'multi-account-futures-trading-complete-guide': 'blog_post_multi_account.html',
}

@app.route('/blog')
def blog_index():
    """Blog landing page."""
    return render_template('blog_index.html')

@app.route('/blog/<slug>')
def blog_post(slug):
    """Individual blog post by SEO slug."""
    template = BLOG_SLUG_MAP.get(slug)
    if template:
        return render_template(template)
    return redirect(url_for('blog_index'))

@app.route('/sitemap.xml')
def sitemap():
    """Serve sitemap.xml for SEO."""
    return send_from_directory(os.path.join(os.path.dirname(__file__), 'static'), 'sitemap.xml', mimetype='application/xml')

@app.route('/robots.txt')
def robots():
    """Serve robots.txt for crawlers."""
    return send_from_directory(os.path.join(os.path.dirname(__file__), 'static'), 'robots.txt', mimetype='text/plain')


@app.route('/api/public/stats')
def public_stats():
    """
    Public API endpoint for platform statistics.
    Returns real counts from the database.
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Count total trades executed
        try:
            cursor.execute('SELECT COUNT(*) FROM recorded_trades')
            total_trades = cursor.fetchone()[0] or 0
        except:
            total_trades = 0
        
        # Count total users
        total_users = 0
        if USER_AUTH_AVAILABLE:
            try:
                cursor.execute('SELECT COUNT(*) FROM users')
                total_users = cursor.fetchone()[0] or 0
            except:
                total_users = 0
        
        cursor.close()
        conn.close()
        
        return jsonify({
            'total_trades': total_trades,
            'total_users': total_users,
            'uptime': 99,
            'support': '24/7'
        })
    except Exception as e:
        logger.warning(f"Stats API error: {e}")
        return jsonify({
            'total_trades': 0,
            'total_users': 0,
            'uptime': 99,
            'support': '24/7'
        })
        return jsonify({
            'total_trades': 0,
            'total_users': 0,
            'uptime': 99,
            'support': '24/7'
        })


@app.route('/login', methods=['GET', 'POST'])
def login():
    """Login page and handler."""
    # If already logged in, redirect to dashboard
    if USER_AUTH_AVAILABLE and is_logged_in():
        return redirect(url_for('dashboard'))
    
    if request.method == 'POST':
        if not USER_AUTH_AVAILABLE:
            flash('Authentication system not available.', 'error')
            return render_template('login.html')
        
        username_or_email = request.form.get('username', '').strip()
        password = request.form.get('password', '')
        remember = request.form.get('remember') == 'on'
        
        if not username_or_email or not password:
            flash('Please enter your username/email and password.', 'error')
            return render_template('login.html')
        
        user = authenticate_user(username_or_email, password)
        if user == 'pending_approval':
            flash('Your account is pending admin approval. Please wait for an administrator to approve your account.', 'warning')
            return render_template('login.html')
        elif user:
            login_user(user)
            
            # Set session permanence based on "remember me"
            session.permanent = remember
            
            # Redirect to originally requested page or dashboard
            next_url = session.pop('next_url', None)
            flash(f'Welcome back, {user.display_name}!', 'success')
            return redirect(next_url or url_for('dashboard'))
        else:
            flash('Invalid username/email or password.', 'error')
    
    return render_template('login.html')


@app.route('/register', methods=['GET', 'POST'])
def register():
    """Registration is disabled ‚Äî all accounts come through Whop purchase flow."""
    # If already logged in, redirect to dashboard
    if USER_AUTH_AVAILABLE and is_logged_in():
        return redirect(url_for('dashboard'))

    # No manual registration ‚Äî redirect to pricing page
    # Accounts are created automatically when users purchase on Whop:
    #   1. User buys on Whop ‚Üí webhook fires ‚Üí auto_create_user_from_whop()
    #   2. Activation email sent ‚Üí user clicks link ‚Üí sets username + password
    flash('To get started, choose a plan and sign up through our checkout. You will receive an activation email to set up your account.', 'info')
    return redirect(url_for('pricing'))


@app.route('/logout')
def logout():
    """Logout handler."""
    if USER_AUTH_AVAILABLE:
        logout_user()
        flash('You have been logged out successfully.', 'info')
    return redirect(url_for('login'))


# ============================================================================
# PASSWORD RESET ROUTES
# ============================================================================
@app.route('/forgot-password', methods=['GET', 'POST'])
def forgot_password():
    """Forgot password page ‚Äî enter email to receive reset link."""
    if request.method == 'POST':
        email = request.form.get('email', '').strip().lower()

        if not email:
            flash('Please enter your email address.', 'error')
            return render_template('forgot_password.html')

        # Always show generic success ‚Äî never reveal if email exists
        try:
            from user_auth import get_user_by_email
            from account_activation import generate_password_reset_token, send_password_reset_email

            user = get_user_by_email(email)
            if user:
                token = generate_password_reset_token(user.id, email)
                if token:
                    send_password_reset_email(email, token)
        except Exception as e:
            logger.error(f"Forgot password error for {email}: {e}")

        flash('If an account exists for that email, a password reset link has been sent.', 'info')
        return render_template('forgot_password.html')

    # GET request
    return render_template('forgot_password.html')


@app.route('/reset-password', methods=['GET', 'POST'])
def reset_password():
    """Reset password page ‚Äî validate token and set new password."""
    if request.method == 'POST':
        token = request.form.get('token', '').strip()
        password = request.form.get('password', '')
        confirm_password = request.form.get('confirm_password', '')

        if not token:
            flash('Invalid reset link.', 'error')
            return render_template('reset_password.html', has_token=False)

        from account_activation import validate_password_reset_token, mark_reset_token_used

        user_id, email = validate_password_reset_token(token)
        if not user_id:
            flash('This reset link is invalid or has expired.', 'error')
            return render_template('reset_password.html', has_token=False)

        # Validate password
        if not password or len(password) < 6:
            flash('Password must be at least 6 characters.', 'error')
            return render_template('reset_password.html', has_token=True, token=token)

        if password != confirm_password:
            flash('Passwords do not match.', 'error')
            return render_template('reset_password.html', has_token=True, token=token)

        # Update password
        from user_auth import update_user_password

        success = update_user_password(user_id, password)
        if success:
            mark_reset_token_used(token)
            flash('Your password has been reset successfully. Please sign in.', 'success')
            return redirect(url_for('login'))
        else:
            flash('Failed to reset password. Please try again.', 'error')
            return render_template('reset_password.html', has_token=True, token=token)

    # GET request
    token = request.args.get('token', '').strip()

    if token:
        from account_activation import validate_password_reset_token

        user_id, email = validate_password_reset_token(token)
        if user_id:
            return render_template('reset_password.html', has_token=True, token=token)
        else:
            flash('This reset link is invalid or has expired.', 'error')
            return render_template('reset_password.html', has_token=False)

    # No token ‚Äî show expired state
    return render_template('reset_password.html', has_token=False)


# ============================================================================
# ADMIN ROUTES - System Dashboard
# ============================================================================
@app.route('/admin/dashboard')
@login_required
def admin_dashboard():
    """Admin system dashboard ‚Äî real-time platform health overview."""
    user = get_current_user()
    if not user or not user.is_admin:
        return redirect('/login')
    return render_template('admin_dashboard.html')


@app.route('/api/admin/dashboard-stats')
@login_required
def admin_dashboard_stats():
    """Aggregate endpoint for admin dashboard ‚Äî single fetch for all stats."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Unauthorized'}), 403

    # --- Section 1: System Health (from existing globals) ---
    webhook_workers_alive = sum(1 for t in _fast_webhook_threads if t.is_alive())
    broker_workers_alive = sum(1 for t in _broker_execution_threads if t.is_alive())

    # When external trading engine is running, pull broker worker count from Redis heartbeat
    if _EXTERNAL_ENGINE and broker_workers_alive == 0:
        try:
            from redis_state import get_engine_health, get_broker_stats
            engine_health = get_engine_health()
            if engine_health.get('healthy'):
                broker_workers_alive = engine_health.get('workers_alive', 0)
        except Exception:
            pass

    system_health = {
        'webhook_workers': {'alive': webhook_workers_alive, 'configured': _fast_webhook_worker_count},
        'broker_workers': {'alive': broker_workers_alive, 'configured': _broker_execution_worker_count},
        'webhook_queue': _fast_webhook_queue.qsize() if _fast_webhook_queue else 0,
        'broker_queue': broker_execution_queue.qsize(),
        'broker_stats': dict(_broker_execution_stats),
        'db_pool': 'healthy',
    }

    # --- Section 2: Signal Flow (from existing logs) ---
    activity = get_webhook_activity_log(50)
    recent_signals = []
    for a in activity[:20]:
        recent_signals.append({
            'timestamp': a.get('timestamp', ''),
            'recorder': a.get('recorder', ''),
            'action': a.get('action', ''),
            'symbol': a.get('symbol', ''),
            'status': a.get('status', ''),
            'error': a.get('error'),
        })

    # --- Section 3: User & Trade Stats (DB query) ---
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()

        cursor.execute('SELECT COUNT(*) FROM users')
        total_users = cursor.fetchone()[0]

        if is_postgres:
            cursor.execute("SELECT COUNT(*) FROM users WHERE is_approved = FALSE AND is_admin = FALSE")
        else:
            cursor.execute("SELECT COUNT(*) FROM users WHERE is_approved = 0 AND is_admin = 0")
        pending_users = cursor.fetchone()[0]

        if is_postgres:
            cursor.execute("SELECT COUNT(*) FROM recorders WHERE is_recording = TRUE")
        else:
            cursor.execute("SELECT COUNT(*) FROM recorders WHERE is_recording = 1")
        active_recorders = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM recorders')
        total_recorders = cursor.fetchone()[0]

        cursor.execute('SELECT COUNT(*) FROM traders')
        total_traders = cursor.fetchone()[0]

        if is_postgres:
            cursor.execute("SELECT COUNT(*) FROM traders WHERE enabled = TRUE")
        else:
            cursor.execute("SELECT COUNT(*) FROM traders WHERE enabled = 1")
        enabled_traders = cursor.fetchone()[0]

        if is_postgres:
            cursor.execute("SELECT COUNT(*) FROM recorded_trades WHERE created_at::date = CURRENT_DATE")
        else:
            cursor.execute("SELECT COUNT(*) FROM recorded_trades WHERE DATE(created_at) = DATE('now')")
        todays_trades = cursor.fetchone()[0]

        if is_postgres:
            cursor.execute("SELECT COALESCE(SUM(pnl), 0) FROM recorded_trades WHERE exit_time::date = CURRENT_DATE AND status = 'closed'")
        else:
            cursor.execute("SELECT COALESCE(SUM(pnl), 0) FROM recorded_trades WHERE DATE(exit_time) = DATE('now') AND status = 'closed'")
        todays_pnl = round(cursor.fetchone()[0] or 0, 2)

        cursor.close()
        conn.close()

        user_trade_stats = {
            'total_users': total_users,
            'pending_users': pending_users,
            'active_recorders': active_recorders,
            'total_recorders': total_recorders,
            'total_traders': total_traders,
            'enabled_traders': enabled_traders,
            'todays_trades': todays_trades,
            'todays_pnl': todays_pnl,
        }
    except Exception as e:
        user_trade_stats = {'error': str(e)}

    # --- Section 4: Recent Failures ---
    failures = get_broker_failures(10)

    return jsonify({
        'success': True,
        'system_health': system_health,
        'recent_signals': recent_signals,
        'user_trade_stats': user_trade_stats,
        'recent_failures': failures,
    })


# ============================================================================
# ADMIN ROUTES - User Management
# ============================================================================
@app.route('/admin/users')
def admin_users():
    """Admin page for managing users."""
    if not USER_AUTH_AVAILABLE:
        flash('Authentication system not available.', 'error')
        return redirect(url_for('dashboard'))
    
    if not is_logged_in():
        return redirect(url_for('login'))
    
    user = get_current_user()
    if not user or not user.is_admin:
        flash('You do not have permission to access this page.', 'error')
        return redirect(url_for('dashboard'))
    
    from user_auth import get_all_users, get_pending_users
    from datetime import datetime, timedelta
    
    users = get_all_users()
    pending_count = len([u for u in users if not u.is_approved and not u.is_admin])
    
    # Calculate online/offline status for each user (online if logged in within last 2 hours)
    # Note: Using 2 hours since last_login only updates on login, not continuously
    users_online = {}
    now = datetime.now()
    online_threshold = timedelta(hours=2)  # Extended to 2 hours since last_login only updates on login
    
    for u in users:
        is_online = False
        if u.last_login and u.is_active:
            try:
                last_login_str = str(u.last_login).strip()
                
                # Parse last_login timestamp - handle different formats
                if 'T' in last_login_str:
                    # ISO format: '2025-01-12T14:30:00' or '2025-01-12T14:30:00.123456'
                    clean_str = last_login_str.split('+')[0].split('Z')[0]  # Remove timezone
                    if '.' in clean_str:
                        clean_str = clean_str.split('.')[0]  # Remove microseconds
                    try:
                        last_login_dt = datetime.strptime(clean_str, '%Y-%m-%dT%H:%M:%S')
                    except:
                        last_login_dt = datetime.fromisoformat(clean_str)
                else:
                    # SQLite format: '2025-01-12 14:30:00' or '2025-01-12 14:30:00.123456'
                    clean_str = last_login_str.split('.')[0]  # Remove microseconds if present
                    last_login_dt = datetime.strptime(clean_str, '%Y-%m-%d %H:%M:%S')
                
                # Remove timezone info if present for comparison
                if hasattr(last_login_dt, 'tzinfo') and last_login_dt.tzinfo:
                    last_login_dt = last_login_dt.replace(tzinfo=None)
                
                # Check if within threshold
                time_diff = now - last_login_dt
                is_online = time_diff < online_threshold and time_diff.total_seconds() >= 0
            except Exception as e:
                # Log parse errors for debugging
                logger.debug(f"Error parsing last_login for user {u.id} ({u.username}): {e}, value: {u.last_login}")
                is_online = False
        
        users_online[u.id] = is_online
    
    # Get pending username changes
    pending_username_changes = []
    try:
        from user_auth import get_auth_db_connection
        auth_conn, auth_db_type = get_auth_db_connection()
        auth_cursor = auth_conn.cursor()
        
        # Ensure table exists
        if auth_db_type == 'postgresql':
            auth_cursor.execute('''
                CREATE TABLE IF NOT EXISTS pending_username_changes (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL,
                    old_username VARCHAR(100) NOT NULL,
                    new_username VARCHAR(100) NOT NULL,
                    status VARCHAR(20) DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    reviewed_by INTEGER,
                    reviewed_at TIMESTAMP
                )
            ''')
        else:
            auth_cursor.execute('''
                CREATE TABLE IF NOT EXISTS pending_username_changes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    old_username TEXT NOT NULL,
                    new_username TEXT NOT NULL,
                    status TEXT DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    reviewed_by INTEGER,
                    reviewed_at TIMESTAMP
                )
            ''')
        auth_conn.commit()
        
        # Get pending changes
        if auth_db_type == 'postgresql':
            auth_cursor.execute('''
                SELECT puc.*, u.email
                FROM pending_username_changes puc
                JOIN users u ON puc.user_id = u.id
                WHERE puc.status = 'pending'
                ORDER BY puc.created_at DESC
            ''')
        else:
            auth_cursor.execute('''
                SELECT puc.*, u.email
                FROM pending_username_changes puc
                JOIN users u ON puc.user_id = u.id
                WHERE puc.status = 'pending'
                ORDER BY puc.created_at DESC
            ''')
        
        rows = auth_cursor.fetchall()
        for row in rows:
            if hasattr(row, 'keys'):
                pending_username_changes.append(dict(row))
            else:
                pending_username_changes.append({
                    'id': row[0], 'user_id': row[1], 'old_username': row[2],
                    'new_username': row[3], 'status': row[4], 'created_at': row[5],
                    'email': row[6] if len(row) > 6 else ''
                })
    except Exception as e:
        logger.error(f"‚ùå Error loading pending username changes: {e}")
    finally:
        if 'auth_cursor' in locals():
            auth_cursor.close()
        if 'auth_conn' in locals():
            auth_conn.close()
    
    return render_template('admin_users.html', 
                         users=users, 
                         users_online=users_online,
                         pending_count=pending_count,
                         pending_username_changes=pending_username_changes)


@app.route('/admin/users/approve/<int:user_id>', methods=['POST'])
def admin_approve_user(user_id):
    """Admin endpoint to approve a pending user."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'success': False, 'error': 'Admin access required'}), 403
    
    from user_auth import approve_user
    if approve_user(user_id):
        flash('User approved successfully!', 'success')
        return jsonify({'success': True})
    else:
        return jsonify({'success': False, 'error': 'Failed to approve user'}), 400


@app.route('/admin/users/reject/<int:user_id>', methods=['POST'])
def admin_reject_user(user_id):
    """Admin endpoint to reject (delete) a pending user."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'success': False, 'error': 'Admin access required'}), 403
    
    from user_auth import reject_user
    if reject_user(user_id):
        flash('User rejected and removed.', 'success')
        return jsonify({'success': True})
    else:
        return jsonify({'success': False, 'error': 'Failed to reject user'}), 400


@app.route('/admin/users/<int:user_id>/send-activation', methods=['POST'])
def admin_send_activation(user_id):
    """Admin endpoint to send (or resend) activation email to a user."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400

    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401

    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'success': False, 'error': 'Admin access required'}), 403

    from user_auth import get_user_by_id
    target_user = get_user_by_id(user_id)
    if not target_user:
        return jsonify({'success': False, 'error': 'User not found'}), 404

    try:
        from account_activation import generate_activation_token, send_activation_email
        token = generate_activation_token(target_user.id, target_user.email)
        if token:
            sent = send_activation_email(target_user.email, token)
            if sent:
                logger.info(f"Admin sent activation email to {target_user.email}")
                return jsonify({'success': True, 'message': f'Activation email sent to {target_user.email}'})
            else:
                return jsonify({'success': False, 'error': 'Failed to send email ‚Äî check BREVO_API_KEY config'}), 500
        else:
            return jsonify({'success': False, 'error': 'Failed to generate activation token'}), 500
    except Exception as e:
        logger.error(f"Admin send-activation error for user {user_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/admin/resend-all-activations', methods=['POST'])
def admin_resend_all_activations():
    """Batch resend activation emails to all unactivated Whop users."""
    api_key = request.headers.get('X-Admin-Key')
    if not api_key or api_key != os.environ.get('ADMIN_API_KEY'):
        # Fall back to session auth
        if not is_logged_in():
            return jsonify({'success': False, 'error': 'Not authorized'}), 401
        user = get_current_user()
        if not user or not user.is_admin:
            return jsonify({'success': False, 'error': 'Admin access required'}), 403

    try:
        from user_auth import get_all_users
        from account_activation import generate_activation_token, send_activation_email

        all_users = get_all_users()
        sent = []
        failed = []

        for u in all_users:
            # whop_ prefix = auto-created but never activated
            if not u.username or not u.username.startswith('whop_'):
                continue
            try:
                token = generate_activation_token(u.id, u.email)
                if token and send_activation_email(u.email, token):
                    sent.append(u.email)
                    logger.info(f"üìß Batch resend: activation email sent to {u.email}")
                else:
                    failed.append(u.email)
            except Exception as e:
                logger.error(f"üìß Batch resend failed for {u.email}: {e}")
                failed.append(u.email)

        logger.info(f"üìß Batch resend complete: {len(sent)} sent, {len(failed)} failed")
        return jsonify({
            'success': True,
            'sent': sent,
            'sent_count': len(sent),
            'failed': failed,
            'failed_count': len(failed)
        })
    except Exception as e:
        logger.error(f"Batch resend error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/admin/users/create', methods=['POST'])
def admin_create_user():
    """Admin endpoint to create a new user."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return redirect(url_for('login'))
    
    user = get_current_user()
    if not user or not user.is_admin:
        flash('You do not have permission to perform this action.', 'error')
        return redirect(url_for('dashboard'))
    
    username = request.form.get('username', '').strip().lower()
    email = request.form.get('email', '').strip().lower()
    password = request.form.get('password', '')
    display_name = request.form.get('display_name', '').strip() or username
    is_admin = request.form.get('is_admin') == '1'
    
    if not username or not email or not password:
        flash('All fields are required.', 'error')
        return redirect(url_for('admin_users'))
    
    new_user = create_user(username, email, password, display_name, is_admin)
    if new_user:
        # Admin-created users are automatically approved
        from user_auth import approve_user
        approve_user(new_user.id)
        flash(f'User "{username}" created and approved successfully.', 'success')
    else:
        flash('Failed to create user. Username or email may already exist.', 'error')
    
    return redirect(url_for('admin_users'))


@app.route('/admin/users/<int:user_id>/delete', methods=['POST'])
def admin_delete_user(user_id):
    """Admin endpoint to delete a user."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403
    
    # Don't allow deleting yourself
    if user_id == current.id:
        return jsonify({'success': False, 'error': 'Cannot delete your own account'}), 400
    
    from user_auth import get_auth_db_connection
    conn, db_type = get_auth_db_connection()
    cursor = conn.cursor()
    
    try:
        ph = '%s' if db_type == 'postgresql' else '?'

        # Delete from child tables in dependency order (most dependent first)
        cursor.execute(f'DELETE FROM support_messages WHERE ticket_id IN (SELECT id FROM support_tickets WHERE user_id = {ph})', (user_id,))
        cursor.execute(f'DELETE FROM recorded_trades WHERE user_id = {ph}', (user_id,))
        cursor.execute(f'DELETE FROM recorded_signals WHERE recorder_id IN (SELECT id FROM recorders WHERE user_id = {ph})', (user_id,))
        cursor.execute(f'DELETE FROM recorder_positions WHERE recorder_id IN (SELECT id FROM recorders WHERE user_id = {ph})', (user_id,))
        cursor.execute(f'DELETE FROM traders WHERE user_id = {ph}', (user_id,))
        cursor.execute(f'DELETE FROM support_tickets WHERE user_id = {ph}', (user_id,))
        cursor.execute(f'DELETE FROM recorders WHERE user_id = {ph}', (user_id,))
        cursor.execute(f'DELETE FROM strategies WHERE user_id = {ph}', (user_id,))
        cursor.execute(f'DELETE FROM push_subscriptions WHERE user_id = {ph}', (user_id,))
        cursor.execute(f'DELETE FROM accounts WHERE user_id = {ph}', (user_id,))
        # Nullify audit columns (don't delete announcements/applications, just unlink)
        cursor.execute(f'UPDATE announcements SET created_by = NULL WHERE created_by = {ph}', (user_id,))
        cursor.execute(f'UPDATE affiliate_applications SET reviewed_by = NULL WHERE reviewed_by = {ph}', (user_id,))
        # Clean up token tables (savepoint protects transaction if table doesn't exist)
        try:
            cursor.execute('SAVEPOINT token_cleanup_1')
            cursor.execute(f'DELETE FROM activation_tokens WHERE user_id = {ph}', (user_id,))
            cursor.execute('RELEASE SAVEPOINT token_cleanup_1')
        except Exception:
            cursor.execute('ROLLBACK TO SAVEPOINT token_cleanup_1')
        try:
            cursor.execute('SAVEPOINT token_cleanup_2')
            cursor.execute(f'DELETE FROM password_reset_tokens WHERE user_id = {ph}', (user_id,))
            cursor.execute('RELEASE SAVEPOINT token_cleanup_2')
        except Exception:
            cursor.execute('ROLLBACK TO SAVEPOINT token_cleanup_2')
        # Finally delete the user
        cursor.execute(f'DELETE FROM users WHERE id = {ph}', (user_id,))
        conn.commit()
        return jsonify({'success': True})
    except Exception as e:
        conn.rollback()
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        cursor.close()
        conn.close()


@app.route('/admin/users/<int:user_id>/get', methods=['GET'])
def admin_get_user(user_id):
    """Admin endpoint to get user data for editing."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403
    
    from user_auth import get_auth_db_connection
    conn, db_type = get_auth_db_connection()
    cursor = conn.cursor()
    
    try:
        if db_type == 'postgresql':
            cursor.execute('SELECT id, username, email, display_name, is_admin, is_active, is_approved FROM users WHERE id = %s', (user_id,))
        else:
            cursor.execute('SELECT id, username, email, display_name, is_admin, is_active, is_approved FROM users WHERE id = ?', (user_id,))
        
        row = cursor.fetchone()
        if not row:
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # Convert row to dict
        if hasattr(row, 'keys'):
            user_data = dict(row)
        else:
            user_data = {
                'id': row[0],
                'username': row[1],
                'email': row[2],
                'display_name': row[3],
                'is_admin': bool(row[4]),
                'is_active': bool(row[5]),
                'is_approved': bool(row[6])
            }
        
        return jsonify({'success': True, 'user': user_data})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        cursor.close()
        conn.close()


@app.route('/admin/users/<int:user_id>/edit', methods=['POST'])
def admin_edit_user(user_id):
    """Admin endpoint to update a user."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403
    
    data = request.get_json()
    if not data:
        return jsonify({'success': False, 'error': 'No data provided'}), 400
    
    username = data.get('username', '').strip()
    email = data.get('email', '').strip()
    display_name = data.get('display_name', '').strip()
    password = data.get('password', '').strip()
    is_admin = data.get('is_admin', False)
    is_approved = data.get('is_approved', False)
    is_active = data.get('is_active', True)
    
    if not username or not email:
        return jsonify({'success': False, 'error': 'Username and email are required'}), 400
    
    from user_auth import get_auth_db_connection
    import hashlib
    
    conn, db_type = get_auth_db_connection()
    cursor = conn.cursor()
    
    try:
        # Check for duplicate username/email (excluding current user)
        if db_type == 'postgresql':
            cursor.execute('SELECT id FROM users WHERE (username = %s OR email = %s) AND id != %s', (username, email, user_id))
        else:
            cursor.execute('SELECT id FROM users WHERE (username = ? OR email = ?) AND id != ?', (username, email, user_id))
        
        if cursor.fetchone():
            return jsonify({'success': False, 'error': 'Username or email already exists'}), 400
        
        # Build update query
        if password:
            # Hash the new password
            password_hash = hashlib.sha256(password.encode()).hexdigest()
            if db_type == 'postgresql':
                cursor.execute('''
                    UPDATE users SET username = %s, email = %s, display_name = %s, 
                    password_hash = %s, is_admin = %s, is_approved = %s, is_active = %s
                    WHERE id = %s
                ''', (username, email, display_name or username, password_hash, is_admin, is_approved, is_active, user_id))
            else:
                cursor.execute('''
                    UPDATE users SET username = ?, email = ?, display_name = ?, 
                    password_hash = ?, is_admin = ?, is_approved = ?, is_active = ?
                    WHERE id = ?
                ''', (username, email, display_name or username, password_hash, is_admin, is_approved, is_active, user_id))
        else:
            # Update without changing password
            if db_type == 'postgresql':
                cursor.execute('''
                    UPDATE users SET username = %s, email = %s, display_name = %s,
                    is_admin = %s, is_approved = %s, is_active = %s
                    WHERE id = %s
                ''', (username, email, display_name or username, is_admin, is_approved, is_active, user_id))
            else:
                cursor.execute('''
                    UPDATE users SET username = ?, email = ?, display_name = ?,
                    is_admin = ?, is_approved = ?, is_active = ?
                    WHERE id = ?
                ''', (username, email, display_name or username, is_admin, is_approved, is_active, user_id))
        
        conn.commit()
        logger.info(f"‚úÖ Admin updated user {user_id}: {username} (admin={is_admin}, approved={is_approved}, active={is_active})")
        return jsonify({'success': True})
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå Failed to update user {user_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        cursor.close()
        conn.close()


# ============================================================================
# ADMIN USER DETAILS - Full Overview (Accounts, Recorders, Trades)
# ============================================================================

@app.route('/admin/users/<int:user_id>/details', methods=['GET'])
def admin_get_user_details(user_id):
    """Admin endpoint to get full user details including accounts, recorders, and trades."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400

    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401

    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        # Get user's connected accounts
        # Check both tradovate_token (for Tradovate) AND is_connected column (for ProjectX/other brokers)
        # PostgreSQL uses TRUE/FALSE booleans, SQLite uses 1/0 integers
        if is_postgres:
            is_connected_check = "is_connected = TRUE"
        else:
            is_connected_check = "is_connected = 1"

        cursor.execute(f'''
            SELECT id, name, broker, environment, enabled,
                   CASE
                       WHEN tradovate_token IS NOT NULL AND tradovate_token != '' THEN 1
                       WHEN {is_connected_check} THEN 1
                       ELSE 0
                   END as is_connected,
                   tradovate_accounts, created_at, updated_at
            FROM accounts
            WHERE user_id = {placeholder}
            ORDER BY created_at DESC
        ''', (user_id,))
        
        accounts = []
        for row in cursor.fetchall():
            # Handle both dict and tuple rows
            if hasattr(row, 'keys'):
                acc = dict(row)
            else:
                acc = {
                    'id': row[0],
                    'name': row[1],
                    'broker': row[2],
                    'environment': row[3],
                    'enabled': bool(row[4]),
                    'is_connected': bool(row[5]),
                    'tradovate_accounts': row[6],
                    'created_at': str(row[7]) if row[7] else None,
                    'updated_at': str(row[8]) if row[8] else None
                }
            
            # Parse tradovate subaccounts
            subaccounts = []
            if acc.get('tradovate_accounts'):
                try:
                    ta = json.loads(acc['tradovate_accounts']) if isinstance(acc['tradovate_accounts'], str) else acc['tradovate_accounts']
                    for sub in ta:
                        subaccounts.append({
                            'id': sub.get('id'),
                            'name': sub.get('name'),
                            'is_demo': sub.get('is_demo', True)
                        })
                except:
                    pass
            acc['subaccounts'] = subaccounts
            acc.pop('tradovate_accounts', None)
            accounts.append(acc)

        # Get user's recorders/strategies (using only columns guaranteed to exist)
        cursor.execute(f'''
            SELECT id, name, strategy_type, enabled, ticker, created_at
            FROM recorders 
            WHERE user_id = {placeholder}
            ORDER BY created_at DESC
        ''', (user_id,))
        
        recorders = []
        for row in cursor.fetchall():
            if hasattr(row, 'keys'):
                rec = dict(row)
            else:
                rec = {
                    'id': row[0],
                    'name': row[1],
                    'strategy_type': row[2],
                    'enabled': bool(row[3]),
                    'ticker': row[4],
                    'created_at': str(row[5]) if row[5] else None
                }
            
            # Get linked accounts count from traders table instead
            try:
                cursor.execute(f'''
                    SELECT COUNT(*) FROM traders 
                    WHERE recorder_id = {placeholder} AND enabled = {'TRUE' if is_postgres else '1'}
                ''', (rec['id'],))
                count_row = cursor.fetchone()
                rec['enabled_accounts_count'] = count_row[0] if count_row else 0
            except:
                conn.rollback()  # Reset PostgreSQL transaction state
                rec['enabled_accounts_count'] = 0
            
            recorders.append(rec)

        # Get user's recent trades from recorder_positions (closed positions have P&L)
        try:
            cursor.execute(f'''
                SELECT rp.id, rp.ticker, rp.side, rp.total_quantity, rp.avg_entry_price, rp.exit_price,
                       rp.realized_pnl, rp.status, rp.opened_at, rp.closed_at,
                       r.name as recorder_name
                FROM recorder_positions rp
                LEFT JOIN recorders r ON rp.recorder_id = r.id
                WHERE r.user_id = {placeholder}
                ORDER BY rp.opened_at DESC
                LIMIT 50
            ''', (user_id,))
        except Exception as query_err:
            # Fallback: try recorded_trades table if recorder_positions doesn't work
            conn.rollback()  # Reset PostgreSQL transaction state before fallback query
            logger.debug(f"recorder_positions query failed, trying recorded_trades: {query_err}")
            cursor.execute(f'''
                SELECT rt.id, rt.ticker, rt.side, rt.quantity, rt.entry_price, rt.exit_price,
                       0 as realized_pnl, rt.status, rt.entry_time, rt.exit_time,
                       r.name as recorder_name
                FROM recorded_trades rt
                LEFT JOIN recorders r ON rt.recorder_id = r.id
                WHERE r.user_id = {placeholder}
                ORDER BY rt.entry_time DESC
                LIMIT 50
            ''', (user_id,))
        
        trades = []
        total_pnl = 0
        winning_trades = 0
        losing_trades = 0
        
        for row in cursor.fetchall():
            if hasattr(row, 'keys'):
                trade = dict(row)
            else:
                trade = {
                    'id': row[0],
                    'ticker': row[1],
                    'side': row[2],
                    'quantity': row[3],
                    'entry_price': float(row[4]) if row[4] else None,
                    'exit_price': float(row[5]) if row[5] else None,
                    'realized_pnl': float(row[6]) if row[6] else None,
                    'status': row[7],
                    'entry_time': str(row[8]) if row[8] else None,
                    'exit_time': str(row[9]) if row[9] else None,
                    'recorder_name': row[10]
                }
            
            if trade.get('realized_pnl'):
                total_pnl += trade['realized_pnl']
                if trade['realized_pnl'] > 0:
                    winning_trades += 1
                elif trade['realized_pnl'] < 0:
                    losing_trades += 1
            
            trades.append(trade)

        # Get user's trader links (account-to-strategy connections)
        traders_list = []
        try:
            cursor.execute(f'''
                SELECT t.id, t.recorder_id, t.account_id, t.subaccount_id, t.subaccount_name,
                       t.enabled, t.initial_position_size, t.add_position_size, t.multiplier,
                       t.max_contracts, t.custom_ticker, t.is_demo,
                       r.name as recorder_name, r.ticker as recorder_ticker,
                       a.name as account_name, a.broker as account_broker
                FROM traders t
                LEFT JOIN recorders r ON t.recorder_id = r.id
                LEFT JOIN accounts a ON t.account_id = a.id
                WHERE t.user_id = {placeholder}
                ORDER BY t.id DESC
            ''', (user_id,))
            for row in cursor.fetchall():
                if hasattr(row, 'keys'):
                    trader = dict(row)
                else:
                    trader = {
                        'id': row[0],
                        'recorder_id': row[1],
                        'account_id': row[2],
                        'subaccount_id': row[3],
                        'subaccount_name': row[4],
                        'enabled': bool(row[5]),
                        'initial_position_size': row[6],
                        'add_position_size': row[7],
                        'multiplier': float(row[8]) if row[8] else 1.0,
                        'max_contracts': row[9],
                        'custom_ticker': row[10],
                        'is_demo': bool(row[11]),
                        'recorder_name': row[12],
                        'recorder_ticker': row[13],
                        'account_name': row[14],
                        'account_broker': row[15]
                    }
                traders_list.append(trader)
        except Exception as traders_err:
            logger.debug(f"Traders query failed: {traders_err}")
            try:
                conn.rollback()
            except:
                pass

        conn.close()

        # Calculate stats
        total_trades = winning_trades + losing_trades
        win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0

        return jsonify({
            'success': True,
            'user_id': user_id,
            'accounts': accounts,
            'accounts_count': len(accounts),
            'recorders': recorders,
            'recorders_count': len(recorders),
            'traders': traders_list,
            'traders_count': len(traders_list),
            'trades': trades,
            'trade_stats': {
                'total_trades': len(trades),
                'closed_trades': total_trades,
                'winning_trades': winning_trades,
                'losing_trades': losing_trades,
                'win_rate': round(win_rate, 1),
                'total_pnl': round(total_pnl, 2)
            }
        })

    except Exception as e:
        logger.error(f"‚ùå Failed to get user details for {user_id}: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/admin/users/<int:user_id>/live-pnl', methods=['GET'])
def admin_get_user_live_pnl(user_id):
    """Admin endpoint to get cached live PnL data for a specific user's accounts."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403

    cached_data = _tradovate_pnl_cache.get('data', {})
    user_pnl = {}
    for acc_id, acc_data in cached_data.items():
        if acc_data.get('user_id') == user_id:
            user_pnl[acc_id] = acc_data
    return jsonify({'success': True, 'pnl_data': user_pnl})


# ============================================================================
# ADMIN SUBSCRIPTION MANAGEMENT
# ============================================================================

@app.route('/admin/users/<int:user_id>/subscription', methods=['GET'])
def admin_get_user_subscription(user_id):
    """Admin endpoint to get a user's current subscription."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403
    
    if not SUBSCRIPTION_SYSTEM_AVAILABLE:
        return jsonify({'success': False, 'error': 'Subscription system not available'}), 400
    
    try:
        from subscription_models import get_user_subscription, get_user_plan_tier, get_all_plans
        
        platform_sub = get_user_subscription(user_id, plan_type='platform')
        discord_sub = get_user_subscription(user_id, plan_type='discord')
        tier = get_user_plan_tier(user_id)
        all_plans = get_all_plans()
        
        return jsonify({
            'success': True,
            'platform_subscription': platform_sub,
            'discord_subscription': discord_sub,
            'tier': tier,
            'available_plans': all_plans
        })
    except Exception as e:
        logger.error(f"‚ùå Failed to get subscription for user {user_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/admin/users/<int:user_id>/subscription', methods=['POST'])
def admin_set_user_subscription(user_id):
    """Admin endpoint to set/change a user's subscription tier."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403
    
    if not SUBSCRIPTION_SYSTEM_AVAILABLE:
        return jsonify({'success': False, 'error': 'Subscription system not available'}), 400
    
    data = request.get_json()
    if not data:
        return jsonify({'success': False, 'error': 'No data provided'}), 400
    
    plan_slug = data.get('plan_slug', '').strip()
    action = data.get('action', 'set')  # 'set' or 'cancel'
    
    try:
        from subscription_models import (
            create_subscription, cancel_subscription, get_plan_by_slug,
            get_user_subscription
        )
        
        if action == 'cancel':
            # Cancel all subscriptions for this user
            success = cancel_subscription(user_id=user_id)
            if success:
                logger.info(f"‚úÖ Admin cancelled subscription for user {user_id}")
                return jsonify({'success': True, 'message': 'Subscription cancelled'})
            else:
                return jsonify({'success': False, 'error': 'No active subscription to cancel'}), 400
        
        elif action == 'set':
            if not plan_slug:
                return jsonify({'success': False, 'error': 'plan_slug is required'}), 400
            
            # Verify plan exists
            plan = get_plan_by_slug(plan_slug)
            if not plan:
                return jsonify({'success': False, 'error': f'Invalid plan: {plan_slug}'}), 400
            
            # Cancel existing subscription of same type first
            existing = get_user_subscription(user_id, plan_type=plan['plan_type'])
            if existing:
                cancel_subscription(user_id=user_id)
            
            # Create new subscription (admin-granted, no Whop needed)
            sub_id = create_subscription(
                user_id=user_id,
                plan_slug=plan_slug,
                whop_membership_id=f"admin_granted_{user_id}_{plan_slug}_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                trial_days=0
            )
            
            if sub_id:
                logger.info(f"‚úÖ Admin set subscription for user {user_id}: {plan_slug}")
                return jsonify({
                    'success': True, 
                    'message': f'Subscription set to {plan["name"]}',
                    'subscription_id': sub_id
                })
            else:
                return jsonify({'success': False, 'error': 'Failed to create subscription'}), 500
        
        else:
            return jsonify({'success': False, 'error': f'Invalid action: {action}'}), 400
            
    except Exception as e:
        logger.error(f"‚ùå Failed to set subscription for user {user_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/admin/username-changes', methods=['GET'])
def admin_username_changes():
    """Admin page showing pending username change requests."""
    if not USER_AUTH_AVAILABLE:
        flash('Authentication system not available.', 'error')
        return redirect(url_for('dashboard'))
    
    if not is_logged_in():
        return redirect(url_for('login'))
    
    user = get_current_user()
    if not user or not user.is_admin:
        flash('You do not have permission to access this page.', 'error')
        return redirect(url_for('dashboard'))
    
    try:
        from user_auth import get_auth_db_connection
        conn, db_type = get_auth_db_connection()
        cursor = conn.cursor()
        
        # Ensure table exists
        if db_type == 'postgresql':
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pending_username_changes (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL,
                    old_username VARCHAR(100) NOT NULL,
                    new_username VARCHAR(100) NOT NULL,
                    status VARCHAR(20) DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    reviewed_by INTEGER,
                    reviewed_at TIMESTAMP
                )
            ''')
        else:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pending_username_changes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    old_username TEXT NOT NULL,
                    new_username TEXT NOT NULL,
                    status TEXT DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    reviewed_by INTEGER,
                    reviewed_at TIMESTAMP
                )
            ''')
        conn.commit()
        
        # Get pending username changes
        if db_type == 'postgresql':
            cursor.execute('''
                SELECT puc.*, u.email, u.display_name
                FROM pending_username_changes puc
                JOIN users u ON puc.user_id = u.id
                WHERE puc.status = 'pending'
                ORDER BY puc.created_at DESC
            ''')
        else:
            cursor.execute('''
                SELECT puc.*, u.email, u.display_name
                FROM pending_username_changes puc
                JOIN users u ON puc.user_id = u.id
                WHERE puc.status = 'pending'
                ORDER BY puc.created_at DESC
            ''')
        
        rows = cursor.fetchall()
        pending_changes = []
        for row in rows:
            if hasattr(row, 'keys'):
                pending_changes.append(dict(row))
            else:
                pending_changes.append({
                    'id': row[0],
                    'user_id': row[1],
                    'old_username': row[2],
                    'new_username': row[3],
                    'status': row[4],
                    'created_at': row[5],
                    'email': row[6] if len(row) > 6 else '',
                    'display_name': row[7] if len(row) > 7 else ''
                })
        
        # For now, redirect to admin_users which now shows pending changes
        return redirect(url_for('admin_users'))
    except Exception as e:
        logger.error(f"‚ùå Error loading username changes: {e}")
        flash(f'Error loading username changes: {e}', 'error')
        return redirect(url_for('admin_users'))


@app.route('/admin/username-changes/<int:change_id>/approve', methods=['POST'])
def admin_approve_username_change(change_id):
    """Approve a pending username change."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403
    
    try:
        from user_auth import get_auth_db_connection
        conn, db_type = get_auth_db_connection()
        cursor = conn.cursor()
        
        # Get the pending change
        if db_type == 'postgresql':
            cursor.execute('SELECT * FROM pending_username_changes WHERE id = %s AND status = %s', (change_id, 'pending'))
        else:
            cursor.execute('SELECT * FROM pending_username_changes WHERE id = ? AND status = ?', (change_id, 'pending'))
        
        row = cursor.fetchone()
        if not row:
            return jsonify({'success': False, 'error': 'Username change request not found'}), 404
        
        change_data = dict(row) if hasattr(row, 'keys') else {
            'id': row[0], 'user_id': row[1], 'old_username': row[2], 'new_username': row[3]
        }
        
        # Check if new username already exists
        if db_type == 'postgresql':
            cursor.execute('SELECT id FROM users WHERE username = %s', (change_data['new_username'],))
        else:
            cursor.execute('SELECT id FROM users WHERE username = ?', (change_data['new_username'],))
        
        if cursor.fetchone():
            return jsonify({'success': False, 'error': 'Username already exists'}), 400
        
        # Update username and mark as approved
        if db_type == 'postgresql':
            cursor.execute('UPDATE users SET username = %s WHERE id = %s', (change_data['new_username'], change_data['user_id']))
            cursor.execute('''
                UPDATE pending_username_changes 
                SET status = %s, reviewed_by = %s, reviewed_at = NOW()
                WHERE id = %s
            ''', ('approved', current.id, change_id))
        else:
            cursor.execute('UPDATE users SET username = ? WHERE id = ?', (change_data['new_username'], change_data['user_id']))
            cursor.execute('''
                UPDATE pending_username_changes 
                SET status = ?, reviewed_by = ?, reviewed_at = CURRENT_TIMESTAMP
                WHERE id = ?
            ''', ('approved', current.id, change_id))
        
        conn.commit()
        logger.info(f"‚úÖ Admin approved username change: user {change_data['user_id']} ({change_data['old_username']} -> {change_data['new_username']})")
        return jsonify({'success': True, 'message': 'Username change approved'})
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå Error approving username change: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        cursor.close()
        conn.close()


@app.route('/admin/username-changes/<int:change_id>/reject', methods=['POST'])
def admin_reject_username_change(change_id):
    """Reject a pending username change."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    current = get_current_user()
    if not current or not current.is_admin:
        return jsonify({'success': False, 'error': 'Not authorized'}), 403
    
    try:
        from user_auth import get_auth_db_connection
        conn, db_type = get_auth_db_connection()
        cursor = conn.cursor()
        
        if db_type == 'postgresql':
            cursor.execute('''
                UPDATE pending_username_changes 
                SET status = %s, reviewed_by = %s, reviewed_at = NOW()
                WHERE id = %s AND status = %s
            ''', ('rejected', current.id, change_id, 'pending'))
        else:
            cursor.execute('''
                UPDATE pending_username_changes 
                SET status = ?, reviewed_by = ?, reviewed_at = CURRENT_TIMESTAMP
                WHERE id = ? AND status = ?
            ''', ('rejected', current.id, change_id, 'pending'))
        
        conn.commit()
        logger.info(f"‚úÖ Admin rejected username change request {change_id}")
        return jsonify({'success': True, 'message': 'Username change rejected'})
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå Failed to reject username change: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        cursor.close()
        conn.close()


@app.route('/admin/subscriptions')
def admin_subscriptions():
    """Admin page showing all user subscriptions."""
    if not USER_AUTH_AVAILABLE:
        flash('Authentication system not available.', 'error')
        return redirect(url_for('dashboard'))
    
    if not is_logged_in():
        return redirect(url_for('login'))
    
    user = get_current_user()
    if not user or not user.is_admin:
        flash('You do not have permission to access this page.', 'error')
        return redirect(url_for('dashboard'))
    
    if not SUBSCRIPTION_SYSTEM_AVAILABLE:
        flash('Subscription system not available.', 'error')
        return redirect(url_for('admin_users'))
    
    try:
        from subscription_models import get_all_plans
        from user_auth import get_all_users
        
        users = get_all_users()
        plans = get_all_plans()
        
        # Get subscription info for each user
        from subscription_models import get_user_subscription, get_user_plan_tier
        users_with_subs = []
        for u in users:
            user_data = {
                'id': u.id,
                'username': u.username,
                'email': u.email,
                'display_name': u.display_name,
                'is_admin': u.is_admin,
                'platform_sub': get_user_subscription(u.id, plan_type='platform'),
                'discord_sub': get_user_subscription(u.id, plan_type='discord'),
                'tier': get_user_plan_tier(u.id)
            }
            users_with_subs.append(user_data)
        
        return render_template('admin_users.html', 
                             users=users, 
                             users_with_subs=users_with_subs,
                             plans=plans,
                             pending_count=len([u for u in users if not u.is_approved and not u.is_admin]),
                             show_subscriptions=True)
    except Exception as e:
        logger.error(f"‚ùå Error loading admin subscriptions: {e}")
        flash(f'Error loading subscriptions: {e}', 'error')
        return redirect(url_for('admin_users'))


# ============================================================================
# ADMIN ANNOUNCEMENTS/BULLETINS
# ============================================================================

def ensure_announcements_table():
    """Ensure the announcements table exists (for migrations)."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS announcements (
                    id SERIAL PRIMARY KEY,
                    title VARCHAR(255) NOT NULL,
                    message TEXT NOT NULL,
                    type VARCHAR(20) DEFAULT 'info',
                    is_active BOOLEAN DEFAULT TRUE,
                    created_by INTEGER REFERENCES users(id),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    expires_at TIMESTAMP,
                    priority INTEGER DEFAULT 0
                )
            ''')
        else:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS announcements (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    title TEXT NOT NULL,
                    message TEXT NOT NULL,
                    type TEXT DEFAULT 'info',
                    is_active INTEGER DEFAULT 1,
                    created_by INTEGER REFERENCES users(id),
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    expires_at TEXT,
                    priority INTEGER DEFAULT 0
                )
            ''')
        
        conn.commit()
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        logger.warning(f"Error ensuring announcements table: {e}")
        return False

# Ensure table exists on startup
ensure_announcements_table()


def ensure_push_subscriptions_table():
    """Ensure the push_subscriptions table exists."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS push_subscriptions (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL REFERENCES users(id),
                    subscription_json TEXT NOT NULL,
                    endpoint TEXT NOT NULL,
                    active BOOLEAN DEFAULT TRUE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    user_agent TEXT,
                    UNIQUE(user_id, endpoint)
                )
            ''')
        else:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS push_subscriptions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL REFERENCES users(id),
                    subscription_json TEXT NOT NULL,
                    endpoint TEXT NOT NULL,
                    active INTEGER DEFAULT 1,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    user_agent TEXT,
                    UNIQUE(user_id, endpoint)
                )
            ''')
        
        conn.commit()
        cursor.close()
        conn.close()
        logger.info("‚úÖ Push subscriptions table ready")
        return True
    except Exception as e:
        logger.warning(f"Error ensuring push_subscriptions table: {e}")
        return False

ensure_push_subscriptions_table()


@app.route('/api/announcements/active')
def get_active_announcements():
    """Get all active announcements for display to users."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                SELECT id, title, message, type, priority, created_at
                FROM announcements 
                WHERE is_active = TRUE 
                AND (expires_at IS NULL OR expires_at > NOW())
                ORDER BY priority DESC, created_at DESC
            ''')
        else:
            cursor.execute('''
                SELECT id, title, message, type, priority, created_at
                FROM announcements 
                WHERE is_active = 1 
                AND (expires_at IS NULL OR expires_at > datetime('now'))
                ORDER BY priority DESC, created_at DESC
            ''')
        
        rows = cursor.fetchall()
        announcements = []
        for row in rows:
            announcements.append({
                'id': row[0],
                'title': row[1],
                'message': row[2],
                'type': row[3] or 'info',
                'priority': row[4] or 0,
                'created_at': str(row[5]) if row[5] else None
            })
        
        cursor.close()
        conn.close()
        return jsonify({'announcements': announcements})
    except Exception as e:
        logger.warning(f"Error fetching announcements: {e}")
        return jsonify({'announcements': []})


@app.route('/api/admin/announcements')
@login_required
def admin_get_announcements():
    """Admin: Get all announcements."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id, title, message, type, is_active, created_by, created_at, expires_at, priority
            FROM announcements 
            ORDER BY created_at DESC
        ''')
        rows = cursor.fetchall()
        announcements = []
        for row in rows:
            announcements.append({
                'id': row[0],
                'title': row[1],
                'message': row[2],
                'type': row[3] or 'info',
                'is_active': bool(row[4]),
                'created_by': row[5],
                'created_at': str(row[6]) if row[6] else None,
                'expires_at': str(row[7]) if row[7] else None,
                'priority': row[8] or 0
            })
        cursor.close()
        conn.close()
        return jsonify({'announcements': announcements})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/announcements', methods=['POST'])
@login_required
def admin_create_announcement():
    """Admin: Create a new announcement."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    data = request.json
    title = data.get('title', '').strip()
    message = data.get('message', '').strip()
    ann_type = data.get('type', 'info')
    expires_at = data.get('expires_at')
    priority = data.get('priority', 0)
    
    if not title or not message:
        return jsonify({'error': 'Title and message are required'}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                INSERT INTO announcements (title, message, type, created_by, expires_at, priority)
                VALUES (%s, %s, %s, %s, %s, %s)
                RETURNING id
            ''', (title, message, ann_type, user.id, expires_at, priority))
            new_id = cursor.fetchone()[0]
        else:
            cursor.execute('''
                INSERT INTO announcements (title, message, type, created_by, expires_at, priority)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (title, message, ann_type, user.id, expires_at, priority))
            new_id = cursor.lastrowid
        
        conn.commit()
        cursor.close()
        conn.close()

        logger.info(f"Admin {user.username} created announcement: {title}")
        
        # Optional: Broadcast to Discord users
        broadcast_discord = data.get('broadcast_discord', False)
        discord_sent = 0
        if broadcast_discord and DISCORD_NOTIFICATIONS_ENABLED:
            discord_sent = broadcast_announcement(title, message, ann_type)
            logger.info(f"üì¢ Discord broadcast sent to {discord_sent} users")
        
        return jsonify({'success': True, 'id': new_id, 'discord_sent': discord_sent})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


## /api/admin/discord/broadcast route moved to discord_routes.py

@app.route('/api/admin/announcements/<int:ann_id>', methods=['PUT'])
@login_required
def admin_update_announcement(ann_id):
    """Admin: Update an announcement."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    data = request.json
    title = data.get('title', '').strip()
    message = data.get('message', '').strip()
    ann_type = data.get('type', 'info')
    is_active = data.get('is_active', True)
    expires_at = data.get('expires_at')
    priority = data.get('priority', 0)
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                UPDATE announcements 
                SET title = %s, message = %s, type = %s, is_active = %s, expires_at = %s, priority = %s
                WHERE id = %s
            ''', (title, message, ann_type, is_active, expires_at, priority, ann_id))
        else:
            cursor.execute('''
                UPDATE announcements 
                SET title = ?, message = ?, type = ?, is_active = ?, expires_at = ?, priority = ?
                WHERE id = ?
            ''', (title, message, ann_type, 1 if is_active else 0, expires_at, priority, ann_id))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"Admin {user.username} updated announcement {ann_id}")
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/announcements/<int:ann_id>', methods=['DELETE'])
@login_required
def admin_delete_announcement(ann_id):
    """Admin: Delete an announcement."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('DELETE FROM announcements WHERE id = %s', (ann_id,))
        else:
            cursor.execute('DELETE FROM announcements WHERE id = ?', (ann_id,))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"Admin {user.username} deleted announcement {ann_id}")
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/fix-columns', methods=['GET', 'POST'])
@admin_or_api_key_required
def admin_fix_columns():
    """Fix missing columns in traders table - no auth required for emergency fix."""
    results = []
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # All columns that might be missing
        columns_to_add = [
            ('sl_type', "TEXT DEFAULT 'Fixed'"),
            ('avg_down_units', "TEXT DEFAULT 'Ticks'"),
            ('tp_units', "TEXT DEFAULT 'Ticks'"),
            ('trim_units', "TEXT DEFAULT 'Contracts'"),
            ('break_even_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
            ('break_even_ticks', 'INTEGER DEFAULT 10'),
            ('avg_down_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
            ('avg_down_amount', 'INTEGER DEFAULT 0'),
            ('avg_down_point', 'REAL DEFAULT 0'),
        ]
        
        for col_name, col_type in columns_to_add:
            try:
                cursor.execute(f'ALTER TABLE traders ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
                conn.commit()
                results.append(f"‚úÖ Added {col_name}")
            except Exception as col_err:
                if 'already exists' in str(col_err).lower() or 'duplicate column' in str(col_err).lower():
                    results.append(f"‚è≠Ô∏è {col_name} already exists")
                else:
                    results.append(f"‚ùå {col_name}: {str(col_err)}")
        
        cursor.close()
        conn.close()
        return jsonify({'success': True, 'results': results})
    except Exception as e:
        return jsonify({'error': str(e), 'results': results}), 500


@app.route('/api/admin/recorders/fix-user-ids', methods=['GET'])
@login_required
def admin_fix_recorder_user_ids_check():
    """Admin: Check which recorders are missing user_ids."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('SELECT id, name, user_id FROM recorders ORDER BY id')
        else:
            cursor.execute('SELECT id, name, user_id FROM recorders ORDER BY id')
        
        rows = cursor.fetchall()
        recorders = []
        missing_user_id = []
        for row in rows:
            rec = {
                'id': row[0],
                'name': row[1],
                'user_id': row[2]
            }
            recorders.append(rec)
            if not rec['user_id']:
                missing_user_id.append(rec)
        
        cursor.close()
        conn.close()
        
        return jsonify({
            'total_recorders': len(recorders),
            'missing_user_id': len(missing_user_id),
            'recorders': recorders,
            'needs_fix': missing_user_id
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/recorders/<int:recorder_id>/set-user', methods=['POST'])
@login_required
def admin_set_recorder_user(recorder_id):
    """Admin: Set user_id for a recorder."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    data = request.json
    target_user_id = data.get('user_id')
    
    if not target_user_id:
        return jsonify({'error': 'user_id is required'}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('UPDATE recorders SET user_id = %s WHERE id = %s', (target_user_id, recorder_id))
        else:
            cursor.execute('UPDATE recorders SET user_id = ? WHERE id = ?', (target_user_id, recorder_id))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"Admin set recorder {recorder_id} user_id to {target_user_id}")
        return jsonify({'success': True, 'message': f'Recorder {recorder_id} assigned to user {target_user_id}'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


## /api/admin/discord/check and /api/admin/discord/all-users routes moved to discord_routes.py

@app.route('/api/admin/max-loss-monitor/status', methods=['GET'])
@login_required
def admin_max_loss_monitor_status():
    """Admin: Get status of max loss monitors (paper + live)."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403

    status = {
        'paper_trades': {
            'monitor_running': _paper_monitor_running,
            'description': 'Monitors paper trades for TP/SL and max daily loss'
        },
        'live_accounts': {
            'monitor_running': False,
            'connected_accounts': 0,
            'breached_today': []
        }
    }

    try:
        from live_max_loss_monitor import get_max_loss_monitor_status
        live_status = get_max_loss_monitor_status()
        status['live_accounts'].update(live_status)
    except Exception as e:
        status['live_accounts']['error'] = str(e)

    return jsonify(status)


@app.route('/api/admin/export-configs', methods=['GET'])
@admin_or_api_key_required
def admin_export_configs():
    """Export all recorder configs and trader overrides as JSON for backup/documentation."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        enabled_val = 'TRUE' if is_postgres else '1'

        # --- Recorder configs ---
        cursor.execute(f"""
            SELECT id, name, symbol, initial_position_size, add_position_size,
                   tp_units, trim_units, tp_targets, sl_enabled, sl_amount, sl_units,
                   sl_type, trail_trigger, trail_freq, avg_down_enabled,
                   avg_down_amount, avg_down_point, avg_down_units,
                   break_even_enabled, break_even_ticks, break_even_offset,
                   add_delay, signal_cooldown, max_signals_per_session,
                   max_daily_loss, time_filter_1_enabled, time_filter_1_start,
                   time_filter_1_stop, auto_flat_after_cutoff, custom_ticker,
                   inverse_strategy, recording_enabled
            FROM recorders WHERE recording_enabled = {enabled_val}
            ORDER BY name
        """)
        recorder_cols = [desc[0] for desc in cursor.description]
        recorders = []
        for row in cursor.fetchall():
            rec = dict(zip(recorder_cols, row))
            # Parse tp_targets JSON if it's a string
            if isinstance(rec.get('tp_targets'), str):
                try:
                    import json as _json
                    rec['tp_targets'] = _json.loads(rec['tp_targets'])
                except Exception:
                    pass
            recorders.append(rec)

        # --- Trader overrides ---
        cursor.execute(f"""
            SELECT t.id, t.recorder_id, r.name as recorder_name, t.account_id,
                   t.enabled, t.multiplier, t.initial_position_size, t.add_position_size,
                   t.dca_enabled, t.tp_targets, t.sl_enabled, t.sl_amount, t.sl_type,
                   t.trail_trigger, t.trail_freq, t.break_even_enabled,
                   t.break_even_ticks, t.break_even_offset, t.max_daily_loss
            FROM traders t JOIN recorders r ON t.recorder_id = r.id
            WHERE t.enabled = {enabled_val}
            ORDER BY r.name, t.id
        """)
        trader_cols = [desc[0] for desc in cursor.description]
        traders = []
        for row in cursor.fetchall():
            tr = dict(zip(trader_cols, row))
            if isinstance(tr.get('tp_targets'), str):
                try:
                    import json as _json
                    tr['tp_targets'] = _json.loads(tr['tp_targets'])
                except Exception:
                    pass
            traders.append(tr)

        cursor.close()
        conn.close()

        return jsonify({
            'snapshot_time': datetime.now().isoformat(),
            'recorders': recorders,
            'traders': traders,
            'recorder_count': len(recorders),
            'trader_count': len(traders)
        })

    except Exception as e:
        logger.error(f"Export configs failed: {e}")
        return jsonify({'error': str(e)}), 500


## /api/admin/discord/enable route moved to discord_routes.py

@app.route('/api/admin/recorders/fix-all', methods=['POST'])
@login_required  
def admin_fix_all_recorder_user_ids():
    """Admin: Assign all recorders without user_id to a specific user."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    data = request.json
    target_user_id = data.get('user_id')
    
    if not target_user_id:
        return jsonify({'error': 'user_id is required'}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('UPDATE recorders SET user_id = %s WHERE user_id IS NULL', (target_user_id,))
        else:
            cursor.execute('UPDATE recorders SET user_id = ? WHERE user_id IS NULL', (target_user_id,))
        
        affected = cursor.rowcount
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"Admin fixed {affected} recorders, assigned to user {target_user_id}")
        return jsonify({'success': True, 'fixed_count': affected, 'message': f'Assigned {affected} recorders to user {target_user_id}'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/announcements/<int:ann_id>/toggle', methods=['POST'])
@login_required
def admin_toggle_announcement(ann_id):
    """Admin: Toggle announcement active status."""
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'error': 'Admin access required'}), 403
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('UPDATE announcements SET is_active = NOT is_active WHERE id = %s', (ann_id,))
        else:
            cursor.execute('UPDATE announcements SET is_active = 1 - is_active WHERE id = ?', (ann_id,))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/health/detailed')
@admin_or_api_key_required
def health_detailed():
    """Detailed health check - includes DB and service checks (slower)."""
    try:
        # Check database connection
        conn = get_db_connection()
        conn.execute("SELECT 1")
        conn.close()
        db_status = "healthy"
        db_type = "postgresql" if is_using_postgres() else "sqlite"
    except Exception as e:
        db_status = f"error: {str(e)}"
        db_type = "unknown"
    
    # Check Redis/cache if available
    cache_status = "not configured"
    if CACHE_AVAILABLE:
        try:
            cache_info = get_cache_status()
            cache_status = cache_info.get('backend', 'memory')
            if cache_info.get('connected'):
                cache_status += " (connected)"
        except:
            cache_status = "error"
    
    # Check async utils
    async_status = "available" if ASYNC_UTILS_AVAILABLE else "not loaded"
    
    # Get broker API queue stats
    queue_stats = broker_api_queue.get_stats()
    
    # Get TradingView WebSocket status
    try:
        tv_status = get_tradingview_connection_status()
    except:
        tv_status = {'status': 'unknown', 'connected': False}
    
    # Check webhook health
    webhook_status = "healthy"
    webhook_age = 0
    if _webhook_last_received > 0:
        webhook_age = time.time() - _webhook_last_received
        # If webhook was received but not processed within threshold, it's stuck
        if _webhook_last_received > _webhook_last_processed and webhook_age > _webhook_stuck_threshold:
            webhook_status = "stuck"
    
    # Overall health considers TradingView connection and webhook processing
    overall_healthy = db_status == "healthy" and tv_status.get('status') != 'error' and webhook_status != 'stuck'
    
    status = {
        "status": "healthy" if overall_healthy else "degraded",
        "database": db_status,
        "database_type": db_type,
        "cache": cache_status,
        "async_utils": async_status,
        "broker_api_queue": queue_stats,
        "tradingview": tv_status,
        "webhook": {
            "status": webhook_status,
            "last_received_ago_seconds": int(time.time() - _webhook_last_received) if _webhook_last_received > 0 else None,
            "last_processed_ago_seconds": int(time.time() - _webhook_last_processed) if _webhook_last_processed > 0 else None,
            "total_processed": _webhook_processing_count,
            "total_errors": _webhook_error_count
        },
        "timestamp": datetime.now().isoformat(),
        "version": "2025-12-29-robust-tv-watchdog"
    }
    
    return jsonify(status), 200 if overall_healthy else 503

@app.route('/api/broker-queue/stats')
def broker_queue_stats():
    """Get broker API queue statistics - useful for monitoring rate limit status."""
    stats = broker_api_queue.get_stats()
    stats['rate_limited'] = broker_api_queue.is_rate_limited()
    if broker_api_queue.is_rate_limited():
        stats['rate_limit_expires_in'] = max(0, broker_api_queue._rate_limit_until - time.time())
    return jsonify(stats)

@app.route('/api/broker-queue/clear-cache', methods=['POST'])
@admin_or_api_key_required
def broker_queue_clear_cache():
    """Clear broker API cache - useful after account changes."""
    broker_api_queue.clear_cache()
    return jsonify({'success': True, 'message': 'Cache cleared'})


@app.route('/api/webhook-activity')
@admin_or_api_key_required
def webhook_activity():
    """Get recent webhook activity log - shows all incoming webhooks and their processing status."""
    limit = request.args.get('limit', 50, type=int)
    activity = get_webhook_activity_log(limit)

    # Calculate stats
    total = len(activity)
    success_count = len([a for a in activity if a.get('status') == 'success'])
    failed_count = len([a for a in activity if a.get('status') == 'failed'])
    blocked_count = len([a for a in activity if a.get('status') == 'blocked'])

    return jsonify({
        'success': True,
        'stats': {
            'total': total,
            'success': success_count,
            'failed': failed_count,
            'blocked': blocked_count,
            'success_rate': f"{(success_count/total*100):.1f}%" if total > 0 else "N/A"
        },
        'activity': activity
    })


@app.route('/api/raw-webhooks')
@admin_or_api_key_required
def raw_webhooks():
    """Get raw webhook log - shows ALL incoming webhooks before any filtering.
    Useful for debugging why signals might not be processing.
    """
    limit = request.args.get('limit', 50, type=int)
    raw_log = get_raw_webhook_log(limit)

    # Parse body previews to extract action for analysis
    action_counts = {}
    for entry in raw_log:
        body = entry.get('body_preview', '')
        # Try to extract action from body
        action = 'unknown'
        if '"action":"buy"' in body.lower() or '"action": "buy"' in body.lower():
            action = 'buy'
        elif '"action":"sell"' in body.lower() or '"action": "sell"' in body.lower():
            action = 'sell'
        elif '"action":"long"' in body.lower() or '"action": "long"' in body.lower():
            action = 'long'
        elif '"action":"short"' in body.lower() or '"action": "short"' in body.lower():
            action = 'short'
        elif '"action":"close"' in body.lower() or '"action": "close"' in body.lower():
            action = 'close'
        action_counts[action] = action_counts.get(action, 0) + 1

    return jsonify({
        'success': True,
        'total': len(raw_log),
        'action_breakdown': action_counts,
        'webhooks': raw_log
    })


@app.route('/api/migrate-db', methods=['POST'])
@admin_or_api_key_required
def migrate_database():
    """Run database migrations to add missing columns."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        migrations_run = []
        
        logger.info(f"Running migrations - PostgreSQL: {is_postgres}")
        
        # For PostgreSQL, use ADD COLUMN IF NOT EXISTS (Postgres 9.6+)
        # For SQLite, catch the error
        
        columns_to_add = [
            ('is_private', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
            ('user_id', 'INTEGER'),
            ('account_id', 'INTEGER'),
            ('time_filter_1_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
            ('time_filter_2_enabled', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
            ('time_filter_1_start', "TEXT DEFAULT ''"),
            ('time_filter_1_stop', "TEXT DEFAULT ''"),
            ('time_filter_2_start', "TEXT DEFAULT ''"),
            ('time_filter_2_stop', "TEXT DEFAULT ''"),
            ('auto_flat_after_cutoff', 'BOOLEAN DEFAULT FALSE' if is_postgres else 'INTEGER DEFAULT 0'),
        ]
        
        for col_name, col_type in columns_to_add:
            try:
                if is_postgres:
                    # PostgreSQL: Use IF NOT EXISTS
                    cursor.execute(f'ALTER TABLE recorders ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
                else:
                    cursor.execute(f'ALTER TABLE recorders ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
                migrations_run.append(f'Added/verified {col_name}')
            except Exception as e:
                err_str = str(e).lower()
                if 'already exists' in err_str or 'duplicate' in err_str:
                    migrations_run.append(f'{col_name} already exists')
                else:
                    migrations_run.append(f'{col_name} error: {str(e)}')
        
        # Add missing columns to recorder_positions table
        position_columns = [
            ('best_unrealized_pnl', 'REAL DEFAULT 0'),
            ('worst_unrealized_pnl', 'REAL DEFAULT 0'),
            ('current_price', 'REAL'),
            ('unrealized_pnl', 'REAL DEFAULT 0'),
        ]
        
        for col_name, col_type in position_columns:
            try:
                if is_postgres:
                    cursor.execute(f'ALTER TABLE recorder_positions ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
                else:
                    cursor.execute(f'ALTER TABLE recorder_positions ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
                migrations_run.append(f'recorder_positions.{col_name} added')
            except Exception as e:
                if 'already exists' not in str(e).lower() and 'duplicate' not in str(e).lower():
                    migrations_run.append(f'recorder_positions.{col_name}: {str(e)}')
        
        # Add missing columns to recorded_trades table
        trade_columns = [
            ('max_favorable', 'REAL DEFAULT 0'),
            ('max_adverse', 'REAL DEFAULT 0'),
        ]
        
        for col_name, col_type in trade_columns:
            try:
                if is_postgres:
                    cursor.execute(f'ALTER TABLE recorded_trades ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
                else:
                    cursor.execute(f'ALTER TABLE recorded_trades ADD COLUMN IF NOT EXISTS {col_name} {col_type}')
                migrations_run.append(f'recorded_trades.{col_name} added')
            except Exception as e:
                if 'already exists' not in str(e).lower() and 'duplicate' not in str(e).lower():
                    migrations_run.append(f'recorded_trades.{col_name}: {str(e)}')
        
        conn.commit()
        conn.close()
        
        return jsonify({
            'success': True,
            'is_postgres': is_postgres,
            'migrations_run': migrations_run if migrations_run else ['No new migrations needed']
        })
    except Exception as e:
        logger.error(f"Migration error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/tradingview/status')
def tradingview_status():
    """Get TradingView WebSocket connection status - critical for live price updates."""
    try:
        status = get_tradingview_connection_status()
        return jsonify(status)
    except Exception as e:
        return jsonify({
            'connected': False,
            'error': str(e),
            'status': 'error'
        }), 500


@app.route('/api/tradingview/reconnect', methods=['POST'])
def tradingview_reconnect():
    """Force TradingView WebSocket to reconnect - use if prices are stale."""
    global _tradingview_force_reconnect
    _tradingview_force_reconnect = True
    logger.info("üîÑ Manual TradingView reconnect requested via API")
    return jsonify({
        'success': True,
        'message': 'Reconnect requested - WebSocket will reconnect within 10 seconds'
    })


@app.route('/api/price-update', methods=['POST'])
@admin_or_api_key_required
def api_price_update():
    """
    Receive real-time price updates from TradingView alerts.
    This is the simplest way to get real-time prices without WebSocket complexity.

    TradingView alert message format:
    {"ticker":"{{ticker}}","price":"{{close}}"}

    Supports multiple symbols in one request:
    {"prices":[{"ticker":"NQ","price":"25400"},{"ticker":"ES","price":"6900"}]}
    """
    global _market_data_cache

    try:
        data = request.get_json() or {}

        updated = []

        # Handle single price update
        if 'ticker' in data:
            ticker = data.get('ticker', '')
            price = data.get('price', data.get('close', 0))

            if ticker and price:
                root = extract_symbol_root(ticker)
                if root:
                    _market_data_cache[root] = {
                        'last': float(price),
                        'source': 'webhook_realtime',
                        'updated': time.time()
                    }
                    updated.append({'symbol': root, 'price': float(price)})
                    logger.info(f"üí∞ Real-time price update: {root} = {price}")

        # Handle batch price updates
        if 'prices' in data:
            for item in data['prices']:
                ticker = item.get('ticker', '')
                price = item.get('price', item.get('close', 0))

                if ticker and price:
                    root = extract_symbol_root(ticker)
                    if root:
                        _market_data_cache[root] = {
                            'last': float(price),
                            'source': 'webhook_realtime',
                            'updated': time.time()
                        }
                        updated.append({'symbol': root, 'price': float(price)})

        return jsonify({
            'success': True,
            'updated': updated,
            'cache_size': len(_market_data_cache)
        })

    except Exception as e:
        logger.error(f"Price update error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/prices', methods=['GET'])
def api_get_prices():
    """Get all cached prices - useful for dashboard to display current market prices."""
    global _market_data_cache

    prices = {}
    for symbol, data in _market_data_cache.items():
        prices[symbol] = {
            'price': data.get('last'),
            'source': data.get('source', 'unknown'),
            'age_seconds': round(time.time() - data.get('updated', 0), 1) if data.get('updated') else None
        }

    return jsonify({
        'success': True,
        'prices': prices,
        'count': len(prices)
    })


@app.route('/api/position-discrepancy-check')
@admin_or_api_key_required
def check_position_discrepancies():
    """
    Compare recorder_positions (TradingView tracking) with actual broker positions.
    Returns discrepancies for debugging sync issues between TV tracking and broker.
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Get all active recorder positions (TradingView tracked)
        cursor.execute('''
            SELECT rp.id, rp.recorder_id, rp.ticker, rp.side, rp.total_quantity, 
                   rp.avg_entry_price, rp.status,
                   r.name as recorder_name
            FROM recorder_positions rp
            JOIN recorders r ON rp.recorder_id = r.id
            WHERE rp.status = 'open'
        ''')
        
        tv_positions = [dict(row) for row in cursor.fetchall()]
        
        # For each recorder position, get linked traders and their broker positions
        discrepancies = []
        
        for tv_pos in tv_positions:
            recorder_id = tv_pos['recorder_id']
            ticker = tv_pos['ticker']
            tv_qty = tv_pos['total_quantity'] or 0
            tv_side = tv_pos['side']
            
            # Get all traders linked to this recorder (handle SQLite vs PostgreSQL)
            if is_postgres:
                cursor.execute('''
                    SELECT t.id, t.name, t.subaccount_id, t.subaccount_name, t.is_demo,
                           a.tradovate_token, a.name as account_name
                    FROM traders t
                    JOIN accounts a ON t.account_id = a.id
                    WHERE t.recorder_id = %s AND t.enabled = TRUE
                ''', (recorder_id,))
            else:
                cursor.execute('''
                    SELECT t.id, t.name, t.subaccount_id, t.subaccount_name, t.is_demo,
                           a.tradovate_token, a.name as account_name
                    FROM traders t
                    JOIN accounts a ON t.account_id = a.id
                    WHERE t.recorder_id = ? AND t.enabled = 1
                ''', (recorder_id,))
            
            traders = [dict(row) for row in cursor.fetchall()]
            
            for trader in traders:
                broker_qty = 0
                broker_side = 'FLAT'
                broker_error = None
                
                try:
                    # Import here to avoid circular imports
                    from recorder_service import get_broker_position_for_recorder
                    
                    # Convert ticker to tradovate symbol
                    contract_symbol = ticker.replace('!', '').replace('1', '')  # Basic conversion
                    broker_pos = get_broker_position_for_recorder(recorder_id, contract_symbol)
                    
                    if broker_pos:
                        broker_qty = abs(broker_pos.get('quantity', 0))
                        raw_qty = broker_pos.get('quantity', 0)
                        if raw_qty > 0:
                            broker_side = 'LONG'
                        elif raw_qty < 0:
                            broker_side = 'SHORT'
                        else:
                            broker_side = 'FLAT'
                except Exception as e:
                    broker_error = str(e)
                
                # Calculate discrepancy
                qty_diff = tv_qty - broker_qty
                side_match = (tv_side == broker_side) or (tv_qty == 0 and broker_qty == 0)
                
                discrepancy_entry = {
                    'recorder_name': tv_pos['recorder_name'],
                    'recorder_id': recorder_id,
                    'ticker': ticker,
                    'trader_name': trader['name'],
                    'account_name': trader['account_name'],
                    'subaccount': trader['subaccount_name'],
                    'tv_tracking': {
                        'side': tv_side,
                        'quantity': tv_qty,
                        'avg_price': tv_pos['avg_entry_price']
                    },
                    'broker_actual': {
                        'side': broker_side,
                        'quantity': broker_qty,
                        'error': broker_error
                    },
                    'discrepancy': {
                        'qty_difference': qty_diff,
                        'side_mismatch': not side_match,
                        'is_synced': qty_diff == 0 and side_match
                    }
                }
                
                discrepancies.append(discrepancy_entry)
        
        conn.close()
        
        # Summary
        total_positions = len(discrepancies)
        synced = sum(1 for d in discrepancies if d['discrepancy']['is_synced'])
        out_of_sync = total_positions - synced
        
        return jsonify({
            'success': True,
            'summary': {
                'total_positions': total_positions,
                'synced': synced,
                'out_of_sync': out_of_sync
            },
            'positions': discrepancies,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Position discrepancy check error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/position-sync', methods=['POST'])
@admin_or_api_key_required
def sync_positions_to_broker():
    """
    Sync broker positions to match TradingView tracker.
    If broker is behind TV tracker, place catch-up orders.
    
    POST body (optional):
    {
        "recorder_id": 2,  // specific recorder, or omit for all
        "dry_run": true    // just check what would happen, don't execute
    }
    """
    try:
        data = request.get_json() or {}
        specific_recorder_id = data.get('recorder_id')
        dry_run = data.get('dry_run', False)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get all active recorder positions
        cursor.execute('''
            SELECT rp.id, rp.recorder_id, rp.ticker, rp.side, rp.total_quantity,
                   rp.avg_entry_price, r.name as recorder_name, r.tp_targets
            FROM recorder_positions rp
            JOIN recorders r ON rp.recorder_id = r.id
            WHERE rp.status = 'open'
        ''')
        
        tv_positions = [dict(row) for row in cursor.fetchall()]
        
        if specific_recorder_id:
            tv_positions = [p for p in tv_positions if p['recorder_id'] == specific_recorder_id]
        
        sync_results = []
        
        for tv_pos in tv_positions:
            recorder_id = tv_pos['recorder_id']
            ticker = tv_pos['ticker']
            tv_qty = tv_pos['total_quantity'] or 0
            tv_side = tv_pos['side']
            
            # Get all traders linked to this recorder
            if is_postgres:
                cursor.execute('''
                    SELECT t.id, t.name, t.subaccount_id, t.subaccount_name, t.is_demo,
                           a.tradovate_token, a.name as account_name, a.id as account_id
                    FROM traders t
                    JOIN accounts a ON t.account_id = a.id
                    WHERE t.recorder_id = %s AND t.enabled = TRUE
                ''', (recorder_id,))
            else:
                cursor.execute('''
                    SELECT t.id, t.name, t.subaccount_id, t.subaccount_name, t.is_demo,
                           a.tradovate_token, a.name as account_name, a.id as account_id
                    FROM traders t
                    JOIN accounts a ON t.account_id = a.id
                    WHERE t.recorder_id = ? AND t.enabled = 1
                ''', (recorder_id,))
            
            traders = [dict(row) for row in cursor.fetchall()]
            
            for trader in traders:
                try:
                    from recorder_service import get_broker_position_for_recorder, execute_trade_simple
                    
                    # Get broker's current position
                    contract_symbol = ticker.replace('!', '').replace('1', '')
                    broker_pos = get_broker_position_for_recorder(recorder_id, contract_symbol)
                    
                    broker_qty = 0
                    broker_side = 'FLAT'
                    if broker_pos:
                        raw_qty = broker_pos.get('quantity', 0)
                        broker_qty = abs(raw_qty)
                        if raw_qty > 0:
                            broker_side = 'LONG'
                        elif raw_qty < 0:
                            broker_side = 'SHORT'
                    
                    # Calculate what we need to do
                    action_needed = None
                    qty_needed = 0
                    
                    if tv_side == broker_side:
                        # Same side - just need to add contracts if broker is behind
                        if broker_qty < tv_qty:
                            qty_needed = tv_qty - broker_qty
                            action_needed = 'BUY' if tv_side == 'LONG' else 'SELL'
                    elif broker_side == 'FLAT':
                        # Broker is flat but should have position
                        qty_needed = tv_qty
                        action_needed = 'BUY' if tv_side == 'LONG' else 'SELL'
                    else:
                        # Different sides - broker went opposite direction
                        # Would need to close and reverse - too risky for auto-sync
                        sync_results.append({
                            'recorder_name': tv_pos['recorder_name'],
                            'trader_name': trader['name'],
                            'account_name': trader['account_name'],
                            'ticker': ticker,
                            'status': 'SKIPPED',
                            'reason': f'Side mismatch: TV={tv_side}, Broker={broker_side}. Manual intervention required.',
                            'tv_qty': tv_qty,
                            'broker_qty': broker_qty
                        })
                        continue
                    
                    if qty_needed > 0:
                        result_entry = {
                            'recorder_name': tv_pos['recorder_name'],
                            'recorder_id': recorder_id,
                            'trader_name': trader['name'],
                            'account_name': trader['account_name'],
                            'ticker': ticker,
                            'action': action_needed,
                            'qty_needed': qty_needed,
                            'tv_qty': tv_qty,
                            'broker_qty': broker_qty
                        }
                        
                        if dry_run:
                            result_entry['status'] = 'DRY_RUN'
                            result_entry['message'] = f'Would place {action_needed} {qty_needed} to catch up'
                        else:
                            # Actually execute the catch-up order
                            logger.info(f"üîÑ POSITION SYNC: {action_needed} {qty_needed} {ticker} for {trader['name']}")
                            
                            # Get TP settings from recorder
                            tp_ticks = 10  # default
                            try:
                                tp_targets = json.loads(tv_pos.get('tp_targets', '[]'))
                                if tp_targets and len(tp_targets) > 0:
                                    tp_ticks = tp_targets[0].get('ticks', 10)
                            except:
                                pass
                            
                            exec_result = execute_trade_simple(
                                recorder_id=recorder_id,
                                action=action_needed,
                                ticker=ticker,
                                quantity=qty_needed,
                                tp_ticks=tp_ticks,
                                sl_ticks=0
                            )
                            
                            if exec_result.get('success'):
                                result_entry['status'] = 'SUCCESS'
                                result_entry['broker_fill'] = exec_result.get('fill_price')
                                result_entry['broker_qty_after'] = exec_result.get('broker_qty')
                            else:
                                result_entry['status'] = 'FAILED'
                                result_entry['error'] = exec_result.get('error')
                        
                        sync_results.append(result_entry)
                    else:
                        sync_results.append({
                            'recorder_name': tv_pos['recorder_name'],
                            'trader_name': trader['name'],
                            'account_name': trader['account_name'],
                            'ticker': ticker,
                            'status': 'IN_SYNC',
                            'tv_qty': tv_qty,
                            'broker_qty': broker_qty
                        })
                        
                except Exception as e:
                    sync_results.append({
                        'recorder_name': tv_pos['recorder_name'],
                        'trader_name': trader['name'],
                        'account_name': trader['account_name'],
                        'ticker': ticker,
                        'status': 'ERROR',
                        'error': str(e)
                    })
        
        conn.close()
        
        # Summary
        in_sync = sum(1 for r in sync_results if r.get('status') == 'IN_SYNC')
        synced = sum(1 for r in sync_results if r.get('status') == 'SUCCESS')
        failed = sum(1 for r in sync_results if r.get('status') in ['FAILED', 'ERROR'])
        skipped = sum(1 for r in sync_results if r.get('status') == 'SKIPPED')
        dry_run_count = sum(1 for r in sync_results if r.get('status') == 'DRY_RUN')
        
        return jsonify({
            'success': True,
            'dry_run': dry_run,
            'summary': {
                'total': len(sync_results),
                'already_in_sync': in_sync,
                'synced': synced,
                'failed': failed,
                'skipped': skipped,
                'dry_run_would_sync': dry_run_count
            },
            'results': sync_results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Position sync error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/accounts/auth-status')
def api_accounts_auth_status():
    """
    Get authentication status for all accounts.
    Returns list of accounts that need OAuth re-authentication.
    Use this to show warnings in the UI when accounts need manual reconnection.
    """
    try:
        accounts_need_reauth = get_accounts_needing_reauth()
        
        # Get account names for the IDs
        account_details = []
        if accounts_need_reauth:
            conn = get_db_connection()
            cursor = conn.cursor()
            ph = '%s' if is_using_postgres() else '?'
            for acct_id in accounts_need_reauth:
                cursor.execute(f'SELECT id, name FROM accounts WHERE id = {ph}', (acct_id,))
                row = cursor.fetchone()
                if row:
                    account_details.append({
                        'id': row['id'] if isinstance(row, dict) else row[0],
                        'name': row['name'] if isinstance(row, dict) else row[1],
                        'status': 'needs_reauth',
                        'action': 'Go to Account Management and click "Connect to Tradovate"'
                    })
            conn.close()
        
        return jsonify({
            'success': True,
            'all_accounts_valid': len(accounts_need_reauth) == 0,
            'accounts_needing_reauth': account_details,
            'count': len(account_details),
            'message': 'All accounts authenticated!' if len(accounts_need_reauth) == 0 else f'{len(accounts_need_reauth)} account(s) need OAuth re-authentication'
        })
    except Exception as e:
        logger.error(f"Error checking auth status: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# =============================================================================
# DEVICE AUTHORIZATION ROUTES (Captcha/Device Trust)
# =============================================================================

@app.route('/device-authorization')
def device_authorization():
    """Device authorization page for API Access verification"""
    return render_template('device_authorization.html')

@app.route('/api/account/<int:account_id>/device-info')
def get_account_device_info(account_id):
    """Get device ID and account info for device authorization"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f'SELECT id, name, device_id FROM accounts WHERE id = {ph}', (account_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return jsonify({'success': False, 'error': 'Account not found'})
        
        return jsonify({
            'success': True,
            'account_id': row['id'],
            'account_name': row['name'],
            'device_id': row['device_id'] or f'Just.Trade-{account_id}'
        })
    except Exception as e:
        logger.error(f"Error getting device info: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/account/<int:account_id>/api-credentials', methods=['GET'])
def get_api_credentials(account_id):
    """Get API credentials for an account (CID only, not secret)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f'SELECT api_key FROM accounts WHERE id = {ph}', (account_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return jsonify({'success': False, 'error': 'Account not found'})
        
        return jsonify({
            'success': True,
            'api_key': row['api_key'] or ''
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/account/<int:account_id>/api-credentials', methods=['POST'])
def save_api_credentials(account_id):
    """Save API credentials (CID and Secret) for an account"""
    try:
        data = request.get_json()
        api_key = data.get('api_key', '').strip()
        api_secret = data.get('api_secret', '').strip()
        
        if not api_key:
            return jsonify({'success': False, 'error': 'Client ID (CID) is required'})
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Update API credentials
        ph = '%s' if is_using_postgres() else '?'
        if api_secret:
            cursor.execute(f'UPDATE accounts SET api_key = {ph}, api_secret = {ph} WHERE id = {ph}',
                          (api_key, api_secret, account_id))
        else:
            # Only update CID if no secret provided (preserve existing secret)
            cursor.execute(f'UPDATE accounts SET api_key = {ph} WHERE id = {ph}',
                          (api_key, account_id))
        
        conn.commit()
        conn.close()
        
        logger.info(f"‚úÖ API credentials saved for account {account_id}")
        return jsonify({'success': True, 'message': 'API credentials saved'})
    except Exception as e:
        logger.error(f"Error saving API credentials: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/account/<int:account_id>/test-api-access', methods=['POST'])
def test_api_access(account_id):
    """Test if API Access works for an account (triggers device verification if needed)"""
    import aiohttp
    import asyncio
    
    async def do_test():
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f'SELECT name, username, password, device_id, environment, api_key, api_secret FROM accounts WHERE id = {ph}', (account_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return {'success': False, 'error': 'Account not found'}
        
        if not row['username'] or not row['password']:
            return {'success': False, 'error': 'No username/password stored for this account'}
        
        device_id = row['device_id'] or f'Just.Trade-{account_id}'
        is_demo = row['environment'] == 'demo'
        base_url = "https://demo.tradovateapi.com/v1" if is_demo else "https://live.tradovateapi.com/v1"
        
        # Use per-account API key if available (TradeManager's approach!)
        use_cid = int(row['api_key']) if row['api_key'] else int(os.getenv("TRADOVATE_API_CID", "8949"))
        use_sec = row['api_secret'] if row['api_secret'] else os.getenv("TRADOVATE_API_SECRET", "c8440ba5-6315-4845-8c69-977651d5c77a")
        
        body = {
            'name': row['username'],
            'password': row['password'],
            'appId': 'JustTrades',
            'appVersion': '1.0',
            'deviceId': device_id,
            'cid': use_cid,
            'sec': use_sec
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(f"{base_url}/auth/accesstokenrequest", json=body) as resp:
                data = await resp.json()
                
                if data.get('p-captcha'):
                    logger.info(f"Device verification triggered for {row['name']} (device: {device_id})")
                    return {
                        'success': False,
                        'p_captcha': True,
                        'p_time': data.get('p-time', 60),
                        'device_id': device_id,
                        'message': 'Device verification required. Check your email from Tradovate.'
                    }
                elif data.get('accessToken'):
                    logger.info(f"‚úÖ API Access successful for {row['name']} - device is trusted!")
                    return {
                        'success': True,
                        'message': 'API Access is working! Device is trusted.'
                    }
                else:
                    return {
                        'success': False,
                        'error': data.get('error', 'Unknown error'),
                        'response': data
                    }
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(do_test())
        loop.close()
        return jsonify(result)
    except Exception as e:
        logger.error(f"Error testing API access: {e}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/dashboard')
def dashboard():
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    
    # Check if user has platform subscription (not just Discord)
    has_platform_subscription = True
    user_tier = 'none'
    platform_subscription = None
    if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
        user = get_current_user()
        if user:
            from subscription_models import get_user_subscription, get_user_plan_tier
            platform_subscription = get_user_subscription(user.id, plan_type='platform')
            has_platform_subscription = platform_subscription is not None
            user_tier = get_user_plan_tier(user.id)
            # Admins always have access
            if user.is_admin:
                has_platform_subscription = True
    
    # Check if current user is admin (for premium showcase)
    is_admin = False
    if USER_AUTH_AVAILABLE:
        user = get_current_user()
        if user and user.is_admin:
            is_admin = True

    return render_template('dashboard.html',
                          has_platform_subscription=has_platform_subscription,
                          user_tier=user_tier,
                          platform_subscription=platform_subscription,
                          is_admin=is_admin)

# =============================================================================
# INSIDER SIGNALS ROUTES (Added Dec 8, 2025)
# Integrated directly - no separate service needed
# =============================================================================

# Initialize insider tables for PostgreSQL/SQLite
def _init_insider_tables():
    """Create insider tables if they don't exist (works with both SQLite and PostgreSQL)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            # PostgreSQL table creation
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_filings (
                    id SERIAL PRIMARY KEY,
                    accession_number TEXT UNIQUE,
                    form_type TEXT,
                    ticker TEXT,
                    company_name TEXT,
                    insider_name TEXT,
                    insider_title TEXT,
                    transaction_type TEXT,
                    shares INTEGER,
                    price REAL,
                    total_value REAL,
                    ownership_change_percent REAL,
                    shares_owned_after REAL,
                    filing_date TEXT,
                    transaction_date TEXT,
                    filing_url TEXT,
                    raw_data TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_signals (
                    id SERIAL PRIMARY KEY,
                    filing_id INTEGER,
                    ticker TEXT,
                    signal_score INTEGER,
                    insider_name TEXT,
                    insider_role TEXT,
                    transaction_type TEXT,
                    dollar_value REAL,
                    reason_flags TEXT,
                    is_highlighted INTEGER DEFAULT 0,
                    is_conviction INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_poll_status (
                    id INTEGER PRIMARY KEY,
                    last_poll_time TEXT,
                    last_filing_date TEXT,
                    filings_processed INTEGER DEFAULT 0,
                    errors_count INTEGER DEFAULT 0,
                    last_error TEXT
                )
            ''')
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_watchlist (
                    id SERIAL PRIMARY KEY,
                    watch_type TEXT NOT NULL,
                    watch_value TEXT NOT NULL,
                    notes TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(watch_type, watch_value)
                )
            ''')
            # Initialize poll status if not exists
            cursor.execute('SELECT COUNT(*) FROM insider_poll_status')
            if cursor.fetchone()[0] == 0:
                cursor.execute('INSERT INTO insider_poll_status (id, last_poll_time, filings_processed) VALUES (1, %s, 0)', (datetime.now().isoformat(),))
        else:
            # SQLite table creation
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_filings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    accession_number TEXT UNIQUE,
                    form_type TEXT,
                    ticker TEXT,
                    company_name TEXT,
                    insider_name TEXT,
                    insider_title TEXT,
                    transaction_type TEXT,
                    shares INTEGER,
                    price REAL,
                    total_value REAL,
                    ownership_change_percent REAL,
                    shares_owned_after REAL,
                    filing_date TEXT,
                    transaction_date TEXT,
                    filing_url TEXT,
                    raw_data TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_signals (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filing_id INTEGER,
                    ticker TEXT,
                    signal_score INTEGER,
                    insider_name TEXT,
                    insider_role TEXT,
                    transaction_type TEXT,
                    dollar_value REAL,
                    reason_flags TEXT,
                    is_highlighted INTEGER DEFAULT 0,
                    is_conviction INTEGER DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_poll_status (
                    id INTEGER PRIMARY KEY,
                    last_poll_time TEXT,
                    last_filing_date TEXT,
                    filings_processed INTEGER DEFAULT 0,
                    errors_count INTEGER DEFAULT 0,
                    last_error TEXT
                )
            ''')
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS insider_watchlist (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    watch_type TEXT NOT NULL,
                    watch_value TEXT NOT NULL,
                    notes TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(watch_type, watch_value)
                )
            ''')
            # Initialize poll status if not exists
            cursor.execute('SELECT COUNT(*) FROM insider_poll_status')
            if cursor.fetchone()[0] == 0:
                _ph = '%s' if is_using_postgres() else '?'
                cursor.execute(f'INSERT INTO insider_poll_status (id, last_poll_time, filings_processed) VALUES (1, {_ph}, 0)', (datetime.now().isoformat(),))
        
        conn.commit()
        conn.close()
        print("‚úÖ Insider tables initialized")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Insider tables init failed: {e}")
        return False

# Lazy initialization for insider tables
_insider_tables_initialized = False

def _ensure_insider_tables():
    """Lazily initialize insider tables on first use"""
    global _insider_tables_initialized
    if _insider_tables_initialized:
        return True
    try:
        result = _init_insider_tables()
        if result:
            _insider_tables_initialized = True
        return result
    except Exception as e:
        print(f"‚ö†Ô∏è Insider tables init error: {e}")
        return False

# Always mark as available - tables will be created on demand
INSIDER_SERVICE_AVAILABLE = True
print("‚úÖ Insider Signals module ready (tables will init on first use)")

# =============================================================================
# SEC EDGAR POLLING (PostgreSQL-compatible)
# =============================================================================
SEC_USER_AGENT = "JustTrades/1.0 (Contact: support@just.trades)"
INSIDER_POLL_INTERVAL = 300  # 5 minutes
INSIDER_HIGHLIGHT_THRESHOLD = 70
INSIDER_CONVICTION_THRESHOLD = 85

# Scoring weights
INSIDER_SCORING_WEIGHTS = {
    'dollar_value': 0.35, 'ownership_change': 0.20, 'insider_role': 0.15,
    'cluster': 0.15, 'recency': 0.15
}
INSIDER_ROLE_WEIGHTS = {
    'ceo': 1.0, 'chief executive': 1.0, 'cfo': 0.95, 'chief financial': 0.95,
    'coo': 0.9, 'president': 0.9, 'director': 0.7, '10%': 1.1, 'chairman': 0.85
}

def _insider_parse_atom_feed(xml_content):
    """Parse SEC EDGAR Atom feed for Form 4 filings"""
    import xml.etree.ElementTree as ET
    import re
    filings = []
    seen = set()
    try:
        root = ET.fromstring(xml_content)
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        for entry in root.findall('.//atom:entry', ns):
            try:
                title = entry.find('atom:title', ns)
                link = entry.find('atom:link', ns)
                updated = entry.find('atom:updated', ns)
                entry_id = entry.find('atom:id', ns)
                if title is not None and title.text:
                    if '(Issuer)' in title.text:
                        continue
                    accession = None
                    if entry_id is not None and entry_id.text:
                        m = re.search(r'accession-number=(\d{10}-\d{2}-\d{6})', entry_id.text)
                        if m:
                            accession = m.group(1)
                    if accession and accession in seen:
                        continue
                    if accession:
                        seen.add(accession)
                    filing = {
                        'form_type': '4', 'accession_number': accession,
                        'filing_url': link.get('href') if link is not None else None,
                        'filing_date': updated.text if updated is not None else None
                    }
                    if ' - ' in title.text:
                        parts = title.text.split(' - ', 1)
                        if len(parts) > 1 and '(' in parts[1]:
                            filing['insider_name'] = parts[1].split('(')[0].strip()
                    filings.append(filing)
            except:
                continue
    except:
        pass
    return filings

def _insider_fetch_form4_details(filing_url):
    """Fetch detailed Form 4 data from filing URL"""
    import xml.etree.ElementTree as ET
    import re
    details = {'ticker': None, 'insider_name': None, 'insider_title': None, 'transaction_type': None,
               'shares': 0, 'price': 0.0, 'total_value': 0.0, 'shares_owned_after': 0, 'ownership_change_percent': 0.0}
    try:
        if not filing_url:
            return details
        headers = {'User-Agent': SEC_USER_AGENT}
        resp = requests.get(filing_url, headers=headers, timeout=30)
        if resp.status_code != 200:
            return details
        xml_links = re.findall(r'href="([^"]*\.xml)"', resp.text, re.IGNORECASE)
        primary_xml = None
        for link in xml_links:
            if 'xsl' in link.lower() and 'form4' not in link.lower():
                continue
            if 'form4' in link.lower() or 'primary' in link.lower():
                primary_xml = link
                break
            if primary_xml is None:
                primary_xml = link
        if not primary_xml:
            return details
        if primary_xml.startswith('http'):
            xml_url = primary_xml
        elif primary_xml.startswith('/'):
            xml_url = f"https://www.sec.gov{primary_xml}"
        else:
            base_url = '/'.join(filing_url.rstrip('/').split('/')[:-1])
            xml_url = f"{base_url}/{primary_xml}"
        xml_resp = requests.get(xml_url, headers=headers, timeout=30)
        if xml_resp.status_code != 200:
            return details
        # Parse Form 4 XML
        root = ET.fromstring(xml_resp.text)
        issuer = root.find('.//issuer')
        if issuer is not None:
            ticker_elem = issuer.find('issuerTradingSymbol')
            if ticker_elem is not None and ticker_elem.text:
                details['ticker'] = ticker_elem.text.strip().upper()
        owner = root.find('.//reportingOwner')
        if owner is not None:
            owner_id = owner.find('reportingOwnerId')
            if owner_id is not None:
                name_elem = owner_id.find('rptOwnerName')
                if name_elem is not None and name_elem.text:
                    details['insider_name'] = name_elem.text.strip()
            rel = owner.find('reportingOwnerRelationship')
            if rel is not None:
                for tf in ['officerTitle', 'otherText']:
                    te = rel.find(tf)
                    if te is not None and te.text:
                        details['insider_title'] = te.text.strip()
                        break
        total_shares = 0
        total_value = 0.0
        for trans in root.findall('.//nonDerivativeTransaction'):
            code_elem = trans.find('.//transactionCoding/transactionCode')
            if code_elem is not None and code_elem.text:
                c = code_elem.text.upper()
                details['transaction_type'] = {'P': 'BUY', 'S': 'SELL', 'A': 'AWARD', 'G': 'GIFT', 'M': 'EXERCISE'}.get(c, c)
            shares_elem = trans.find('.//transactionAmounts/transactionShares/value')
            if shares_elem is not None and shares_elem.text:
                try:
                    total_shares += float(shares_elem.text)
                except:
                    pass
            price_elem = trans.find('.//transactionAmounts/transactionPricePerShare/value')
            if price_elem is not None and price_elem.text:
                try:
                    p = float(price_elem.text)
                    details['price'] = p
                    if shares_elem and shares_elem.text:
                        total_value += float(shares_elem.text) * p
                except:
                    pass
            after_elem = trans.find('.//postTransactionAmounts/sharesOwnedFollowingTransaction/value')
            if after_elem is not None and after_elem.text:
                try:
                    details['shares_owned_after'] = float(after_elem.text)
                except:
                    pass
        details['shares'] = int(total_shares)
        # Calculate total_value - use accumulated value OR fallback to shares * price
        if total_value > 0:
            details['total_value'] = round(total_value, 2)
        elif total_shares > 0 and details.get('price', 0) > 0:
            # Fallback: calculate from final shares and price if per-transaction calc failed
            details['total_value'] = round(total_shares * details['price'], 2)
        else:
            details['total_value'] = 0.0
        if details['shares_owned_after'] > 0 and total_shares > 0:
            before = details['shares_owned_after'] - total_shares if details['transaction_type'] == 'BUY' else details['shares_owned_after'] + total_shares
            if before > 0:
                details['ownership_change_percent'] = round((total_shares / before) * 100, 2)
    except:
        pass
    return details

def _insider_calculate_score(filing_data):
    """Calculate signal score (0-100) for an insider filing"""
    if filing_data.get('transaction_type') != 'BUY':
        return 0, ['not_buy']
    score = 0
    flags = []
    tv = filing_data.get('total_value', 0) or 0
    if tv >= 1000000:
        ds, flags = 100, flags + ['million_dollar_buy']
    elif tv >= 500000:
        ds, flags = 95, flags + ['large_buy_500k']
    elif tv >= 100000:
        ds, flags = 75, flags + ['significant_buy_100k']
    elif tv >= 50000:
        ds, flags = 65, flags + ['notable_buy_50k']
    else:
        ds = max(20, min(55, tv / 1000))
    score += ds * INSIDER_SCORING_WEIGHTS['dollar_value']
    oc = filing_data.get('ownership_change_percent', 0) or 0
    if oc >= 100:
        os_score, flags = 100, flags + ['doubled_position']
    elif oc >= 50:
        os_score, flags = 90, flags + ['major_position_increase']
    elif oc >= 25:
        os_score = 75
    else:
        os_score = 30
    score += os_score * INSIDER_SCORING_WEIGHTS['ownership_change']
    title = (filing_data.get('insider_title') or '').lower()
    rw = 0.5
    for rk, w in INSIDER_ROLE_WEIGHTS.items():
        if rk in title:
            rw = w
            if w >= 0.9:
                flags.append('c_suite_purchase')
            break
    score += (rw * 100) * INSIDER_SCORING_WEIGHTS['insider_role']
    score += 60 * INSIDER_SCORING_WEIGHTS['recency']
    if filing_data.get('price', 0) > 0:
        score += 5
        flags.append('open_market_purchase')
    return min(100, max(0, round(score))), flags

def _insider_process_filings():
    """Fetch and process SEC filings (works with PostgreSQL and SQLite)"""
    import json
    try:
        _ensure_insider_tables()
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Fetch recent Form 4 filings
        api_url = "https://www.sec.gov/cgi-bin/browse-edgar"
        params = {'action': 'getcurrent', 'type': '4', 'owner': 'only', 'count': '100', 'output': 'atom'}
        headers = {'User-Agent': SEC_USER_AGENT, 'Accept': 'application/atom+xml'}
        
        resp = requests.get(api_url, params=params, headers=headers, timeout=30)
        if resp.status_code != 200:
            conn.close()
            return
        
        filings = _insider_parse_atom_feed(resp.text)
        processed = 0
        
        for filing in filings:
            try:
                acc = filing.get('accession_number')
                if not acc:
                    continue
                # Check if exists
                if is_postgres:
                    cursor.execute('SELECT id FROM insider_filings WHERE accession_number = %s', (acc,))
                else:
                    cursor.execute('SELECT id FROM insider_filings WHERE accession_number = ?', (acc,))
                if cursor.fetchone():
                    continue
                # Fetch details
                details = _insider_fetch_form4_details(filing.get('filing_url'))
                if not details.get('ticker'):
                    continue
                # Insert filing
                if is_postgres:
                    cursor.execute('''INSERT INTO insider_filings (accession_number, form_type, ticker, company_name,
                        insider_name, insider_title, transaction_type, shares, price, total_value,
                        ownership_change_percent, shares_owned_after, filing_date, transaction_date, filing_url, raw_data)
                        VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s) RETURNING id''',
                        (acc, '4', details.get('ticker'), filing.get('company_name'), details.get('insider_name'),
                         details.get('insider_title'), details.get('transaction_type'), details.get('shares'),
                         details.get('price'), details.get('total_value'), details.get('ownership_change_percent'),
                         details.get('shares_owned_after'), filing.get('filing_date'), details.get('transaction_date'),
                         filing.get('filing_url'), json.dumps(details)))
                    filing_id = cursor.fetchone()[0]
                else:
                    cursor.execute('''INSERT INTO insider_filings (accession_number, form_type, ticker, company_name,
                        insider_name, insider_title, transaction_type, shares, price, total_value,
                        ownership_change_percent, shares_owned_after, filing_date, transaction_date, filing_url, raw_data)
                        VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''',
                        (acc, '4', details.get('ticker'), filing.get('company_name'), details.get('insider_name'),
                         details.get('insider_title'), details.get('transaction_type'), details.get('shares'),
                         details.get('price'), details.get('total_value'), details.get('ownership_change_percent'),
                         details.get('shares_owned_after'), filing.get('filing_date'), details.get('transaction_date'),
                         filing.get('filing_url'), json.dumps(details)))
                    filing_id = cursor.lastrowid
                # Score and create signal
                score, flags = _insider_calculate_score(details)
                if details.get('transaction_type') == 'BUY' and score > 0:
                    is_high = 1 if score >= INSIDER_HIGHLIGHT_THRESHOLD else 0
                    is_conv = 1 if score >= INSIDER_CONVICTION_THRESHOLD else 0
                    if is_postgres:
                        cursor.execute('''INSERT INTO insider_signals (filing_id, ticker, signal_score, insider_name,
                            insider_role, transaction_type, dollar_value, reason_flags, is_highlighted, is_conviction)
                            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)''',
                            (filing_id, details.get('ticker'), score, details.get('insider_name'),
                             details.get('insider_title'), details.get('transaction_type'), details.get('total_value'),
                             json.dumps(flags), is_high, is_conv))
                    else:
                        cursor.execute('''INSERT INTO insider_signals (filing_id, ticker, signal_score, insider_name,
                            insider_role, transaction_type, dollar_value, reason_flags, is_highlighted, is_conviction)
                            VALUES (?,?,?,?,?,?,?,?,?,?)''',
                            (filing_id, details.get('ticker'), score, details.get('insider_name'),
                             details.get('insider_title'), details.get('transaction_type'), details.get('total_value'),
                             json.dumps(flags), is_high, is_conv))
                processed += 1
                time.sleep(0.1)  # Rate limit
            except Exception as e:
                continue
        
        # Update poll status
        if is_postgres:
            cursor.execute('UPDATE insider_poll_status SET last_poll_time = %s, filings_processed = filings_processed + %s WHERE id = 1',
                          (datetime.now().isoformat(), processed))
        else:
            cursor.execute('UPDATE insider_poll_status SET last_poll_time = ?, filings_processed = filings_processed + ? WHERE id = 1',
                          (datetime.now().isoformat(), processed))
        conn.commit()
        conn.close()
        if processed > 0:
            print(f"üìä Insider polling: {processed} new filings processed")
    except Exception as e:
        print(f"‚ö†Ô∏è Insider polling error: {e}")

def _insider_polling_loop():
    """Background thread for SEC polling"""
    print(f"üöÄ Starting SEC insider polling (every {INSIDER_POLL_INTERVAL}s)")
    time.sleep(10)  # Initial delay
    while True:
        try:
            _insider_process_filings()
        except Exception as e:
            print(f"‚ö†Ô∏è Polling error: {e}")
        time.sleep(INSIDER_POLL_INTERVAL)

# Start the polling thread
_insider_poll_thread = threading.Thread(target=_insider_polling_loop, daemon=True)
_insider_poll_thread.start()

# ONE-TIME FIX: Enable DCA on all JADVIX traders (field name mismatch stored False)
try:
    _fix_conn = get_db_connection()
    _fix_cur = _fix_conn.cursor()
    if is_using_postgres():
        _fix_cur.execute('''
            UPDATE traders SET dca_enabled = TRUE
            WHERE recorder_id IN (SELECT id FROM recorders WHERE UPPER(name) LIKE '%JADVIX%')
            AND (dca_enabled IS NULL OR dca_enabled = FALSE)
        ''')
    else:
        _fix_cur.execute('''
            UPDATE traders SET dca_enabled = 1
            WHERE recorder_id IN (SELECT id FROM recorders WHERE UPPER(name) LIKE '%JADVIX%')
            AND (dca_enabled IS NULL OR dca_enabled = 0)
        ''')
    _jadvix_fixed = _fix_cur.rowcount
    _fix_conn.commit()
    _fix_cur.close()
    _fix_conn.close()
    if _jadvix_fixed > 0:
        print(f"‚úÖ STARTUP FIX: Enabled dca_enabled on {_jadvix_fixed} JADVIX traders")
except Exception as _fix_err:
    print(f"‚ö†Ô∏è JADVIX DCA startup fix failed: {_fix_err}")

# ============================================================================
# WHOP MEMBERSHIP SYNC DAEMON ‚Äî catches users that Whop webhooks missed
# ============================================================================
WHOP_SYNC_INTERVAL = 30  # 30 seconds
WHOP_RESEND_INTERVAL = 86400  # 24 hours between resends per user
WHOP_RESEND_MAX_DAYS = 3  # Stop resending after 3 days (they need to contact support)

# email ‚Üí timestamp of last resend (persists across sync cycles within a deploy)
_whop_email_last_resent = {}
# Track sync stats for the admin status endpoint
_whop_sync_stats = {
    'last_run': None,
    'last_synced': 0,
    'last_resent': 0,
    'total_synced': 0,
    'total_resent': 0,
    'stuck_users': [],  # Users who haven't activated after max resends
}

def _whop_membership_sync():
    """Poll Whop for all active memberships and create missing platform accounts."""
    from whop_integration import whop_api_request, WHOP_PRODUCT_MAP
    from user_auth import get_user_by_email
    from account_activation import auto_create_user_from_whop, generate_activation_token, send_activation_email
    from subscription_models import create_subscription, get_user_subscription

    result = whop_api_request('GET', '/memberships?per=100')
    if not result:
        print("‚ö†Ô∏è Whop sync: API call returned None ‚Äî check WHOP_API_KEY")
        return

    memberships = result.get('data', [])
    print(f"üîÑ Whop sync: got {len(memberships)} memberships from Whop")
    synced_count = 0
    resent_count = 0
    stuck = []
    now = time.time()

    for membership in memberships:
        try:
            # Only process active/trialing memberships
            if not membership.get('valid'):
                continue

            # Get product ID ‚Äî Whop returns product as string ID or dict
            product = membership.get('product')
            if isinstance(product, dict):
                product_id = product.get('id')
            else:
                product_id = product  # Already a string like "prod_xxx"
            plan_slug = WHOP_PRODUCT_MAP.get(product_id)
            if not plan_slug or not plan_slug.startswith('platform_'):
                continue

            # Get user email ‚Äî Whop returns email at top level, user as string ID or dict
            user_email = membership.get('email')
            user_field = membership.get('user')
            if not user_email and isinstance(user_field, dict):
                user_email = user_field.get('email')
            if not user_email:
                continue

            whop_user_id = user_field if isinstance(user_field, str) else (user_field.get('id') if isinstance(user_field, dict) else None)
            membership_id = membership.get('id')
            is_trial = membership.get('status') == 'trialing'

            # Check if user exists on our platform
            user = get_user_by_email(user_email)

            if user:
                # User exists ‚Äî ensure they have a subscription
                existing_sub = get_user_subscription(user.id, plan_type='platform')
                if not existing_sub:
                    create_subscription(
                        user_id=user.id,
                        plan_slug=plan_slug,
                        whop_membership_id=membership_id,
                        whop_customer_id=whop_user_id,
                        trial_days=7 if is_trial else 0
                    )
                    synced_count += 1
                    print(f"üîÑ Whop sync: created subscription for existing user {user_email} ({plan_slug})")

                # Resend activation email if user hasn't activated yet
                if user.username and user.username.startswith('whop_'):
                    last_sent = _whop_email_last_resent.get(user_email, 0)
                    elapsed = now - last_sent

                    if elapsed >= WHOP_RESEND_INTERVAL:
                        # Check if account is older than max resend window
                        account_age_days = 0
                        if hasattr(user, 'created_at') and user.created_at:
                            try:
                                created = datetime.fromisoformat(str(user.created_at).replace('Z', '+00:00'))
                                account_age_days = (datetime.now(created.tzinfo) - created).days if created.tzinfo else (datetime.now() - created).days
                            except Exception:
                                pass

                        if account_age_days > WHOP_RESEND_MAX_DAYS:
                            stuck.append(user_email)
                            print(f"‚ö†Ô∏è Whop sync: {user_email} unactivated for {account_age_days} days ‚Äî needs manual support")
                        else:
                            try:
                                token = generate_activation_token(user.id, user.email)
                                if token and send_activation_email(user.email, token):
                                    _whop_email_last_resent[user_email] = now
                                    resent_count += 1
                                    print(f"üìß Whop sync: sent activation email to {user_email}")
                                else:
                                    print(f"üìß Whop sync: email send FAILED for {user_email} ‚Äî check Brevo")
                            except Exception as e:
                                print(f"üìß Whop sync: failed to resend activation to {user_email}: {e}")
            else:
                # User doesn't exist ‚Äî auto-create account + subscription
                new_user_id = auto_create_user_from_whop(user_email, whop_user_id or 'unknown')
                if new_user_id:
                    create_subscription(
                        user_id=new_user_id,
                        plan_slug=plan_slug,
                        whop_membership_id=membership_id,
                        whop_customer_id=whop_user_id,
                        trial_days=7 if is_trial else 0
                    )
                    _whop_email_last_resent[user_email] = now
                    synced_count += 1
                    print(f"üîÑ Whop sync: auto-created user + subscription for {user_email} ({plan_slug})")
                else:
                    print(f"‚ö†Ô∏è Whop sync: failed to auto-create user for {user_email}")

        except Exception as e:
            print(f"‚ö†Ô∏è Whop sync: error processing membership {membership.get('id', '?')}: {e}")
            continue

    # Update stats for admin visibility
    _whop_sync_stats['last_run'] = datetime.now().isoformat()
    _whop_sync_stats['last_synced'] = synced_count
    _whop_sync_stats['last_resent'] = resent_count
    _whop_sync_stats['total_synced'] += synced_count
    _whop_sync_stats['total_resent'] += resent_count
    _whop_sync_stats['stuck_users'] = stuck

    if synced_count > 0 or resent_count > 0 or stuck:
        print(f"üîÑ Whop sync complete: {synced_count} created, {resent_count} emails sent, {len(stuck)} stuck")
        logger.info(f"üîÑ Whop sync complete: {synced_count} created, {resent_count} emails sent, {len(stuck)} stuck")


def _whop_sync_loop():
    """Background thread for Whop membership sync."""
    print(f"üöÄ Starting Whop membership sync (every {WHOP_SYNC_INTERVAL}s)")
    time.sleep(30)  # Initial delay ‚Äî let subscription system initialize
    while True:
        try:
            _whop_membership_sync()
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Whop sync loop error: {e}")
        time.sleep(WHOP_SYNC_INTERVAL)


if SUBSCRIPTION_SYSTEM_AVAILABLE:
    _whop_sync_thread = threading.Thread(target=_whop_sync_loop, daemon=True, name="Whop-Sync")
    _whop_sync_thread.start()


@app.route('/api/admin/whop-sync-status')
def api_whop_sync_status():
    """Admin endpoint to check Whop sync daemon health."""
    api_key = request.headers.get('X-Admin-Key')
    if not api_key or api_key != os.environ.get('ADMIN_API_KEY'):
        if not is_logged_in():
            return jsonify({'error': 'Not authorized'}), 401
        user = get_current_user()
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
    return jsonify({
        'sync_interval_seconds': WHOP_SYNC_INTERVAL,
        'resend_interval_hours': WHOP_RESEND_INTERVAL // 3600,
        'resend_max_days': WHOP_RESEND_MAX_DAYS,
        'pending_resends': len(_whop_email_last_resent),
        **_whop_sync_stats
    })

@app.route('/insider-signals')
@app.route('/insider_signals')
def insider_signals():
    """Render the Insider Signals tab"""
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    
    # Check subscription access
    has_access = True
    user_tier = 'none'
    if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
        user = get_current_user()
        if user:
            from subscription_models import check_feature_access, get_user_plan_tier
            has_access = check_feature_access(user.id, 'insider_signals')
            user_tier = get_user_plan_tier(user.id)
            # Admins always have access
            if user.is_admin:
                has_access = True
    
    return render_template('insider_signals.html', 
                          feature_access=has_access,
                          user_tier=user_tier)

@app.route('/api/insiders/status')
@feature_required('insider_signals')
def api_insiders_status():
    """Get insider service status"""
    try:
        # Ensure tables exist (lazy init)
        _ensure_insider_tables()
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get counts
        cursor.execute('SELECT COUNT(*) FROM insider_filings')
        total_filings = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM insider_signals')
        total_signals = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM insider_signals WHERE is_conviction = 1')
        conviction_signals = cursor.fetchone()[0]
        
        today = datetime.now().strftime('%Y-%m-%d')
        if is_postgres:
            cursor.execute("SELECT COUNT(*) FROM insider_signals WHERE DATE(created_at) = %s", (today,))
        else:
            cursor.execute("SELECT COUNT(*) FROM insider_signals WHERE DATE(created_at) = ?", (today,))
        today_signals = cursor.fetchone()[0]
        
        # Get poll status
        cursor.execute('SELECT * FROM insider_poll_status WHERE id = 1')
        row = cursor.fetchone()
        poll_status = None
        if row:
            if is_postgres:
                poll_status = {'last_poll_time': row[1], 'filings_processed': row[3], 'errors_count': row[4]}
            else:
                poll_status = {'last_poll_time': row[1], 'filings_processed': row[3], 'errors_count': row[4]}
        
        conn.close()
        
        return jsonify({
            'service': 'insider_signals_integrated',
            'status': 'running',
            'poll_interval_seconds': 300,
            'last_poll_time': poll_status['last_poll_time'] if poll_status else None,
            'total_filings': total_filings,
            'total_signals': total_signals,
            'conviction_signals': conviction_signals,
            'today_signals': today_signals,
            'filings_processed': poll_status['filings_processed'] if poll_status else 0,
            'errors_count': poll_status['errors_count'] if poll_status else 0
        })
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

@app.route('/api/insiders/today')
@feature_required('insider_signals')
def api_insiders_today():
    """Get today's insider signals"""
    try:
        _ensure_insider_tables()
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        today = datetime.now().strftime('%Y-%m-%d')
        
        if is_postgres:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE DATE(s.created_at) = %s
                ORDER BY s.signal_score DESC
            ''', (today,))
        else:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE DATE(s.created_at) = ?
                ORDER BY s.signal_score DESC
            ''', (today,))
        
        columns = ['id', 'filing_id', 'ticker', 'signal_score', 'insider_name', 'insider_role',
                   'transaction_type', 'dollar_value', 'reason_flags', 'is_highlighted', 'is_conviction',
                   'created_at', 'company_name', 'filing_url', 'shares', 'price',
                   'ownership_change_percent', 'filing_date', 'transaction_date']
        signals = [dict(zip(columns, row)) for row in cursor.fetchall()]
        conn.close()
        
        return jsonify({
            'success': True,
            'date': today,
            'count': len(signals),
            'signals': signals
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e), "signals": []}), 500

@app.route('/api/insiders/top')
@feature_required('insider_signals')
def api_insiders_top():
    """Get top signals with filters"""
    try:
        _ensure_insider_tables()
        limit = request.args.get('limit', 50, type=int)
        min_score = request.args.get('min_score', 0, type=int)
        days = request.args.get('days', 7, type=int)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        since_date = (datetime.now() - timedelta(days=days)).isoformat()
        
        if is_postgres:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE s.signal_score >= %s
                AND s.created_at >= %s
                ORDER BY s.signal_score DESC, s.created_at DESC
                LIMIT %s
            ''', (min_score, since_date, limit))
        else:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE s.signal_score >= ?
                AND s.created_at >= ?
                ORDER BY s.signal_score DESC, s.created_at DESC
                LIMIT ?
            ''', (min_score, since_date, limit))
        
        columns = ['id', 'filing_id', 'ticker', 'signal_score', 'insider_name', 'insider_role',
                   'transaction_type', 'dollar_value', 'reason_flags', 'is_highlighted', 'is_conviction',
                   'created_at', 'company_name', 'filing_url', 'shares', 'price',
                   'ownership_change_percent', 'filing_date', 'transaction_date']
        signals = [dict(zip(columns, row)) for row in cursor.fetchall()]
        conn.close()
        
        return jsonify({
            'success': True,
            'count': len(signals),
            'filters': {'limit': limit, 'min_score': min_score, 'days': days},
            'signals': signals
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e), "signals": []}), 500

@app.route('/api/insiders/ticker/<symbol>')
@feature_required('insider_signals')
def api_insiders_ticker(symbol):
    """Get signals for specific ticker"""
    try:
        _ensure_insider_tables()
        symbol = symbol.upper()
        limit = request.args.get('limit', 20, type=int)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE s.ticker = %s
                ORDER BY s.created_at DESC
                LIMIT %s
            ''', (symbol, limit))
        else:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE s.ticker = ?
                ORDER BY s.created_at DESC
                LIMIT ?
            ''', (symbol, limit))
        
        columns = ['id', 'filing_id', 'ticker', 'signal_score', 'insider_name', 'insider_role',
                   'transaction_type', 'dollar_value', 'reason_flags', 'is_highlighted', 'is_conviction',
                   'created_at', 'company_name', 'filing_url', 'shares', 'price',
                   'ownership_change_percent', 'filing_date', 'transaction_date']
        signals = [dict(zip(columns, row)) for row in cursor.fetchall()]
        conn.close()
        
        return jsonify({
            'success': True,
            'ticker': symbol,
            'count': len(signals),
            'signals': signals
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e), "signals": []}), 500

@app.route('/api/insiders/conviction')
@feature_required('insider_signals')
def api_insiders_conviction():
    """Get high conviction signals"""
    try:
        _ensure_insider_tables()
        limit = request.args.get('limit', 20, type=int)
        days = request.args.get('days', 30, type=int)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        since_date = (datetime.now() - timedelta(days=days)).isoformat()
        
        if is_postgres:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE s.is_conviction = 1
                AND s.created_at >= %s
                ORDER BY s.signal_score DESC, s.created_at DESC
                LIMIT %s
            ''', (since_date, limit))
        else:
            cursor.execute('''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE s.is_conviction = 1
                AND s.created_at >= ?
                ORDER BY s.signal_score DESC, s.created_at DESC
                LIMIT ?
            ''', (since_date, limit))
        
        columns = ['id', 'filing_id', 'ticker', 'signal_score', 'insider_name', 'insider_role',
                   'transaction_type', 'dollar_value', 'reason_flags', 'is_highlighted', 'is_conviction',
                   'created_at', 'company_name', 'filing_url', 'shares', 'price',
                   'ownership_change_percent', 'filing_date', 'transaction_date']
        signals = [dict(zip(columns, row)) for row in cursor.fetchall()]
        conn.close()
        
        return jsonify({
            'success': True,
            'count': len(signals),
            'signals': signals
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e), "signals": []}), 500

@app.route('/api/insiders/refresh', methods=['POST'])
@feature_required('insider_signals')
def api_insiders_refresh():
    """Trigger manual refresh - only works in SQLite mode"""
    try:
        _ensure_insider_tables()
        if is_using_postgres():
            return jsonify({
                'success': True,
                'message': 'PostgreSQL mode - manual refresh not available (data is from SEC filings)'
            })
        thread = threading.Thread(target=insider_service.process_filings, daemon=True)
        thread.start()
        return jsonify({
            'success': True,
            'message': 'Refresh triggered - processing in background'
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/insiders/price/<ticker>')
@feature_required('insider_signals')
def api_insiders_price(ticker):
    """Get stock price using Yahoo Finance"""
    try:
        # Use Yahoo Finance directly (no insider_service dependency)
        import time as time_module
        url = f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker.upper()}"
        params = {'interval': '1d', 'range': '5d'}
        headers = {'User-Agent': 'JustTrades/1.0'}
        
        response = requests.get(url, params=params, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            result = data.get('chart', {}).get('result', [])
            
            if result:
                meta = result[0].get('meta', {})
                price = meta.get('regularMarketPrice')
                prev_close = meta.get('previousClose') or meta.get('chartPreviousClose')
                
                if price:
                    change = None
                    change_pct = None
                    if prev_close:
                        change = round(price - prev_close, 2)
                        change_pct = round((change / prev_close) * 100, 2)
                    
                    return jsonify({
                        'success': True,
                        'ticker': ticker.upper(),
                        'price': round(price, 2),
                        'change': change,
                        'change_pct': change_pct
                    })
        
        return jsonify({'success': False, 'ticker': ticker.upper(), 'error': 'Price unavailable'}), 404
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/insiders/watchlist', methods=['GET', 'POST'])
@feature_required('insider_signals')
def api_insiders_watchlist():
    """Watchlist operations"""
    try:
        _ensure_insider_tables()
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if request.method == 'POST':
            data = request.get_json()
            watch_type = data.get('type', 'ticker')
            watch_value = data.get('value', '').strip().upper() if watch_type == 'ticker' else data.get('value', '').strip()
            notes = data.get('notes', '')
            
            if not watch_value:
                return jsonify({'success': False, 'error': 'Value is required'}), 400
            
            try:
                if is_postgres:
                    cursor.execute('''
                        INSERT INTO insider_watchlist (watch_type, watch_value, notes)
                        VALUES (%s, %s, %s) RETURNING id
                    ''', (watch_type, watch_value, notes))
                    watchlist_id = cursor.fetchone()[0]
                else:
                    cursor.execute('''
                        INSERT INTO insider_watchlist (watch_type, watch_value, notes)
                        VALUES (?, ?, ?)
                    ''', (watch_type, watch_value, notes))
                    watchlist_id = cursor.lastrowid
                conn.commit()
                conn.close()
                return jsonify({'success': True, 'id': watchlist_id, 'message': f'Added {watch_value} to watchlist'})
            except Exception as e:
                conn.close()
                if 'unique' in str(e).lower() or 'duplicate' in str(e).lower():
                    return jsonify({'success': False, 'error': 'Already in watchlist'}), 409
                raise
        else:
            cursor.execute('SELECT id, watch_type, watch_value, notes, created_at FROM insider_watchlist ORDER BY created_at DESC')
            columns = ['id', 'watch_type', 'watch_value', 'notes', 'created_at']
            items = [dict(zip(columns, row)) for row in cursor.fetchall()]
            conn.close()
            return jsonify({'success': True, 'count': len(items), 'watchlist': items})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/insiders/watchlist/<int:item_id>', methods=['DELETE'])
@feature_required('insider_signals')
def api_insiders_watchlist_delete(item_id):
    """Delete watchlist item"""
    try:
        _ensure_insider_tables()
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            cursor.execute('DELETE FROM insider_watchlist WHERE id = %s', (item_id,))
        else:
            cursor.execute('DELETE FROM insider_watchlist WHERE id = ?', (item_id,))
        deleted = cursor.rowcount > 0
        conn.commit()
        conn.close()
        
        if deleted:
            return jsonify({'success': True, 'message': 'Removed from watchlist'})
        else:
            return jsonify({'success': False, 'error': 'Item not found'}), 404
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/insiders/watchlist/signals')
@feature_required('insider_signals')
def api_insiders_watchlist_signals():
    """Get signals matching watchlist"""
    try:
        _ensure_insider_tables()
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        cursor.execute('SELECT watch_type, watch_value FROM insider_watchlist')
        watchlist = cursor.fetchall()
        
        if not watchlist:
            conn.close()
            return jsonify({'success': True, 'count': 0, 'signals': []})
        
        ticker_list = [w[1] for w in watchlist if w[0] == 'ticker']
        signals = []
        
        if ticker_list:
            if is_postgres:
                placeholders = ','.join(['%s' for _ in ticker_list])
            else:
                placeholders = ','.join(['?' for _ in ticker_list])
            
            query = f'''
                SELECT s.id, s.filing_id, s.ticker, s.signal_score, s.insider_name, s.insider_role,
                       s.transaction_type, s.dollar_value, s.reason_flags, s.is_highlighted, s.is_conviction,
                       s.created_at, f.company_name, f.filing_url, f.shares, f.price,
                       f.ownership_change_percent, f.filing_date, f.transaction_date
                FROM insider_signals s
                JOIN insider_filings f ON s.filing_id = f.id
                WHERE s.ticker IN ({placeholders})
                ORDER BY s.created_at DESC
                LIMIT 50
            '''
            cursor.execute(query, ticker_list)
            
            columns = ['id', 'filing_id', 'ticker', 'signal_score', 'insider_name', 'insider_role',
                       'transaction_type', 'dollar_value', 'reason_flags', 'is_highlighted', 'is_conviction',
                       'created_at', 'company_name', 'filing_url', 'shares', 'price',
                       'ownership_change_percent', 'filing_date', 'transaction_date']
            signals = [dict(zip(columns, row)) for row in cursor.fetchall()]
        
        conn.close()
        return jsonify({'success': True, 'count': len(signals), 'signals': signals})
    except Exception as e:
        return jsonify({"success": False, "error": str(e), "signals": []}), 500

# =============================================================================
# END INSIDER SIGNALS ROUTES
# =============================================================================

@app.route('/accounts')
def accounts():
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    # Inject the fetch MD token script into the template context
    return render_template('account_management.html', include_md_token_script=True)

@app.route('/api/accounts', methods=['GET'])
def get_accounts():
    """Get all accounts for the current user"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Filter by user_id if user is logged in - STRICT: only show user's own accounts
        if USER_AUTH_AVAILABLE and is_logged_in():
            user_id = get_current_user_id()
            if is_postgres:
                cursor.execute("SELECT * FROM accounts WHERE user_id = %s", (user_id,))
            else:
                cursor.execute("SELECT * FROM accounts WHERE user_id = ?", (user_id,))
        else:
            # Not logged in - show nothing (require login)
            cursor.execute("SELECT * FROM accounts WHERE 1=0")
        accounts = cursor.fetchall()
        conn.close()
        
        # Convert rows to dict format
        if accounts and not hasattr(accounts[0], 'keys'):
            # PostgreSQL returns tuples, need to convert
            columns = ['id', 'user_id', 'name', 'broker', 'auth_type', 'enabled', 'username', 'password', 
                      'client_id', 'client_secret', 'environment', 'access_token', 'refresh_token', 
                      'md_access_token', 'token_expiry', 'tradovate_accounts', 'subaccounts', 'created_at', 'updated_at']
            accounts = [dict(zip(columns[:len(row)], row)) for row in accounts]
        
        accounts_list = []
        for account in accounts:
            # Convert to dict if needed
            if not isinstance(account, dict):
                account = dict(account)
            
            parsed_subaccounts = []
            try:
                if account.get('subaccounts'):
                    parsed_subaccounts = json.loads(account['subaccounts'])
            except Exception as parse_err:
                logger.warning(f"Unable to parse subaccounts for account {account.get('id')}: {parse_err}")
                parsed_subaccounts = []
            
            parsed_tradovate_accounts = []
            try:
                if account.get('tradovate_accounts'):
                    raw_tradovate_accounts = json.loads(account['tradovate_accounts'])
                    if isinstance(raw_tradovate_accounts, list):
                        for raw_acct in raw_tradovate_accounts:
                            acct_copy = dict(raw_acct) if isinstance(raw_acct, dict) else {}
                            if 'is_demo' not in acct_copy:
                                env_value = acct_copy.get('environment') or acct_copy.get('env')
                                name_value = acct_copy.get('name') or ''
                                inferred_demo = False
                                if isinstance(env_value, str):
                                    inferred_demo = env_value.lower() == 'demo'
                                elif isinstance(name_value, str):
                                    inferred_demo = name_value.upper().startswith('DEMO')
                                acct_copy['is_demo'] = inferred_demo
                            parsed_tradovate_accounts.append(acct_copy)
            except Exception as parse_err:
                logger.warning(f"Unable to parse tradovate_accounts for account {account.get('id')}: {parse_err}")
                parsed_tradovate_accounts = []
            
            has_demo = any(sub.get('is_demo') for sub in parsed_subaccounts) or \
                any(trad.get('is_demo') for trad in parsed_tradovate_accounts)
            has_live = any(not sub.get('is_demo') for sub in parsed_subaccounts) or \
                any(not trad.get('is_demo') for trad in parsed_tradovate_accounts)
            
            # Check for access token (tradovate_token is legacy name, access_token is new)
            has_token = bool(account.get('access_token') or account.get('tradovate_token'))
            # ProjectX accounts use projectx_username instead of tradovate tokens
            has_projectx = bool(account.get('projectx_username') or account.get('projectx_api_key'))
            is_connected = has_token or has_projectx or bool(account.get('is_connected'))

            accounts_list.append({
                'id': account.get('id'),
                'name': account.get('name'),
                'broker': account.get('broker', ''),
                'enabled': bool(account.get('enabled', True)),
                'created_at': str(account.get('created_at', '')),
                'tradovate_token': has_token,
                'is_connected': is_connected,
                'subaccounts': parsed_subaccounts,
                'tradovate_accounts': parsed_tradovate_accounts,
                'has_demo': has_demo,
                'has_live': has_live
            })
        
        return jsonify({'success': True, 'accounts': accounts_list, 'count': len(accounts_list)})
    except Exception as e:
        logger.error(f"Error getting accounts: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/accounts', methods=['POST'])
def create_account():
    """Create a new account"""
    try:
        data = request.get_json()
        account_name = data.get('accountName') or data.get('name', '').strip()
        
        if not account_name:
            return jsonify({'success': False, 'error': 'Account name is required'}), 400
        
        # Get current user_id for data isolation FIRST
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()

        # Enforce tier-based account limits
        if current_user_id and SUBSCRIPTION_SYSTEM_AVAILABLE:
            is_admin_user = False
            try:
                user = get_current_user()
                if user and user.is_admin:
                    is_admin_user = True
            except Exception:
                pass
            if not is_admin_user:
                count_conn = get_db_connection()
                count_cursor = count_conn.cursor()
                count_ph = '%s' if is_using_postgres() else '?'
                count_cursor.execute(f"SELECT COUNT(*) FROM accounts WHERE user_id = {count_ph}", (current_user_id,))
                current_count = count_cursor.fetchone()[0]
                count_conn.close()
                if not check_limit(current_user_id, 'max_broker_accounts', current_count):
                    return jsonify({'success': False, 'error': 'You have reached the maximum number of broker accounts for your plan. Please upgrade to add more.'}), 403

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Check if account name already exists FOR THIS USER (not globally)
        ph = '%s' if is_postgres else '?'
        if current_user_id:
            cursor.execute(f"SELECT id FROM accounts WHERE name = {ph} AND user_id = {ph}", (account_name, current_user_id))
        else:
            cursor.execute(f"SELECT id FROM accounts WHERE name = {ph} AND user_id IS NULL", (account_name,))
        
        if cursor.fetchone():
            conn.close()
            return jsonify({'success': False, 'error': 'You already have an account with this name'}), 400
        
        # Insert new account with default auth_type and user_id
        if is_postgres:
            # PostgreSQL: use RETURNING to get the ID
            cursor.execute("""
                INSERT INTO accounts (name, broker, auth_type, enabled, created_at, user_id)
                VALUES (%s, %s, %s, %s, %s, %s) RETURNING id
            """, (account_name, 'Tradovate', 'oauth', True, datetime.now().isoformat(), current_user_id))
            result = cursor.fetchone()
            # Result is a dict from RealDictCursor
            if result:
                account_id = result.get('id') if isinstance(result, dict) else result[0]
            else:
                account_id = None
        else:
            # SQLite: use lastrowid
            cursor.execute("""
                INSERT INTO accounts (name, broker, auth_type, enabled, created_at, user_id)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (account_name, 'Tradovate', 'oauth', 1, datetime.now().isoformat(), current_user_id))
            account_id = cursor.lastrowid
        
        conn.commit()
        conn.close()
        
        # Return success with redirect URL for broker selection
        return jsonify({
            'success': True,
            'account_id': account_id,
            'redirect': True,
            'broker_selection_url': f'/accounts/{account_id}/broker-selection',
            'connect_url': f'/api/accounts/{account_id}/connect'
        })
    except Exception as e:
        logger.error(f"Error creating account: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/accounts/<int:account_id>/broker-selection')
def broker_selection(account_id):
    """Render the broker selection page for a new account"""
    return render_template('broker_selection.html', account_id=account_id)

@app.route('/api/accounts/<int:account_id>/set-broker', methods=['POST'])
def set_broker(account_id):
    """Set broker for an account"""
    try:
        data = request.get_json()
        broker_name = data.get('broker') or data.get('brokerName', 'Tradovate')
        
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f"UPDATE accounts SET broker = {ph} WHERE id = {ph}", (broker_name, account_id))
        conn.commit()
        conn.close()
        
        return jsonify({'success': True, 'broker': broker_name})
    except Exception as e:
        logger.error(f"Error setting broker: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================
# Broker Agreement Page
# ============================================================

# Feature configurations for each broker
BROKER_FEATURES = {
    'tradovate': {
        'broker_name': 'Tradovate',
        'asset_tag': 'Futures',
        'broker_icon': 'show_chart',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="dash">‚Äî</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/credentials'
    },
    'ninjatrader': {
        'broker_name': 'NinjaTrader',
        'asset_tag': 'Futures',
        'broker_icon': 'insights',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="dash">‚Äî</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/credentials'
    },
    'projectx': {
        'broker_name': 'ProjectX / TopstepX',
        'asset_tag': 'Futures (Prop Firm)',
        'broker_icon': 'trending_up',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="dash">‚Äî</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/projectx-credentials'
    },
    'webull': {
        'broker_name': 'Webull',
        'asset_tag': 'Stocks & Options',
        'broker_icon': 'account_balance',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="dash">‚Äî</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/webull-credentials'
    },
    'rithmic': {
        'broker_name': 'Rithmic',
        'asset_tag': 'Futures (Pro Infrastructure)',
        'broker_icon': 'speed',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="check">‚úì</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/rithmic-credentials'
    },
    'tradestation': {
        'broker_name': 'TradeStation',
        'asset_tag': 'Stocks, Futures & Options',
        'broker_icon': 'timeline',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/tradestation-credentials'
    },
    'alpaca': {
        'broker_name': 'Alpaca',
        'asset_tag': 'Stocks & Crypto',
        'broker_icon': 'code',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="dash">‚Äî</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="check">‚úì</span>',
        },
        'continue_path': '/accounts/{account_id}/alpaca-credentials'
    },
    'robinhood': {
        'broker_name': 'Robinhood',
        'asset_tag': 'Stocks, Options & Crypto',
        'broker_icon': 'local_florist',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="dash">‚Äî</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="dash">‚Äî</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="dash">‚Äî</span>',
            'shorting': '<span class="dash">‚Äî</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="dash">‚Äî</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/robinhood-credentials'
    },
    'interactivebrokers': {
        'broker_name': 'Interactive Brokers',
        'asset_tag': 'Multi-Asset Global',
        'broker_icon': 'public',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="check">‚úì</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="check">‚úì</span>',
        },
        'continue_path': '/accounts/{account_id}/ibkr-credentials'
    },
    'etrade': {
        'broker_name': 'E*TRADE',
        'asset_tag': 'Stocks & Options',
        'broker_icon': 'star',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="dash">‚Äî</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/etrade-credentials'
    },
    'tastytrade': {
        'broker_name': 'tastytrade',
        'asset_tag': 'Options & Futures',
        'broker_icon': 'restaurant',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/tastytrade-credentials'
    },
    'tradier': {
        'broker_name': 'Tradier',
        'asset_tag': 'Stocks & Options',
        'broker_icon': 'api',
        'features': {
            'stocks': '<span class="check">‚úì</span>',
            'futures': '<span class="dash">‚Äî</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="dash">‚Äî</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="dash">‚Äî</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/tradier-credentials'
    },
    'coinbase': {
        'broker_name': 'Coinbase',
        'asset_tag': 'Crypto',
        'broker_icon': 'currency_bitcoin',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="dash">‚Äî</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="dash">‚Äî</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="dash">‚Äî</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="dash">‚Äî</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/coinbase-credentials'
    },
    'kraken': {
        'broker_name': 'Kraken',
        'asset_tag': 'Crypto',
        'broker_icon': 'waves',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="dash">‚Äî</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/kraken-credentials'
    },
    'binance': {
        'broker_name': 'Binance',
        'asset_tag': 'Crypto',
        'broker_icon': 'diamond',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="check">‚úì</span>',
        },
        'continue_path': '/accounts/{account_id}/binance-credentials'
    },
    'bybit': {
        'broker_name': 'Bybit',
        'asset_tag': 'Crypto Derivatives',
        'broker_icon': 'bolt',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="check">‚úì</span>',
            'options': '<span class="check">‚úì</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="check">‚úì</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="check">‚úì</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="check">‚úì</span>',
            'stop_loss_legs': '<span class="check">‚úì</span>',
            'take_profit_legs': '<span class="check">‚úì</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="check">‚úì</span>',
        },
        'continue_path': '/accounts/{account_id}/bybit-credentials'
    },
    'cryptocom': {
        'broker_name': 'Crypto.com',
        'asset_tag': 'Crypto',
        'broker_icon': 'hexagon',
        'features': {
            'stocks': '<span class="dash">‚Äî</span>',
            'futures': '<span class="dash">‚Äî</span>',
            'options': '<span class="dash">‚Äî</span>',
            'crypto': '<span class="check">‚úì</span>',
            'live': '<span class="check">‚úì</span>',
            'paper': '<span class="dash">‚Äî</span>',
            'auto_submit': '<span class="check">‚úì</span>',
            'side_swapping': '<span class="check">‚úì</span>',
            'shorting': '<span class="dash">‚Äî</span>',
            'fractional': '<span class="check">‚úì</span>',
            'multiple_orders': '<span class="check">‚úì</span>',
            'stop_losses': '<span class="check">‚úì</span>',
            'take_profits': '<span class="check">‚úì</span>',
            'trailing_stops': '<span class="dash">‚Äî</span>',
            'stop_loss_legs': '<span class="dash">‚Äî</span>',
            'take_profit_legs': '<span class="dash">‚Äî</span>',
            'trailing_stop_legs': '<span class="dash">‚Äî</span>',
            'market_data': '<span class="check">‚úì</span>',
            'paper_market_data_source': '<span class="dash">‚Äî</span>',
        },
        'continue_path': '/accounts/{account_id}/cryptocom-credentials'
    },
}


@app.route('/accounts/<int:account_id>/connect/<broker_slug>/agreement')
def broker_agreement(account_id, broker_slug):
    """Render the broker agreement page before connecting.

    Shows disclaimer, terms, feature support matrix, and requires checkbox agreement.
    Flow: Broker Selection -> Agreement -> Credentials/OAuth
    """
    # Get broker config or use defaults
    broker_config = BROKER_FEATURES.get(broker_slug.lower(), {})

    # Build context
    broker_name = broker_config.get('broker_name', broker_slug.title())
    asset_tag = broker_config.get('asset_tag', 'Trading')
    broker_icon = broker_config.get('broker_icon', 'account_balance')
    features = broker_config.get('features', {})
    continue_path = broker_config.get('continue_path', '/accounts/{account_id}/credentials')
    continue_url = continue_path.format(account_id=account_id)

    return render_template('broker_agreement.html',
        broker_name=broker_name,
        broker_slug=broker_slug.lower(),
        account_id=account_id,
        asset_tag=asset_tag,
        broker_icon=broker_icon,
        features=features,
        continue_url=continue_url
    )


@app.route('/accounts/<int:account_id>/credentials')
def collect_credentials(account_id):
    """Render credentials collection page for Tradovate account"""
    return render_template('collect_credentials.html', account_id=account_id)

@app.route('/api/accounts/<int:account_id>/store-credentials', methods=['POST'])
def store_credentials(account_id):
    """Store username/password for an account (optional, for mdAccessToken)"""
    try:
        data = request.get_json()
        username = data.get('username', '').strip()
        password = data.get('password', '').strip()
        environment = data.get('environment', 'demo')  # 'live' or 'demo'
        
        if not username or not password:
            return jsonify({
                'success': False,
                'error': 'Username and password are required'
            }), 400
        
        # Store credentials in database
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            UPDATE accounts 
            SET username = ?, 
                password = ?,
                environment = ?
            WHERE id = ?
        """, (username, password, environment, account_id))
        conn.commit()
        conn.close()
        
        logger.info(f"Stored credentials for account {account_id}")
        
        # Optionally fetch mdAccessToken immediately if we want
        # For now, just store and continue to OAuth
        
        return jsonify({
            'success': True,
            'message': 'Credentials stored successfully',
            'redirect_url': f'/api/accounts/{account_id}/connect'
        })
    except Exception as e:
        logger.error(f"Error storing credentials: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

# ============================================================
# ProjectX / TopstepX Routes (Added Jan 2026)
# ============================================================

@app.route('/accounts/<int:account_id>/projectx-credentials')
def projectx_credentials(account_id):
    """Render ProjectX credentials collection page"""
    return render_template('projectx_credentials.html', account_id=account_id)

@app.route('/api/accounts/<int:account_id>/projectx-connect', methods=['POST'])
def projectx_connect(account_id):
    """Test and store ProjectX credentials (supports both password and API key auth)"""
    try:
        data = request.get_json()
        username = data.get('username', '').strip()
        password = data.get('password', '').strip() if data.get('password') else None
        api_key = data.get('api_key', '').strip() if data.get('api_key') else None
        auth_method = data.get('auth_method', 'password')  # 'password' (FREE) or 'apikey'
        environment = data.get('environment', 'demo')
        
        if not username:
            return jsonify({
                'success': False,
                'error': 'Username is required'
            }), 400
        
        if auth_method == 'password' and not password:
            return jsonify({
                'success': False,
                'error': 'Password is required for password authentication'
            }), 400
        
        if auth_method == 'apikey' and not api_key:
            return jsonify({
                'success': False,
                'error': 'API key is required for API key authentication'
            }), 400
        
        # Test connection with ProjectX
        import asyncio
        from phantom_scraper.projectx_integration import ProjectXIntegration
        
        async def test_projectx():
            is_demo = environment == 'demo'
            prop_firm = data.get('prop_firm', 'default')
            
            async with ProjectXIntegration(demo=is_demo, prop_firm=prop_firm) as projectx:
                # Need either password or API key
                if not password and not api_key:
                    return {
                        'success': False,
                        'error': 'Please provide either password (FREE) or API key ($14.50/mo subscription)',
                    }
                
                # Try authentication with provided credentials
                login_result = await projectx.login(username, password=password, api_key=api_key)
                
                if not login_result.get('success'):
                    # Return the detailed error message from the auth attempt
                    error_detail = login_result.get('error', 'Unknown authentication error')
                    return {
                        'success': False, 
                        'error': error_detail,
                        'debug_info': {
                            'username': username,
                            'prop_firm': prop_firm,
                            'environment': 'demo' if is_demo else 'live',
                            'auth_method': auth_method,
                            'password_provided': bool(password),
                            'api_key_provided': bool(api_key),
                        }
                    }

                accounts = await projectx.get_accounts()
                return {
                    'success': True,
                    'accounts': accounts,
                    'total_accounts': len(accounts),
                    'auth_method': login_result.get('method', projectx.auth_method),
                    'prop_firm': prop_firm
                }
        
        # Run async test
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(test_projectx())
        finally:
            loop.close()
        
        if not result.get('success'):
            return jsonify(result), 400
        
        # Store credentials in database
        conn = get_db_connection()
        cursor = conn.cursor()

        # Handle PostgreSQL vs SQLite differences
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        connected_val = 'TRUE' if is_postgres else '1'

        # Get the ProjectX accounts found and store them
        projectx_accounts = result.get('accounts', [])
        projectx_accounts_json = json.dumps(projectx_accounts) if projectx_accounts else None
        prop_firm = data.get('prop_firm', 'default')

        # Store password OR api_key based on auth method
        # Also store ProjectX-specific fields
        if auth_method == 'password':
            cursor.execute(f"""
                UPDATE accounts
                SET broker = 'ProjectX',
                    username = {ph},
                    password = {ph},
                    api_key = NULL,
                    environment = {ph},
                    is_connected = {connected_val},
                    projectx_username = {ph},
                    projectx_prop_firm = {ph},
                    tradovate_accounts = {ph}
                WHERE id = {ph}
            """, (username, password, environment, username, prop_firm, projectx_accounts_json, account_id))
        else:
            cursor.execute(f"""
                UPDATE accounts
                SET broker = 'ProjectX',
                    username = {ph},
                    password = NULL,
                    api_key = {ph},
                    environment = {ph},
                    is_connected = {connected_val},
                    projectx_username = {ph},
                    projectx_api_key = {ph},
                    projectx_prop_firm = {ph},
                    tradovate_accounts = {ph}
                WHERE id = {ph}
            """, (username, api_key, environment, username, api_key, prop_firm, projectx_accounts_json, account_id))
        
        conn.commit()
        conn.close()
        
        auth_desc = "FREE (password)" if auth_method == 'password' else "API key"
        logger.info(f"‚úÖ ProjectX credentials stored for account {account_id} ({auth_desc})")
        
        return jsonify({
            'success': True,
            'message': f'ProjectX connected successfully ({auth_desc})',
            'accounts': result.get('accounts', []),
            'total_accounts': result.get('total_accounts', 0),
            'auth_method': auth_method,
            'redirect_url': '/accounts'
        })
        
    except Exception as e:
        logger.error(f"Error connecting ProjectX: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

# ============================================================
# End ProjectX Routes
# ============================================================

# ============================================================
# WEBULL ROUTES
# ============================================================

@app.route('/accounts/<int:account_id>/webull-credentials')
def webull_credentials(account_id):
    """Render Webull credentials collection page"""
    return render_template('webull_credentials.html', account_id=account_id)

@app.route('/api/accounts/<int:account_id>/webull-connect', methods=['POST'])
def webull_connect(account_id):
    """Test and store Webull credentials"""
    try:
        data = request.get_json()
        app_key = data.get('app_key', '').strip()
        app_secret = data.get('app_secret', '').strip()

        if not app_key or not app_secret:
            return jsonify({
                'success': False,
                'error': 'App Key and App Secret are required'
            }), 400

        # Test connection with Webull
        import asyncio
        from phantom_scraper.webull_integration import WebullIntegration

        async def test_webull():
            async with WebullIntegration(app_key, app_secret) as webull:
                login_result = await webull.login()
                
                if not login_result.get('success'):
                    return {
                        'success': False,
                        'error': login_result.get('error', 'Authentication failed'),
                    }

                accounts = await webull.get_accounts()
                return {
                    'success': True,
                    'accounts': accounts,
                    'total_accounts': len(accounts),
                }

        # Run async test
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(test_webull())
        finally:
            loop.close()
        
        if not result.get('success'):
            return jsonify(result), 400
        
        # Store credentials in database
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute("""
                UPDATE accounts 
                SET broker = 'Webull',
                    username = %s, 
                    api_key = %s,
                    is_connected = TRUE
                WHERE id = %s
            """, (app_key, app_secret, account_id))
        else:
            cursor.execute("""
                UPDATE accounts 
                SET broker = 'Webull',
                    username = ?, 
                    api_key = ?,
                    is_connected = 1
                WHERE id = ?
            """, (app_key, app_secret, account_id))
        
        conn.commit()
        conn.close()
        
        logger.info(f"‚úÖ Webull credentials stored for account {account_id}")
        
        return jsonify({
            'success': True,
            'message': 'Webull connected successfully',
            'accounts': result.get('accounts', []),
            'total_accounts': result.get('total_accounts', 0),
            'redirect_url': '/accounts'
        })
        
    except Exception as e:
        logger.error(f"Error connecting Webull: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

# ============================================================
# End Webull Routes
# ============================================================

@app.route('/api/accounts/<int:account_id>/connect')
def connect_account(account_id):
    """Redirect to Tradovate OAuth connection"""
    try:
        # ALWAYS use these OAuth app credentials: cid: 8699, secret: 7c74576b-20b1-4ea5-a2a0-eaeb11326a95
        # Do not use database values - these are the only credentials that work
        DEFAULT_CLIENT_ID = "8699"
        DEFAULT_CLIENT_SECRET = "7c74576b-20b1-4ea5-a2a0-eaeb11326a95"

        client_id = DEFAULT_CLIENT_ID  # Always use 8699
        
        # Get environment from query parameter (demo or live)
        # CRITICAL: Demo accounts require demo OAuth endpoint, live accounts require live OAuth endpoint
        env = request.args.get('env', '').lower()
        
        # SMART DEFAULT (Jan 14, 2026): If no env specified, check if account has live subaccounts
        # This fixes the bug where Reconnect button didn't specify env and defaulted to demo
        if not env:
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                ph = '%s' if is_using_postgres() else '?'
                cursor.execute(f"SELECT tradovate_accounts, environment FROM accounts WHERE id = {ph}", (account_id,))
                row = cursor.fetchone()
                conn.close()
                
                if row:
                    existing_env = row[1] if row[1] else None
                    tradovate_accounts_json = row[0]
                    
                    # Priority 1: Use existing environment if set to 'live'
                    if existing_env == 'live':
                        env = 'live'
                        logger.info(f"üîë Account {account_id} has environment=live, using live OAuth")
                    # Priority 2: Check if account has any live subaccounts
                    elif tradovate_accounts_json:
                        import json
                        try:
                            accounts_data = json.loads(tradovate_accounts_json) if isinstance(tradovate_accounts_json, str) else tradovate_accounts_json
                            subaccounts = accounts_data.get('subaccounts', []) if isinstance(accounts_data, dict) else []
                            # Check for any live subaccounts (numeric names without "DEMO")
                            for sub in subaccounts:
                                sub_name = str(sub.get('name', '')).upper()
                                sub_env = sub.get('environment', '')
                                if sub_env == 'live' or (sub_name and 'DEMO' not in sub_name and sub_name.replace('-', '').isdigit()):
                                    env = 'live'
                                    logger.info(f"üîë Account {account_id} has live subaccount {sub_name}, using live OAuth")
                                    break
                        except Exception as json_err:
                            logger.warning(f"Could not parse tradovate_accounts JSON: {json_err}")
                    
                    if not env:
                        env = 'demo'
                        logger.info(f"üîë Account {account_id} defaulting to demo OAuth (no live subaccounts found)")
            except Exception as e:
                logger.warning(f"Could not determine account environment: {e}, defaulting to demo")
                env = 'demo'
        
        is_demo = env == 'demo'

        # Build redirect URI - use canonical domain in production, fallback for local dev
        # MUST match what's registered in Tradovate developer portal
        oauth_redirect_domain = os.environ.get('OAUTH_REDIRECT_DOMAIN')
        railway_url = os.environ.get('RAILWAY_PUBLIC_DOMAIN')
        if oauth_redirect_domain:
            redirect_uri = f'https://{oauth_redirect_domain}/api/oauth/callback'
            logger.info(f"Using OAUTH_REDIRECT_DOMAIN redirect_uri: {redirect_uri}")
        elif railway_url:
            redirect_uri = f'https://{railway_url}/api/oauth/callback'
            logger.info(f"Using Railway redirect_uri: {redirect_uri}")
        else:
            # Check for ngrok URL file (local dev with ngrok)
            try:
                with open('ngrok_url.txt', 'r') as f:
                    ngrok_url = f.read().strip()
                    if ngrok_url and ngrok_url.startswith('http'):
                        redirect_uri = f'{ngrok_url.rstrip("/")}/api/oauth/callback'
                        logger.info(f"Using ngrok redirect_uri: {redirect_uri}")
                    else:
                        redirect_uri = 'http://localhost:8082/api/oauth/callback'
                        logger.info(f"Using localhost redirect_uri: {redirect_uri}")
            except FileNotFoundError:
                redirect_uri = 'http://localhost:8082/api/oauth/callback'
                logger.info(f"Using localhost redirect_uri: {redirect_uri}")
        
        # Build OAuth URL
        # NOTE: OAuth login ALWAYS goes through trader.tradovate.com
        # The demo vs live distinction happens during TOKEN EXCHANGE, not login
        from urllib.parse import quote_plus
        encoded_redirect_uri = quote_plus(redirect_uri)
        # Pass both account_id and environment via state parameter (comma-separated)
        state_value = f"{account_id},{env}"
        encoded_state = quote_plus(state_value)
        
        # OAuth login is always at trader.tradovate.com - this is Tradovate's unified login
        oauth_base = 'https://trader.tradovate.com/oauth'
        logger.info(f"üîë OAuth login for account {account_id} (will use {env} token endpoint after)")
        
        oauth_url = f'{oauth_base}?response_type=code&client_id={client_id}&redirect_uri={encoded_redirect_uri}&scope=All&state={encoded_state}'
        
        logger.info(f"Redirecting account {account_id} to OAuth ({env}): {oauth_url}")
        logger.info(f"Redirect URI (decoded): {redirect_uri}")
        return redirect(oauth_url)
    except Exception as e:
        logger.error(f"Error connecting account: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/accounts/<int:account_id>/direct-connect', methods=['POST'])
def direct_connect_account(account_id):
    """
    Direct authentication with Tradovate using username/password
    This bypasses OAuth and authenticates directly with the Tradovate API
    """
    try:
        import requests
        
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400
        
        username = data.get('username')
        password = data.get('password')
        is_demo = data.get('is_demo', True)  # Default to demo for safety
        
        if not username or not password:
            return jsonify({'success': False, 'error': 'Username and password are required'}), 400
        
        # Use the correct API endpoint based on account type
        if is_demo:
            auth_url = 'https://demo.tradovateapi.com/v1/auth/accesstokenrequest'
            base_url = 'https://demo.tradovateapi.com/v1'
        else:
            auth_url = 'https://live.tradovateapi.com/v1/auth/accesstokenrequest'
            base_url = 'https://live.tradovateapi.com/v1'
        
        # Build authentication payload
        auth_payload = {
            'name': username,
            'password': password,
            'appId': 'JustTrades',
            'appVersion': '1.0',
            'cid': 8699,  # Our OAuth app client ID
            'sec': '7c74576b-20b1-4ea5-a2a0-eaeb11326a95'  # Our OAuth app secret
        }
        
        logger.info(f"Attempting direct auth for account {account_id} ({'demo' if is_demo else 'live'}) with user: {username}")
        
        # Make authentication request
        response = requests.post(auth_url, json=auth_payload, headers={'Content-Type': 'application/json'})
        
        if response.status_code != 200:
            error_text = response.text[:500]
            logger.error(f"Direct auth failed: {response.status_code} - {error_text}")
            return jsonify({'success': False, 'error': f'Authentication failed: {error_text}'}), 400
        
        token_data = response.json()
        
        # Check for error in response body (Tradovate returns 200 with error sometimes)
        if 'errorText' in token_data:
            logger.error(f"Direct auth error: {token_data['errorText']}")
            return jsonify({'success': False, 'error': token_data['errorText']}), 400
        
        if 'accessToken' not in token_data:
            logger.error(f"Direct auth missing accessToken: {token_data}")
            return jsonify({'success': False, 'error': 'No access token received'}), 400
        
        access_token = token_data.get('accessToken')
        expiration_time = token_data.get('expirationTime')
        user_id = token_data.get('userId')
        
        logger.info(f"‚úÖ Direct auth successful for account {account_id}, userId: {user_id}")
        
        # Store the tokens in database
        conn = get_db_connection()
        cursor = conn.cursor()
        
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        timestamp_fn = 'NOW()' if is_postgres else "datetime('now')"
        
        # Update account with tokens
        cursor.execute(f'''
            UPDATE accounts 
            SET access_token = {placeholder},
                token_expires_at = {placeholder},
                is_demo = {placeholder},
                enabled = true,
                updated_at = {timestamp_fn}
            WHERE id = {placeholder}
        ''', (access_token, expiration_time, is_demo, account_id))
        
        conn.commit()
        
        # Now fetch subaccounts using the new token
        try:
            accounts_url = f"{base_url}/account/list"
            headers = {'Authorization': f'Bearer {access_token}'}
            acct_response = requests.get(accounts_url, headers=headers)
            
            if acct_response.status_code == 200:
                tradovate_accounts = acct_response.json()
                logger.info(f"Fetched {len(tradovate_accounts)} subaccounts")
                
                # Store subaccounts as JSON
                import json
                subaccounts_json = json.dumps([{
                    'id': acc.get('id'),
                    'name': acc.get('name'),
                    'is_demo': is_demo
                } for acc in tradovate_accounts])
                
                cursor.execute(f'''
                    UPDATE accounts 
                    SET tradovate_accounts = {placeholder}
                    WHERE id = {placeholder}
                ''', (subaccounts_json, account_id))
                conn.commit()
        except Exception as sub_err:
            logger.warning(f"Failed to fetch subaccounts: {sub_err}")
        
        cursor.close()
        conn.close()
        
        return jsonify({
            'success': True, 
            'message': 'Connected successfully!',
            'user_id': user_id,
            'is_demo': is_demo
        })
        
    except Exception as e:
        logger.error(f"Error in direct connect: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/oauth/callback')
def oauth_callback():
    """Handle OAuth callback from Tradovate"""
    try:
        # Get account_id and environment from state parameter (format: "account_id,env")
        state = request.args.get('state')
        if not state:
            logger.error("No state in OAuth callback")
            return redirect(f'/accounts?error=no_account_id')
        
        # Parse state - can be "account_id,env" or just "account_id" (legacy)
        state_parts = state.split(',')
        account_id = int(state_parts[0])
        oauth_env = state_parts[1] if len(state_parts) > 1 else 'live'  # Default to live for legacy
        is_demo_oauth = oauth_env == 'demo'
        logger.info(f"OAuth callback for account {account_id}, environment: {oauth_env}")
        
        code = request.args.get('code')
        error = request.args.get('error')
        
        if error:
            error_msg = request.args.get('error_description', error)
            logger.error(f"OAuth error for account {account_id}: {error_msg}")
            return redirect(f'/accounts?error=oauth_error&message={error_msg}')
        
        if not code:
            logger.error(f"No authorization code received for account {account_id}")
            return redirect(f'/accounts?error=no_code')
        
        # ALWAYS use these OAuth app credentials: cid: 8699, secret: 7c74576b-20b1-4ea5-a2a0-eaeb11326a95
        # Do not use database values - these are the only credentials that work
        DEFAULT_CLIENT_ID = "8699"
        DEFAULT_CLIENT_SECRET = "7c74576b-20b1-4ea5-a2a0-eaeb11326a95"
        
        client_id = DEFAULT_CLIENT_ID  # Always use 8699
        client_secret = DEFAULT_CLIENT_SECRET  # Always use the secret
        
        # Build redirect_uri - must match what was sent in connect_account
        # MUST match what's registered in Tradovate developer portal
        oauth_redirect_domain = os.environ.get('OAUTH_REDIRECT_DOMAIN')
        railway_url = os.environ.get('RAILWAY_PUBLIC_DOMAIN')
        if oauth_redirect_domain:
            redirect_uri = f'https://{oauth_redirect_domain}/api/oauth/callback'
        elif railway_url:
            redirect_uri = f'https://{railway_url}/api/oauth/callback'
        else:
            # Check for ngrok URL file (local dev with ngrok)
            try:
                with open('ngrok_url.txt', 'r') as f:
                    ngrok_url = f.read().strip()
                    if ngrok_url and ngrok_url.startswith('http'):
                        redirect_uri = f'{ngrok_url.rstrip("/")}/api/oauth/callback'
                    else:
                        redirect_uri = 'http://localhost:8082/api/oauth/callback'
            except FileNotFoundError:
                redirect_uri = 'http://localhost:8082/api/oauth/callback'
        
        # Exchange authorization code for access token
        # CRITICAL (Jan 5, 2026): Use the MATCHING endpoint for the OAuth environment!
        # Demo OAuth code can only be exchanged at demo endpoint, live at live endpoint
        import requests
        if is_demo_oauth:
            token_endpoints = [
                'https://demo.tradovateapi.com/v1/auth/oauthtoken',
                'https://live.tradovateapi.com/v1/auth/oauthtoken'  # Fallback
            ]
            logger.info(f"üîµ Using DEMO token endpoint for account {account_id}")
        else:
            token_endpoints = [
                'https://live.tradovateapi.com/v1/auth/oauthtoken',
                'https://demo.tradovateapi.com/v1/auth/oauthtoken'  # Fallback
            ]
            logger.info(f"üü¢ Using LIVE token endpoint for account {account_id}")
        token_payload = {
            'grant_type': 'authorization_code',
            'code': code,
            'redirect_uri': redirect_uri,
            'client_id': client_id,
            'client_secret': client_secret  # Always include the secret
        }
        
        # Try each endpoint until one works
        response = None
        for token_url in token_endpoints:
            logger.info(f"Trying token exchange at: {token_url}")
            response = requests.post(token_url, json=token_payload, headers={'Content-Type': 'application/json'})
            if response.status_code == 200:
                logger.info(f"‚úÖ Token exchange succeeded at: {token_url}")
                break
            elif response.status_code == 429:
                logger.warning(f"‚ö†Ô∏è Rate limited (429) at {token_url}, trying next endpoint...")
                continue
            else:
                logger.warning(f"Token exchange failed at {token_url}: {response.status_code} - {response.text[:200]}")
                # For non-429 errors, still try next endpoint
                continue
        
        if response.status_code == 200:
            token_data = response.json()
            logger.info(f"OAuth token response keys: {list(token_data.keys())}")
            
            # Check for error in response body (Tradovate returns 200 with error in body)
            if 'error' in token_data:
                error_msg = token_data.get('error_description', token_data.get('error', 'Unknown error'))
                logger.error(f"OAuth token error from Tradovate: {error_msg}")
                return redirect(f'/accounts?error=oauth_error&message={error_msg}')
            
            access_token = token_data.get('accessToken') or token_data.get('access_token')
            refresh_token = token_data.get('refreshToken') or token_data.get('refresh_token')
            md_access_token = token_data.get('mdAccessToken') or token_data.get('md_access_token')
            
            logger.info(f"Tokens extracted - accessToken: {bool(access_token)}, refreshToken: {bool(refresh_token)}, mdAccessToken: {bool(md_access_token)}")
            
            # Calculate actual expiration time from Tradovate response
            # Tradovate returns expiresIn (seconds) or expirationTime (ISO timestamp)
            expires_at = None
            if 'expirationTime' in token_data:
                # ISO timestamp format
                try:
                    from datetime import datetime
                    expires_at = datetime.fromisoformat(token_data['expirationTime'].replace('Z', '+00:00'))
                    expires_at = expires_at.strftime('%Y-%m-%d %H:%M:%S')
                except Exception as e:
                    logger.warning(f"Could not parse expirationTime: {e}")
            elif 'expiresIn' in token_data:
                # Seconds until expiration - calculate datetime
                try:
                    from datetime import datetime, timedelta
                    expires_in_seconds = int(token_data['expiresIn'])
                    expires_at = (datetime.now() + timedelta(seconds=expires_in_seconds)).strftime('%Y-%m-%d %H:%M:%S')
                except Exception as e:
                    logger.warning(f"Could not parse expiresIn: {e}")
            
            # Fallback: Tradovate access tokens typically expire in 90 minutes
            # But we'll use 85 minutes to refresh proactively
            if not expires_at:
                from datetime import datetime, timedelta
                expires_at = (datetime.now() + timedelta(minutes=85)).strftime('%Y-%m-%d %H:%M:%S')
                logger.info(f"Using default expiration (85 minutes) - Tradovate didn't provide expiration time")
            else:
                logger.info(f"Storing actual expiration time: {expires_at}")
            
            # Store tokens in database with actual expiration time
            # CRITICAL FIX (Jan 14, 2026): Also update environment to match OAuth environment!
            # Without this, token refresh daemon uses wrong endpoint and gets demo tokens for live accounts
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE accounts 
                SET tradovate_token = ?, 
                    tradovate_refresh_token = ?,
                    md_access_token = ?,
                    token_expires_at = ?,
                    environment = ?
                WHERE id = ?
            """, (access_token, refresh_token, md_access_token, expires_at, oauth_env, account_id))
            conn.commit()
            conn.close()
            logger.info(f"‚úÖ Stored tokens for account {account_id} with environment={oauth_env}")
            
            # Clear any reauth flag - account is now authenticated!
            clear_account_reauth(account_id)
            logger.info(f"‚úÖ Account {account_id} OAuth successful - cleared reauth flag")
            
            # OAuth token exchange doesn't return mdAccessToken - try to get it via accessTokenRequest
            if not md_access_token and access_token:
                try:
                    # Check if we have username/password stored
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT username, password, client_id, client_secret, environment
                        FROM accounts WHERE id = ?
                    """, (account_id,))
                    creds = cursor.fetchone()
                    conn.close()
                    
                    if creds and creds[0] and creds[1]:  # Has username and password
                        username, password, client_id, client_secret, environment = creds
                        base_url = "https://live.tradovateapi.com/v1" if environment == 'live' else "https://demo.tradovateapi.com/v1"
                        
                        # Use OAuth client credentials for accessTokenRequest (same as OAuth flow)
                        # Use API credentials: cid: 8720, secret: e76ee8d1-d168-4252-a59e-f11a8b0cdae4
                        # These are used for fetching mdAccessToken via /auth/accesstokenrequest
                        DEFAULT_CLIENT_ID = str(TRADOVATE_API_CID)  # Use global API CID
                        DEFAULT_CLIENT_SECRET = TRADOVATE_API_SECRET  # Use global API secret
                        
                        # Make accessTokenRequest to get mdAccessToken
                        login_data = {
                            "name": username,
                            "password": password,
                            "appId": "Just.Trade",
                            "appVersion": "1.0.0",
                            "deviceId": f"Just.Trade-{account_id}",
                            "cid": DEFAULT_CLIENT_ID,  # Use OAuth client ID
                            "sec": DEFAULT_CLIENT_SECRET  # Use OAuth client secret
                        }
                        
                        logger.info(f"Fetching mdAccessToken via /auth/accesstokenrequest for account {account_id}")
                        token_response = requests.post(
                            f"{base_url}/auth/accesstokenrequest",
                            json=login_data,
                            headers={"Content-Type": "application/json"}
                        )
                        
                        if token_response.status_code == 200:
                            token_data = token_response.json()
                            logger.info(f"accessTokenRequest response keys: {list(token_data.keys())}")
                            
                            # Check for errors first
                            if 'errorText' in token_data:
                                error_msg = token_data.get('errorText', 'Unknown error')
                                logger.warning(f"accessTokenRequest returned error: {error_msg}")
                                if 'not registered' in error_msg.lower():
                                    logger.info("App not registered - this is expected if using OAuth client credentials with accessTokenRequest")
                                    logger.info("mdAccessToken may not be available via this method. WebSocket will use accessToken instead.")
                                elif 'incorrect username' in error_msg.lower() or 'password' in error_msg.lower():
                                    logger.warning("Credentials may be incorrect or don't match OAuth account")
                            else:
                                md_access_token = token_data.get('mdAccessToken') or token_data.get('md_access_token')
                                if md_access_token:
                                    # Update mdAccessToken in database
                                    conn = get_db_connection()
                                    cursor = conn.cursor()
                                    cursor.execute("""
                                        UPDATE accounts SET md_access_token = ? WHERE id = ?
                                    """, (md_access_token, account_id))
                                    conn.commit()
                                    conn.close()
                                    logger.info(f"‚úÖ Successfully retrieved and stored mdAccessToken for account {account_id}")
                                else:
                                    logger.warning(f"mdAccessToken not in accessTokenRequest response for account {account_id}")
                                    logger.info(f"Full response: {token_data}")
                        else:
                            error_text = token_response.text[:200] if hasattr(token_response, 'text') else str(token_response.status_code)
                            logger.warning(f"Failed to get mdAccessToken: {token_response.status_code} - {error_text}")
                    else:
                        logger.info(f"Account {account_id} doesn't have username/password stored. MD Token will be fetched when credentials are added.")
                        logger.info("Note: WebSocket will work with OAuth accessToken, but mdAccessToken provides better market data access.")
                except Exception as e:
                    logger.error(f"Error fetching mdAccessToken: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            
            # Fetch and store Tradovate account + subaccount metadata (TradersPost-style)
            # CRITICAL: Use the correct API base_url based on which OAuth environment was used
            if access_token:
                base_url = "https://demo.tradovateapi.com" if is_demo_oauth else "https://live.tradovateapi.com"
                logger.info(f"Fetching accounts from {base_url} for account {account_id} ({oauth_env} OAuth)")
                fetch_result = fetch_and_store_tradovate_accounts(account_id, access_token, base_url)
                if not fetch_result.get("success"):
                    logger.warning(f"Unable to fetch subaccounts after OAuth for account {account_id}: {fetch_result.get('error')}")
                else:
                    # Register subaccounts for cross-user abuse detection (flag-only, non-fatal)
                    try:
                        from subaccount_abuse_detection import register_subaccounts
                        _sub_conn = get_db_connection()
                        _sub_cursor = _sub_conn.cursor()
                        _sub_ph = '%s' if is_using_postgres() else '?'
                        _sub_cursor.execute(f'SELECT user_id FROM accounts WHERE id = {_sub_ph}', (account_id,))
                        _sub_row = _sub_cursor.fetchone()
                        _sub_conn.close()
                        if _sub_row:
                            _sub_user_id = _sub_row['user_id'] if isinstance(_sub_row, dict) else _sub_row[0]
                            register_subaccounts(_sub_user_id, account_id, fetch_result.get('subaccounts', []), broker='Tradovate')
                    except Exception as _sub_err:
                        logger.debug(f"Subaccount abuse check skipped: {_sub_err}")

            logger.info(f"Successfully stored tokens for account {account_id}")
            return redirect(f'/accounts?success=true&connected={account_id}')
        else:
            logger.error(f"Token exchange failed: {response.status_code} - {response.text}")
            return redirect(f'/accounts?error=token_exchange_failed&message={response.text[:100]}')
            
    except Exception as e:
        logger.error(f"Error in OAuth callback: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return redirect(f'/accounts?error=callback_error&message={str(e)}')

@app.route('/api/accounts/<int:account_id>', methods=['DELETE'])
def delete_account(account_id):
    """Delete an account"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f"DELETE FROM accounts WHERE id = {ph}", (account_id,))
        conn.commit()
        deleted = cursor.rowcount
        conn.close()
        
        if deleted > 0:
            logger.info(f"Deleted account {account_id}")
            return jsonify({'success': True, 'message': 'Account deleted successfully'})
        else:
            return jsonify({'success': False, 'error': 'Account not found'}), 404
    except Exception as e:
        logger.error(f"Error deleting account: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/accounts/<int:account_id>/refresh-subaccounts', methods=['POST'])
def refresh_account_subaccounts(account_id):
    """Refresh subaccounts for an account (Tradovate or ProjectX)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT tradovate_token, broker, username, password, api_key, environment,
                   projectx_username, projectx_api_key, projectx_prop_firm
            FROM accounts
            WHERE id = ?
        """, (account_id,))
        row = cursor.fetchone()
        conn.close()

        if not row:
            return jsonify({'success': False, 'error': 'Account not found'}), 404

        broker = ''
        if isinstance(row, dict):
            broker = (row.get('broker') or '').strip()
        else:
            broker = (row['broker'] or '').strip() if row['broker'] else ''

        # Detect ProjectX accounts by broker field OR by presence of ProjectX credentials
        px_username = row.get('projectx_username') if isinstance(row, dict) else row['projectx_username']
        px_api_key_val = row.get('projectx_api_key') if isinstance(row, dict) else row['projectx_api_key']
        is_projectx = (broker == 'ProjectX' or broker == 'TopstepX' or broker == 'TopStep'
                       or bool(px_username) or bool(px_api_key_val))

        # ProjectX accounts - re-fetch accounts via ProjectX API
        if is_projectx:
            try:
                import asyncio
                from phantom_scraper.projectx_integration import ProjectXIntegration

                px_username = row.get('projectx_username') or row.get('username') or ''
                px_password = row.get('password') or ''
                px_api_key = row.get('projectx_api_key') or row.get('api_key') or ''
                px_prop_firm = row.get('projectx_prop_firm') or 'default'
                is_demo = (row.get('environment') or 'demo') == 'demo'

                if not px_username:
                    return jsonify({'success': False, 'error': 'No ProjectX username stored. Please reconnect.'}), 400

                async def refresh_projectx():
                    async with ProjectXIntegration(demo=is_demo, prop_firm=px_prop_firm) as projectx:
                        login_result = await projectx.login(px_username, password=px_password if px_password else None, api_key=px_api_key if px_api_key else None)
                        if not login_result.get('success'):
                            return {'success': False, 'error': login_result.get('error', 'ProjectX login failed. Please reconnect.')}
                        accounts = await projectx.get_accounts()
                        return {'success': True, 'accounts': accounts}

                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(refresh_projectx())
                finally:
                    loop.close()

                if not result.get('success'):
                    return jsonify(result), 400

                # Store refreshed accounts and ensure broker is set correctly
                px_accounts = result.get('accounts', [])
                conn2 = get_db_connection()
                cursor2 = conn2.cursor()
                cursor2.execute("UPDATE accounts SET tradovate_accounts = ?, broker = 'ProjectX' WHERE id = ?", (json.dumps(px_accounts), account_id))
                conn2.commit()
                conn2.close()

                logger.info(f"Refreshed ProjectX subaccounts for account {account_id}: {len(px_accounts)} accounts")
                return jsonify({'success': True, 'subaccounts': px_accounts})
            except ImportError:
                return jsonify({'success': False, 'error': 'ProjectX integration not available'}), 500
            except Exception as px_err:
                logger.error(f"Error refreshing ProjectX subaccounts: {px_err}")
                return jsonify({'success': False, 'error': str(px_err)}), 500

        # Tradovate / NinjaTrader accounts
        tradovate_token = row.get('tradovate_token') if isinstance(row, dict) else row['tradovate_token']
        if not tradovate_token:
            return jsonify({'success': False, 'error': 'Account not connected to Tradovate. Please reconnect.'}), 400

        fetch_result = fetch_and_store_tradovate_accounts(account_id, tradovate_token)
        if fetch_result.get('success'):
            # Register subaccounts for cross-user abuse detection (flag-only, non-fatal)
            try:
                from subaccount_abuse_detection import register_subaccounts
                _ref_conn = get_db_connection()
                _ref_cur = _ref_conn.cursor()
                _ref_ph = '%s' if is_using_postgres() else '?'
                _ref_cur.execute(f'SELECT user_id FROM accounts WHERE id = {_ref_ph}', (account_id,))
                _ref_row = _ref_cur.fetchone()
                _ref_conn.close()
                if _ref_row:
                    _ref_uid = _ref_row['user_id'] if isinstance(_ref_row, dict) else _ref_row[0]
                    register_subaccounts(_ref_uid, account_id, fetch_result.get('subaccounts', []), broker='Tradovate')
            except Exception as _ref_err:
                logger.debug(f"Subaccount abuse check skipped on refresh: {_ref_err}")
            return jsonify({'success': True, 'subaccounts': fetch_result.get('subaccounts', [])})
        return jsonify({'success': False, 'error': fetch_result.get('error', 'Unable to refresh subaccounts')}), 400
    except Exception as e:
        logger.error(f"Error refreshing subaccounts: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/accounts/<int:account_id>/fetch-md-token', methods=['POST'])
def fetch_md_access_token(account_id):
    """Fetch mdAccessToken for an account - accepts credentials in request or uses stored ones"""
    try:
        data = request.get_json() or {}
        
        # Get credentials from request or database
        username = data.get('username')
        password = data.get('password')
        use_stored = data.get('use_stored', True)  # Default to using stored credentials
        
        if not username or not password:
            if use_stored:
                # Try to get from database
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT username, password, client_id, client_secret, environment
                    FROM accounts WHERE id = ?
                """, (account_id,))
                creds = cursor.fetchone()
                conn.close()
                
                if not creds or not creds[0] or not creds[1]:
                    return jsonify({
                        'success': False,
                        'error': 'No credentials provided and account does not have username/password stored.',
                        'instructions': 'Either provide username/password in the request body, or add them to the account first.'
                    }), 400
                
                username, password, client_id, client_secret, environment = creds
            else:
                return jsonify({
                    'success': False,
                    'error': 'Username and password required in request body'
                }), 400
        else:
            # Get environment and client credentials from database
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT client_id, client_secret, environment
                FROM accounts WHERE id = ?
            """, (account_id,))
            row = cursor.fetchone()
            conn.close()
            
            if row:
                client_id, client_secret, environment = row
            else:
                environment = 'demo'  # Default
                client_id = None
                client_secret = None
        
        base_url = "https://live.tradovateapi.com/v1" if environment == 'live' else "https://demo.tradovateapi.com/v1"
        
        # Make accessTokenRequest to get mdAccessToken
        # Use provided API credentials (prefer stored, fallback to defaults)
        login_data = {
            "name": username,
            "password": password,
            "appId": "Just.Trade",
            "appVersion": "1.0.0",
            "deviceId": f"Just.Trade-{account_id}",
            "cid": client_id or str(TRADOVATE_API_CID),  # Use stored or default API CID
            "sec": client_secret or TRADOVATE_API_SECRET  # Use stored or default API secret
        }
        
        logger.info(f"Fetching mdAccessToken for account {account_id} via /auth/accesstokenrequest")
        token_response = requests.post(
            f"{base_url}/auth/accesstokenrequest",
            json=login_data,
            headers={"Content-Type": "application/json"}
        )
        
        if token_response.status_code == 200:
            token_data = token_response.json()
            md_access_token = token_data.get('mdAccessToken') or token_data.get('md_access_token')
            access_token = token_data.get('accessToken') or token_data.get('access_token')
            refresh_token = token_data.get('refreshToken') or token_data.get('refresh_token')
            
            if md_access_token:
                # Calculate actual expiration time from Tradovate response
                expires_at = None
                if 'expirationTime' in token_data:
                    try:
                        from datetime import datetime
                        expires_at = datetime.fromisoformat(token_data['expirationTime'].replace('Z', '+00:00'))
                        expires_at = expires_at.strftime('%Y-%m-%d %H:%M:%S')
                    except Exception as e:
                        logger.warning(f"Could not parse expirationTime: {e}")
                elif 'expiresIn' in token_data:
                    try:
                        from datetime import datetime, timedelta
                        expires_in_seconds = int(token_data['expiresIn'])
                        expires_at = (datetime.now() + timedelta(seconds=expires_in_seconds)).strftime('%Y-%m-%d %H:%M:%S')
                    except Exception as e:
                        logger.warning(f"Could not parse expiresIn: {e}")
                
                # Fallback: Tradovate access tokens typically expire in 90 minutes
                if not expires_at:
                    from datetime import datetime, timedelta
                    expires_at = (datetime.now() + timedelta(minutes=85)).strftime('%Y-%m-%d %H:%M:%S')
                
                # Update tokens in database
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE accounts 
                    SET md_access_token = ?,
                        tradovate_token = COALESCE(?, tradovate_token),
                        tradovate_refresh_token = COALESCE(?, tradovate_refresh_token),
                        token_expires_at = ?
                    WHERE id = ?
                """, (md_access_token, access_token, refresh_token, expires_at, account_id))
                conn.commit()
                conn.close()
                
                logger.info(f"‚úÖ Successfully stored mdAccessToken for account {account_id}")
                return jsonify({
                    'success': True,
                    'message': 'mdAccessToken fetched and stored successfully. WebSocket will now work properly.',
                    'has_md_token': True
                })
            else:
                logger.warning(f"mdAccessToken not in response: {list(token_data.keys())}")
                return jsonify({
                    'success': False,
                    'error': 'mdAccessToken not in response from Tradovate',
                    'response_keys': list(token_data.keys())
                }), 400
        else:
            error_text = token_response.text[:200]
            logger.error(f"Failed to fetch mdAccessToken: {token_response.status_code} - {error_text}")
            return jsonify({
                'success': False,
                'error': f'Failed to fetch mdAccessToken: {token_response.status_code}',
                'details': error_text
            }), token_response.status_code
            
    except Exception as e:
        logger.error(f"Error fetching mdAccessToken: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# TradingView Session Integration (for real-time price data)
# ============================================================================

@app.route('/api/tradingview/session', methods=['POST'])
def store_tradingview_session():
    """Store TradingView session cookies for real-time price data"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400
        
        sessionid = data.get('sessionid')
        sessionid_sign = data.get('sessionid_sign')
        device_t = data.get('device_t')
        
        if not sessionid:
            return jsonify({'success': False, 'error': 'sessionid is required'}), 400
        
        # Store as JSON in the first available account
        tv_session = json.dumps({
            'sessionid': sessionid,
            'sessionid_sign': sessionid_sign,
            'device_t': device_t,
            'updated_at': datetime.now().isoformat()
        })

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()

        # Find the first account (don't assume id=1 exists)
        if is_postgres:
            cursor.execute("SELECT id FROM accounts ORDER BY id LIMIT 1")
        else:
            cursor.execute("SELECT id FROM accounts ORDER BY id LIMIT 1")

        row = cursor.fetchone()
        if row:
            account_id = row['id'] if isinstance(row, dict) else row[0]
            if is_postgres:
                cursor.execute("UPDATE accounts SET tradingview_session = %s WHERE id = %s", (tv_session, account_id))
            else:
                cursor.execute("UPDATE accounts SET tradingview_session = ? WHERE id = ?", (tv_session, account_id))
            logger.info(f"‚úÖ TradingView session stored on account {account_id}")
        else:
            conn.close()
            return jsonify({'success': False, 'error': 'No accounts found to store session'}), 400

        conn.commit()
        conn.close()
        
        logger.info("‚úÖ TradingView session stored successfully")
        
        # Restart TradingView WebSocket with new session
        start_tradingview_websocket()
        
        return jsonify({
            'success': True,
            'message': 'TradingView session stored. WebSocket will connect for real-time prices.'
        })
        
    except Exception as e:
        logger.error(f"Error storing TradingView session: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/tradingview/session', methods=['GET'])
def get_tradingview_session_status():
    """Check if TradingView session is configured"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()

        # Find any account with tradingview_session set
        if is_postgres:
            cursor.execute("SELECT tradingview_session FROM accounts WHERE tradingview_session IS NOT NULL AND tradingview_session != '' ORDER BY id LIMIT 1")
        else:
            cursor.execute("SELECT tradingview_session FROM accounts WHERE tradingview_session IS NOT NULL AND tradingview_session != '' ORDER BY id LIMIT 1")

        row = cursor.fetchone()
        conn.close()

        session_data_raw = row['tradingview_session'] if isinstance(row, dict) else (row[0] if row else None)

        if session_data_raw:
            session_data = json.loads(session_data_raw)
            return jsonify({
                'success': True,
                'configured': True,
                'updated_at': session_data.get('updated_at'),
                'has_sessionid': bool(session_data.get('sessionid'))
            })
        else:
            return jsonify({
                'success': True,
                'configured': False
            })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/strategies', methods=['GET'])
def get_strategies():
    """Get all strategies"""
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM strategies")
        strategies = cursor.fetchall()
        conn.close()
        
        strategies_list = []
        for strategy in strategies:
            strategies_list.append({
                'id': strategy['id'],
                'name': strategy['name'] if 'name' in strategy.keys() else None,
                'symbol': strategy['symbol'] if 'symbol' in strategy.keys() else None,
                'enabled': bool(strategy['enabled'] if 'enabled' in strategy.keys() else 1),
                'created_at': strategy['created_at'] if 'created_at' in strategy.keys() else None
            })
        
        return jsonify({'success': True, 'strategies': strategies_list})
    except Exception as e:
        logger.error(f"Error getting strategies: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/live-strategies', methods=['GET'])
def get_live_strategies():
    """Get all live/active strategies"""
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        # Try to get enabled strategies, fallback to all if enabled column doesn't exist
        try:
            if is_using_postgres():
                cursor.execute("SELECT * FROM strategies WHERE enabled = true")
            else:
                cursor.execute("SELECT * FROM strategies WHERE enabled = 1")
        except (sqlite3.OperationalError, Exception):
            # enabled column doesn't exist, get all strategies
            cursor.execute("SELECT * FROM strategies")
        strategies = cursor.fetchall()
        conn.close()
        
        strategies_list = []
        for strategy in strategies:
            strategies_list.append({
                'id': strategy['id'],
                'name': strategy['name'] if 'name' in strategy.keys() else None,
                'symbol': strategy['symbol'] if 'symbol' in strategy.keys() else None,
                'enabled': True,
                'created_at': strategy['created_at'] if 'created_at' in strategy.keys() else None
            })
        
        return jsonify({'success': True, 'strategies': strategies_list})
    except Exception as e:
        logger.error(f"Error getting live strategies: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/strategies')
def strategies():
    return render_template('strategies.html')


@app.route('/strategies/manage')
def strategies_manage():
    """Strategy Templates Management Page - Create/Edit/Toggle public strategies"""
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    return render_template('strategy_templates.html')


@app.route('/api/strategies/templates', methods=['GET'])
def get_strategy_templates():
    """
    Get strategy templates for the Create Trader dropdown.
    Returns: All PUBLIC strategies + current user's own strategies.
    Each strategy includes all settings for auto-population.
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get current user ID if logged in
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        # Query: public strategies OR user's own strategies
        if is_postgres:
            if current_user_id:
                cursor.execute('''
                    SELECT s.*, u.username as owner_username
                    FROM strategies s
                    LEFT JOIN users u ON s.user_id = u.id
                    WHERE s.is_public = true OR s.user_id = %s
                    ORDER BY s.is_public DESC, s.name ASC
                ''', (current_user_id,))
            else:
                cursor.execute('''
                    SELECT s.*, u.username as owner_username
                    FROM strategies s
                    LEFT JOIN users u ON s.user_id = u.id
                    WHERE s.is_public = true
                    ORDER BY s.name ASC
                ''')
        else:
            if current_user_id:
                cursor.execute('''
                    SELECT s.*, u.username as owner_username
                    FROM strategies s
                    LEFT JOIN users u ON s.user_id = u.id
                    WHERE s.is_public = 1 OR s.user_id = ?
                    ORDER BY s.is_public DESC, s.name ASC
                ''', (current_user_id,))
            else:
                cursor.execute('''
                    SELECT s.*, u.username as owner_username
                    FROM strategies s
                    LEFT JOIN users u ON s.user_id = u.id
                    WHERE s.is_public = 1
                    ORDER BY s.name ASC
                ''')
        
        rows = cursor.fetchall()
        conn.close()
        
        templates = []
        for row in rows:
            # Convert row to dict
            if hasattr(row, 'keys'):
                row_dict = dict(row)
            else:
                # Handle tuple rows
                columns = [desc[0] for desc in cursor.description] if cursor.description else []
                row_dict = dict(zip(columns, row)) if columns else {}
            
            is_own = row_dict.get('user_id') == current_user_id if current_user_id else False
            is_public = bool(row_dict.get('is_public', 0))
            
            templates.append({
                'id': row_dict.get('id'),
                'name': row_dict.get('name'),
                'symbol': row_dict.get('symbol'),
                'strat_type': row_dict.get('strat_type', 'Futures'),
                # Risk settings for auto-population
                'position_size': row_dict.get('position_size', 1),
                'position_add': row_dict.get('position_add', 1),
                'take_profit': row_dict.get('take_profit', 0),
                'stop_loss': row_dict.get('stop_loss', 0),
                'trim': row_dict.get('trim', 100),
                'tpsl_units': row_dict.get('tpsl_units', 'Ticks'),
                'max_contracts': row_dict.get('max_contracts', 0),
                'max_daily_loss': row_dict.get('max_daily_loss', 0),
                'delay_seconds': row_dict.get('delay_seconds', 0),
                'entry_delay': row_dict.get('entry_delay', 0),
                'signal_cooldown': row_dict.get('signal_cooldown', 0),
                'direction_filter': row_dict.get('direction_filter', 'ALL'),
                'time_filter_enabled': bool(row_dict.get('time_filter_enabled', 0)),
                'time_filter_start': row_dict.get('time_filter_start', ''),
                'time_filter_end': row_dict.get('time_filter_end', ''),
                'notes': row_dict.get('notes', ''),
                # Metadata
                'is_public': is_public,
                'is_own': is_own,
                'owner_username': row_dict.get('owner_username') or row_dict.get('created_by_username') or 'Unknown',
                'created_at': row_dict.get('created_at')
            })
        
        return jsonify({
            'success': True,
            'templates': templates,
            'count': len(templates)
        })
        
    except Exception as e:
        logger.error(f"Error getting strategy templates: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/strategies', methods=['POST'])
def create_strategy():
    """Create a new strategy template"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400
        
        name = data.get('name')
        if not name:
            return jsonify({'success': False, 'error': 'Strategy name is required'}), 400
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get current user
        current_user_id = None
        current_username = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
            user = get_current_user()
            current_username = user.username if user else None
        
        # Insert strategy
        if is_postgres:
            cursor.execute('''
                INSERT INTO strategies (
                    user_id, name, symbol, strat_type, position_size, position_add,
                    take_profit, stop_loss, trim, tpsl_units, max_contracts,
                    max_daily_loss, delay_seconds, entry_delay, signal_cooldown,
                    direction_filter, time_filter_enabled, time_filter_start,
                    time_filter_end, notes, is_public, created_by_username
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id
            ''', (
                current_user_id,
                name,
                data.get('symbol', ''),
                data.get('strat_type', 'Futures'),
                data.get('position_size', 1),
                data.get('position_add', 1),
                data.get('take_profit', 0),
                data.get('stop_loss', 0),
                data.get('trim', 100),
                data.get('tpsl_units', 'Ticks'),
                data.get('max_contracts', 0),
                data.get('max_daily_loss', 0),
                data.get('delay_seconds', 0),
                data.get('entry_delay', 0),
                data.get('signal_cooldown', 0),
                data.get('direction_filter', 'ALL'),
                data.get('time_filter_enabled', False),
                data.get('time_filter_start', ''),
                data.get('time_filter_end', ''),
                data.get('notes', ''),
                data.get('is_public', False),
                current_username
            ))
            strategy_id = cursor.fetchone()[0]
        else:
            cursor.execute('''
                INSERT INTO strategies (
                    user_id, name, symbol, strat_type, position_size, position_add,
                    take_profit, stop_loss, trim, tpsl_units, max_contracts,
                    max_daily_loss, delay_seconds, entry_delay, signal_cooldown,
                    direction_filter, time_filter_enabled, time_filter_start,
                    time_filter_end, notes, is_public, created_by_username
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                current_user_id,
                name,
                data.get('symbol', ''),
                data.get('strat_type', 'Futures'),
                data.get('position_size', 1),
                data.get('position_add', 1),
                data.get('take_profit', 0),
                data.get('stop_loss', 0),
                data.get('trim', 100),
                data.get('tpsl_units', 'Ticks'),
                data.get('max_contracts', 0),
                data.get('max_daily_loss', 0),
                data.get('delay_seconds', 0),
                data.get('entry_delay', 0),
                data.get('signal_cooldown', 0),
                data.get('direction_filter', 'ALL'),
                1 if data.get('time_filter_enabled') else 0,
                data.get('time_filter_start', ''),
                data.get('time_filter_end', ''),
                data.get('notes', ''),
                1 if data.get('is_public') else 0,
                current_username
            ))
            strategy_id = cursor.lastrowid
        
        conn.commit()
        conn.close()
        
        logger.info(f"Created strategy '{name}' (id={strategy_id}, public={data.get('is_public', False)})")
        
        return jsonify({
            'success': True,
            'strategy_id': strategy_id,
            'message': f"Strategy '{name}' created successfully"
        })
        
    except Exception as e:
        logger.error(f"Error creating strategy: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/strategies/<int:strategy_id>', methods=['PUT'])
def update_strategy(strategy_id):
    """Update an existing strategy template"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get current user to verify ownership
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        # Verify ownership (only owner can update)
        if is_postgres:
            cursor.execute('SELECT user_id FROM strategies WHERE id = %s', (strategy_id,))
        else:
            cursor.execute('SELECT user_id FROM strategies WHERE id = ?', (strategy_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return jsonify({'success': False, 'error': 'Strategy not found'}), 404
        
        owner_id = row[0] if isinstance(row, tuple) else row.get('user_id')
        if current_user_id and owner_id and owner_id != current_user_id:
            conn.close()
            return jsonify({'success': False, 'error': 'You can only edit your own strategies'}), 403
        
        # Build update query dynamically based on provided fields
        update_fields = []
        update_values = []
        
        field_mapping = {
            'name': 'name',
            'symbol': 'symbol',
            'strat_type': 'strat_type',
            'position_size': 'position_size',
            'position_add': 'position_add',
            'take_profit': 'take_profit',
            'stop_loss': 'stop_loss',
            'trim': 'trim',
            'tpsl_units': 'tpsl_units',
            'max_contracts': 'max_contracts',
            'max_daily_loss': 'max_daily_loss',
            'delay_seconds': 'delay_seconds',
            'entry_delay': 'entry_delay',
            'signal_cooldown': 'signal_cooldown',
            'direction_filter': 'direction_filter',
            'time_filter_enabled': 'time_filter_enabled',
            'time_filter_start': 'time_filter_start',
            'time_filter_end': 'time_filter_end',
            'notes': 'notes',
            'is_public': 'is_public'
        }
        
        for key, db_field in field_mapping.items():
            if key in data:
                value = data[key]
                # Convert boolean to int for SQLite
                if key in ['is_public', 'time_filter_enabled'] and not is_postgres:
                    value = 1 if value else 0
                update_fields.append(f"{db_field} = %s" if is_postgres else f"{db_field} = ?")
                update_values.append(value)
        
        if not update_fields:
            conn.close()
            return jsonify({'success': False, 'error': 'No fields to update'}), 400
        
        # Add updated_at
        update_fields.append("updated_at = CURRENT_TIMESTAMP")
        
        # Execute update
        update_values.append(strategy_id)
        query = f"UPDATE strategies SET {', '.join(update_fields)} WHERE id = %s" if is_postgres else f"UPDATE strategies SET {', '.join(update_fields)} WHERE id = ?"
        cursor.execute(query, tuple(update_values))
        
        conn.commit()
        conn.close()
        
        logger.info(f"Updated strategy {strategy_id}")
        return jsonify({'success': True, 'message': 'Strategy updated successfully'})
        
    except Exception as e:
        logger.error(f"Error updating strategy: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/strategies/<int:strategy_id>', methods=['DELETE'])
def delete_strategy(strategy_id):
    """Delete a strategy template"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get current user to verify ownership
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        # Verify ownership
        if is_postgres:
            cursor.execute('SELECT user_id, name FROM strategies WHERE id = %s', (strategy_id,))
        else:
            cursor.execute('SELECT user_id, name FROM strategies WHERE id = ?', (strategy_id,))
        row = cursor.fetchone()
        
        if not row:
            conn.close()
            return jsonify({'success': False, 'error': 'Strategy not found'}), 404
        
        owner_id = row[0] if isinstance(row, tuple) else row.get('user_id')
        strategy_name = row[1] if isinstance(row, tuple) else row.get('name')
        
        if current_user_id and owner_id and owner_id != current_user_id:
            conn.close()
            return jsonify({'success': False, 'error': 'You can only delete your own strategies'}), 403
        
        # Delete the strategy
        if is_postgres:
            cursor.execute('DELETE FROM strategies WHERE id = %s', (strategy_id,))
        else:
            cursor.execute('DELETE FROM strategies WHERE id = ?', (strategy_id,))
        
        conn.commit()
        conn.close()
        
        logger.info(f"Deleted strategy {strategy_id}: {strategy_name}")
        return jsonify({'success': True, 'message': f"Strategy '{strategy_name}' deleted"})
        
    except Exception as e:
        logger.error(f"Error deleting strategy: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/my-recorders', methods=['GET'])
def my_recorders_redirect():
    """Redirect /my-recorders to /recorders for backwards compatibility"""
    return redirect(url_for('recorders_list'))


@app.route('/recorders', methods=['GET'])
@feature_required('recorders')
def recorders_list():
    """Render the recorders list page - shows only user's own recorders"""
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get current user ID for filtering - only show THEIR recorders
        user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            user_id = get_current_user_id()
        
        if user_id:
            # Show only recorders created by this user
            if is_postgres:
                cursor.execute('SELECT id, name, ticker, enabled, created_at, is_premium FROM recorders WHERE user_id = %s ORDER BY id DESC', (user_id,))
            else:
                cursor.execute('SELECT id, name, ticker, enabled, created_at, is_premium FROM recorders WHERE user_id = ? ORDER BY id DESC', (user_id,))
        else:
            # No user auth - show all (shouldn't happen if login required)
            cursor.execute('SELECT id, name, ticker, enabled, created_at, is_premium FROM recorders ORDER BY id DESC')
        rows = cursor.fetchall()
        recorders = []
        for row in rows:
            try:
                if isinstance(row, dict):
                    rec = row
                elif hasattr(row, 'keys'):
                    rec = dict(row)
                else:
                    rec = {
                        'id': row[0],
                        'name': row[1],
                        'ticker': row[2],
                        'enabled': row[3],
                        'created_at': row[4],
                        'is_premium': row[5] if len(row) > 5 else False
                    }
                # Ensure template-required fields exist
                recorders.append({
                    'id': rec.get('id'),
                    'name': rec.get('name', 'Unknown'),
                    'symbol': rec.get('ticker') or rec.get('symbol', ''),
                    'is_recording': rec.get('enabled', False),
                    'enabled': rec.get('enabled', False),
                    'created_at': str(rec.get('created_at', '')) if rec.get('created_at') else '',
                    'is_premium': bool(rec.get('is_premium', False))
                })
            except Exception as row_err:
                logger.warning(f"Error processing recorder row: {row_err}")
                continue
        conn.close()

        # Check platform subscription
        has_platform_subscription = True
        user_tier = 'none'
        if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
            user = get_current_user()
            if user:
                from subscription_models import get_user_subscription, get_user_plan_tier
                platform_sub = get_user_subscription(user.id, plan_type='platform')
                has_platform_subscription = platform_sub is not None
                user_tier = get_user_plan_tier(user.id)
                if user.is_admin:
                    has_platform_subscription = True
        
        # Check if current user is admin
        is_admin = False
        if USER_AUTH_AVAILABLE:
            user = get_current_user()
            if user and user.is_admin:
                is_admin = True

        return render_template('recorders_list.html',
                              recorders=recorders,
                              has_platform_subscription=has_platform_subscription,
                              user_tier=user_tier,
                              is_admin=is_admin)
    except Exception as e:
        logger.error(f"Error loading recorders list: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return render_template('recorders_list.html',
                              recorders=[],
                              has_platform_subscription=False,
                              user_tier='none',
                              is_admin=False)

@app.route('/recorders/new')
@feature_required('recorders')
def recorders_new():
    """Render the new recorder form"""
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    try:
        # Get accounts for dropdown
        conn = get_db_connection()
        cursor = conn.cursor()
        # Use simple query without boolean filter for compatibility
        cursor.execute('SELECT id, name FROM accounts')
        rows = cursor.fetchall()
        accounts = []
        for row in rows:
            try:
                if isinstance(row, dict):
                    accounts.append(row)
                elif hasattr(row, 'keys'):
                    accounts.append(dict(row))
                else:
                    accounts.append({'id': row[0], 'name': row[1]})
            except:
                continue
        conn.close()
        return render_template('recorders.html', recorder=None, accounts=accounts, mode='create')
    except Exception as e:
        logger.error(f"Error loading new recorder form: {e}")
        return render_template('recorders.html', recorder=None, accounts=[], mode='create')

@app.route('/recorders/<int:recorder_id>')
@feature_required('recorders')
def recorders_edit(recorder_id):
    """Render the edit recorder form"""
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            cursor.execute('SELECT * FROM recorders WHERE id = %s', (recorder_id,))
        else:
            cursor.execute('SELECT * FROM recorders WHERE id = ?', (recorder_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return redirect('/recorders')

        # Ownership check ‚Äî only owner or admin can view edit page
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
            if isinstance(row, dict):
                owner_id = row.get('user_id')
            elif hasattr(row, 'keys'):
                owner_id = dict(row).get('user_id')
            else:
                owner_id = row[1]  # user_id is second column in SELECT *
            current_user = get_current_user()
            is_admin_user = current_user and current_user.is_admin
            if current_user_id and owner_id and owner_id != current_user_id and not is_admin_user:
                conn.close()
                return redirect('/recorders')

        columns = ['id', 'user_id', 'name', 'enabled', 'webhook_token', 'ticker', 'position_size',
                   'tp_enabled', 'tp_targets', 'sl_enabled', 'sl_amount', 'trailing_sl', 'account_id',
                   'created_at', 'updated_at']
        if isinstance(row, dict):
            recorder = row
        elif hasattr(row, 'keys'):
            recorder = dict(row)
        else:
            recorder = dict(zip(columns[:len(row)], row))
        
        # Parse TP targets JSON
        try:
            recorder['tp_targets'] = json.loads(recorder.get('tp_targets') or '[]')
        except:
            recorder['tp_targets'] = []
        # Get accounts for dropdown (filtered to current user's accounts)
        if USER_AUTH_AVAILABLE and is_logged_in():
            ph_acct = '%s' if is_postgres else '?'
            cursor.execute(f'SELECT id, name FROM accounts WHERE user_id = {ph_acct}', (get_current_user_id(),))
        else:
            cursor.execute('SELECT id, name FROM accounts')
        rows = cursor.fetchall()
        accounts = []
        for row in rows:
            try:
                if isinstance(row, dict):
                    accounts.append(row)
                elif hasattr(row, 'keys'):
                    accounts.append(dict(row))
                else:
                    accounts.append({'id': row[0], 'name': row[1]})
            except:
                continue
        conn.close()

        # Check if current user is admin (for tier pill)
        is_admin = False
        if USER_AUTH_AVAILABLE and is_logged_in():
            user = get_current_user()
            if user and user.is_admin:
                is_admin = True

        return render_template('recorders.html', recorder=recorder, accounts=accounts, mode='edit', is_admin=is_admin)
    except Exception as e:
        logger.error(f"Error loading recorder edit form: {e}")
        return redirect('/recorders')

# ============================================================
# RECORDER API ENDPOINTS
# ============================================================

@app.route('/api/recorders', methods=['GET'])
def api_get_recorders():
    """Get all recorders - public by default"""
    try:
        search = request.args.get('search', '').strip()
        page = int(request.args.get('page', 1))
        per_page = int(request.args.get('per_page', 20))
        offset = (page - 1) * per_page
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()

        # Filter by current user ‚Äî only show their own recorders (admins see all)
        user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user = get_current_user()
            if current_user and not current_user.is_admin:
                user_id = current_user.id

        if is_postgres:
            if user_id and search:
                cursor.execute('''
                    SELECT * FROM recorders
                    WHERE user_id = %s AND name ILIKE %s
                    ORDER BY created_at DESC
                    LIMIT %s OFFSET %s
                ''', (user_id, f'%{search}%', per_page, offset))
            elif user_id:
                cursor.execute('''
                    SELECT * FROM recorders
                    WHERE user_id = %s
                    ORDER BY created_at DESC
                    LIMIT %s OFFSET %s
                ''', (user_id, per_page, offset))
            elif search:
                cursor.execute('''
                    SELECT * FROM recorders
                    WHERE name ILIKE %s
                    ORDER BY created_at DESC
                    LIMIT %s OFFSET %s
                ''', (f'%{search}%', per_page, offset))
            else:
                cursor.execute('''
                    SELECT * FROM recorders
                    ORDER BY created_at DESC
                    LIMIT %s OFFSET %s
                ''', (per_page, offset))
        else:
            if user_id and search:
                cursor.execute('''
                    SELECT * FROM recorders
                    WHERE user_id = ? AND name LIKE ?
                    ORDER BY created_at DESC
                    LIMIT ? OFFSET ?
                ''', (user_id, f'%{search}%', per_page, offset))
            elif user_id:
                cursor.execute('''
                    SELECT * FROM recorders
                    WHERE user_id = ?
                    ORDER BY created_at DESC
                    LIMIT ? OFFSET ?
                ''', (user_id, per_page, offset))
            elif search:
                cursor.execute('''
                    SELECT * FROM recorders
                    WHERE name LIKE ?
                    ORDER BY created_at DESC
                    LIMIT ? OFFSET ?
                ''', (f'%{search}%', per_page, offset))
            else:
                cursor.execute('''
                    SELECT * FROM recorders
                    ORDER BY created_at DESC
                    LIMIT ? OFFSET ?
                ''', (per_page, offset))
        
        rows = cursor.fetchall()
        recorders = []
        columns = ['id', 'user_id', 'name', 'enabled', 'webhook_token', 'ticker', 'position_size', 
                   'tp_enabled', 'tp_targets', 'sl_enabled', 'sl_amount', 'trailing_sl', 'account_id', 
                   'created_at', 'updated_at']
        for row in rows:
            if hasattr(row, 'keys'):
                recorder = dict(row)
            else:
                recorder = dict(zip(columns[:len(row)], row))
            try:
                recorder['tp_targets'] = json.loads(recorder.get('tp_targets') or '[]')
            except:
                recorder['tp_targets'] = []
            recorders.append(recorder)
        
        # Get total count matching the same user filter
        if is_postgres:
            if user_id and search:
                cursor.execute('SELECT COUNT(*) as count FROM recorders WHERE user_id = %s AND name ILIKE %s', (user_id, f'%{search}%'))
            elif user_id:
                cursor.execute('SELECT COUNT(*) as count FROM recorders WHERE user_id = %s', (user_id,))
            elif search:
                cursor.execute('SELECT COUNT(*) as count FROM recorders WHERE name ILIKE %s', (f'%{search}%',))
            else:
                cursor.execute('SELECT COUNT(*) as count FROM recorders')
        else:
            if user_id and search:
                cursor.execute('SELECT COUNT(*) as count FROM recorders WHERE user_id = ? AND name LIKE ?', (user_id, f'%{search}%'))
            elif user_id:
                cursor.execute('SELECT COUNT(*) as count FROM recorders WHERE user_id = ?', (user_id,))
            elif search:
                cursor.execute('SELECT COUNT(*) as count FROM recorders WHERE name LIKE ?', (f'%{search}%',))
            else:
                cursor.execute('SELECT COUNT(*) as count FROM recorders')
        total_row = cursor.fetchone()
        total = total_row[0] if total_row else 0
        
        conn.close()
        
        return jsonify({
            'success': True,
            'recorders': recorders,
            'total': total,
            'page': page,
            'per_page': per_page,
            'total_pages': (total + per_page - 1) // per_page
        })
    except Exception as e:
        logger.error(f"Error getting recorders: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>', methods=['GET'])
def api_get_recorder(recorder_id):
    """Get a single recorder by ID with all settings for auto-population"""
    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        if is_postgres:
            cursor.execute('SELECT * FROM recorders WHERE id = %s', (recorder_id,))
        else:
            cursor.execute('SELECT * FROM recorders WHERE id = ?', (recorder_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404
        
        # Convert row to dict - works for both PostgreSQL RealDictRow and SQLite Row
        if hasattr(row, 'keys'):
            recorder = dict(row)
        elif hasattr(row, '_fields'):
            recorder = row._asdict()
        else:
            # Fallback for tuple rows - use column description
            columns = [desc[0] for desc in cursor.description] if cursor.description else []
            recorder = dict(zip(columns, row))
        
        # Parse JSON fields
        try:
            recorder['tp_targets'] = json.loads(recorder.get('tp_targets') or '[]')
        except:
            recorder['tp_targets'] = []
        
        # Ensure all settings fields have defaults for the form
        defaults = {
            'strategy_type': 'Futures',
            'initial_position_size': 1,
            'add_position_size': 1,
            'tp_units': 'Ticks',
            'trim_units': 'Contracts',
            'sl_enabled': False,
            'sl_amount': 0,
            'sl_units': 'Ticks',
            'sl_type': 'Fixed',
            'break_even_enabled': False,
            'break_even_ticks': 10,
            'avg_down_enabled': False,
            'signal_blocking': False,
            'avg_down_amount': 1,
            'avg_down_point': 10,
            'avg_down_units': 'Ticks',
            'max_contracts': 0,
            'time_filter_1_enabled': False,
            'time_filter_1_start': '',
            'time_filter_1_stop': '',
            'time_filter_2_enabled': False,
            'time_filter_2_start': '',
            'time_filter_2_stop': ''
        }
        
        for key, default_value in defaults.items():
            if key not in recorder or recorder[key] is None:
                recorder[key] = default_value
        
        return jsonify({'success': True, 'recorder': recorder})
    except Exception as e:
        logger.error(f"Error getting recorder {recorder_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders', methods=['POST'])
def api_create_recorder():
    """Create a new recorder with ALL form fields"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400
        
        name = data.get('name', '').strip()
        if not name:
            return jsonify({'success': False, 'error': 'Strategy name is required'}), 400
        
        # Generate webhook token
        import secrets
        webhook_token = secrets.token_urlsafe(16)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        
        # Validate risk settings before saving
        validation_errors = validate_trader_risk_settings(data)
        if validation_errors:
            return jsonify({'success': False, 'error': 'Invalid risk settings', 'validation_errors': validation_errors}), 400

        # Serialize TP targets
        tp_targets = json.dumps(data.get('tp_targets', []))

        # Get current user_id for data isolation
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        # Helper to convert boolean for DB
        def to_bool(val, default=False):
            if val is None:
                return default
            if is_postgres:
                return bool(val)
            return 1 if val else 0
        
        # Build full INSERT with ALL fields
        insert_sql = f'''
            INSERT INTO recorders (
                name, strategy_type, symbol, enabled, webhook_token, user_id, is_private,
                initial_position_size, add_position_size,
                tp_units, trim_units, tp_targets, tp_enabled,
                sl_enabled, sl_amount, sl_units, sl_type, trailing_sl,
                break_even_enabled, break_even_ticks, break_even_offset,
                trail_trigger, trail_freq,
                avg_down_enabled, signal_blocking, avg_down_amount, avg_down_point, avg_down_units,
                add_delay, max_contracts_per_trade, option_premium_filter, direction_filter,
                time_filter_1_enabled, time_filter_1_start, time_filter_1_stop,
                time_filter_2_enabled, time_filter_2_start, time_filter_2_stop,
                signal_cooldown, max_signals_per_session, max_daily_loss, auto_flat_after_cutoff,
                same_direction_ignore, notes, recording_enabled
            ) VALUES (
                {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph},
                {ph}, {ph},
                {ph}, {ph}, {ph}, {ph},
                {ph}, {ph}, {ph}, {ph}, {ph},
                {ph}, {ph}, {ph},
                {ph}, {ph},
                {ph}, {ph}, {ph}, {ph}, {ph},
                {ph}, {ph}, {ph}, {ph},
                {ph}, {ph}, {ph},
                {ph}, {ph}, {ph},
                {ph}, {ph}, {ph}, {ph},
                {ph}, {ph}, {ph}
            )'''
        
        if is_postgres:
            insert_sql += ' RETURNING id'
        
        values = [
            name,
            data.get('strategy_type', 'Futures'),
            data.get('symbol') or data.get('ticker'),
            to_bool(True),  # enabled
            webhook_token,
            current_user_id,
            to_bool(data.get('is_private', False)),
            # Positional
            data.get('initial_position_size', 0),
            data.get('add_position_size', 0),
            # TP
            data.get('tp_units', 'Ticks'),
            data.get('trim_units', 'Contracts'),
            tp_targets,
            to_bool(True),  # tp_enabled
            # SL
            to_bool(data.get('sl_enabled', False)),
            data.get('sl_amount', 0),
            data.get('sl_units', 'Ticks'),
            data.get('sl_type', 'Fixed'),
            to_bool(data.get('trailing_sl', False)),
            # Break-Even
            to_bool(data.get('break_even_enabled', False)),
            data.get('break_even_ticks', 10),
            data.get('break_even_offset', 0),
            # Trailing Stop
            data.get('trail_trigger', 0),
            data.get('trail_freq', 0),
            # Avg Down
            to_bool(data.get('avg_down_enabled', False)),
            to_bool(data.get('signal_blocking', False)),
            data.get('avg_down_amount', 0),
            data.get('avg_down_point', 0),
            data.get('avg_down_units', 'Ticks'),
            # Filters
            data.get('add_delay', 0),
            data.get('max_contracts_per_trade', 0),
            data.get('option_premium_filter', 0),
            data.get('direction_filter') or None,
            # Time Filters
            to_bool(data.get('time_filter_1_enabled', False)),
            data.get('time_filter_1_start', ''),
            data.get('time_filter_1_stop', ''),
            to_bool(data.get('time_filter_2_enabled', False)),
            data.get('time_filter_2_start', ''),
            data.get('time_filter_2_stop', ''),
            # Execution
            data.get('signal_cooldown', 0),
            data.get('max_signals_per_session', 0),
            data.get('max_daily_loss', 0),
            to_bool(data.get('auto_flat_after_cutoff', False)),
            # Misc
            to_bool(data.get('same_direction_ignore', False)),
            data.get('notes', ''),
            to_bool(data.get('recording_enabled', True)),
        ]
        
        cursor.execute(insert_sql, values)
        
        if is_postgres:
            result = cursor.fetchone()
            recorder_id = result.get('id') if isinstance(result, dict) else result[0] if result else None
        else:
            recorder_id = cursor.lastrowid
        
        conn.commit()
        conn.close()
        
        logger.info(f"Created recorder: {name} (ID: {recorder_id}) with ALL settings")
        
        return jsonify({
            'success': True,
            'message': f'Recorder "{name}" created successfully',
            'recorder_id': recorder_id,
            'webhook_token': webhook_token
        })
    except Exception as e:
        logger.error(f"Error creating recorder: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>', methods=['PUT'])
def api_update_recorder(recorder_id):
    """Update an existing recorder"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400
        
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        placeholder = '%s' if is_postgres else '?'
        
        # Check if recorder exists and get owner
        cursor.execute(f'SELECT id, user_id FROM recorders WHERE id = {placeholder}', (recorder_id,))
        existing = cursor.fetchone()
        if not existing:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404

        # Ownership check ‚Äî only owner or admin can update
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
            owner_id = existing[1] if isinstance(existing, tuple) else existing.get('user_id')
            current_user = get_current_user()
            is_admin = current_user and current_user.is_admin
            if current_user_id and owner_id and owner_id != current_user_id and not is_admin:
                conn.close()
                return jsonify({'success': False, 'error': 'You can only edit your own recorders'}), 403

        # Validate risk settings before saving
        validation_errors = validate_trader_risk_settings(data)
        if validation_errors:
            conn.close()
            return jsonify({'success': False, 'error': 'Invalid risk settings', 'validation_errors': validation_errors}), 400

        # Build update query dynamically
        fields = []
        values = []
        
        field_mapping = {
            'name': 'name',
            'strategy_type': 'strategy_type',
            'symbol': 'symbol',
            'demo_account_id': 'demo_account_id',
            'account_id': 'account_id',
            'initial_position_size': 'initial_position_size',
            'add_position_size': 'add_position_size',
            'tp_units': 'tp_units',
            'trim_units': 'trim_units',
            'sl_amount': 'sl_amount',
            'sl_units': 'sl_units',
            'sl_type': 'sl_type',
            'break_even_ticks': 'break_even_ticks',  # FIXED: Was missing!
            'avg_down_amount': 'avg_down_amount',
            'avg_down_point': 'avg_down_point',
            'avg_down_units': 'avg_down_units',
            'add_delay': 'add_delay',
            'max_contracts_per_trade': 'max_contracts_per_trade',
            'option_premium_filter': 'option_premium_filter',
            'direction_filter': 'direction_filter',
            'time_filter_1_start': 'time_filter_1_start',
            'time_filter_1_stop': 'time_filter_1_stop',
            'time_filter_2_start': 'time_filter_2_start',
            'time_filter_2_stop': 'time_filter_2_stop',
            'signal_cooldown': 'signal_cooldown',
            'max_signals_per_session': 'max_signals_per_session',
            'max_daily_loss': 'max_daily_loss',
            'notes': 'notes',
            'same_direction_ignore': 'same_direction_ignore',
            'inverse_signals': 'inverse_signals',
            'required_tier': 'required_tier',
        }
        
        for key, db_field in field_mapping.items():
            if key in data:
                fields.append(f'{db_field} = {placeholder}')
                values.append(data[key])
        
        # Handle boolean fields - PostgreSQL needs True/False, SQLite needs 1/0
        boolean_fields = [
            'sl_enabled',
            'break_even_enabled',  # FIXED: Was missing!
            'avg_down_enabled',
            'signal_blocking',
            'auto_flat_after_cutoff',
            'recording_enabled',
            'is_private',
            'time_filter_1_enabled',  # FIXED: Was missing!
            'time_filter_2_enabled',  # FIXED: Was missing!
        ]
        for bf in boolean_fields:
            if bf in data:
                fields.append(f'{bf} = {placeholder}')
                values.append(bool(data[bf]) if is_postgres else (1 if data[bf] else 0))
        
        # Handle TP targets JSON
        if 'tp_targets' in data:
            fields.append(f'tp_targets = {placeholder}')
            values.append(json.dumps(data['tp_targets']))
        
        # Always update updated_at
        timestamp_fn = 'NOW()' if is_postgres else 'CURRENT_TIMESTAMP'
        fields.append(f'updated_at = {timestamp_fn}')
        
        if fields:
            values.append(recorder_id)
            cursor.execute(f'''
                UPDATE recorders SET {', '.join(fields)} WHERE id = {placeholder}
            ''', values)
            conn.commit()

        # CRITICAL: Clear recorder cache so webhooks use updated settings immediately
        cursor.execute(f'SELECT webhook_token FROM recorders WHERE id = {placeholder}', (recorder_id,))
        token_row = cursor.fetchone()
        if token_row:
            webhook_token = token_row[0] if isinstance(token_row, tuple) else token_row.get('webhook_token')
            if webhook_token and webhook_token in recorder_cache:
                del recorder_cache[webhook_token]
                if webhook_token in recorder_cache_time:
                    del recorder_cache_time[webhook_token]
                logger.info(f"üßπ Cleared cache for recorder {recorder_id} after update")

        conn.close()

        logger.info(f"Updated recorder ID: {recorder_id}")
        
        return jsonify({
            'success': True,
            'message': 'Recorder updated successfully'
        })
    except Exception as e:
        logger.error(f"Error updating recorder {recorder_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>', methods=['DELETE'])
def api_delete_recorder(recorder_id):
    """Delete a recorder and ALL associated data (trades, signals, positions)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Check if recorder exists
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f'SELECT name, user_id FROM recorders WHERE id = {ph}', (recorder_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404

        # Ownership check ‚Äî only owner or admin can delete
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
            owner_id = row[1] if isinstance(row, tuple) else row.get('user_id')
            current_user = get_current_user()
            is_admin = current_user and current_user.is_admin
            if current_user_id and owner_id and owner_id != current_user_id and not is_admin:
                conn.close()
                return jsonify({'success': False, 'error': 'You can only delete your own recorders'}), 403

        name = row[0] if isinstance(row, tuple) else row.get('name')

        # CASCADE DELETE: Delete all associated data FIRST
        cursor.execute(f'DELETE FROM recorded_trades WHERE recorder_id = {ph}', (recorder_id,))
        trades_deleted = cursor.rowcount

        cursor.execute(f'DELETE FROM recorded_signals WHERE recorder_id = {ph}', (recorder_id,))
        signals_deleted = cursor.rowcount

        cursor.execute(f'DELETE FROM recorder_positions WHERE recorder_id = {ph}', (recorder_id,))
        positions_deleted = cursor.rowcount

        # Now delete the recorder itself
        cursor.execute(f'DELETE FROM recorders WHERE id = {ph}', (recorder_id,))
        conn.commit()
        conn.close()
        
        logger.info(f"Deleted recorder: {name} (ID: {recorder_id}) - Cascade deleted {trades_deleted} trades, {signals_deleted} signals, {positions_deleted} positions")
        
        return jsonify({
            'success': True,
            'message': f'Recorder "{name}" deleted successfully (including {trades_deleted} trades, {signals_deleted} signals, {positions_deleted} positions)'
        })
    except Exception as e:
        logger.error(f"Error deleting recorder {recorder_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>/clone', methods=['POST'])
def api_clone_recorder(recorder_id):
    """Clone an existing recorder"""
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'

        cursor.execute(f'SELECT * FROM recorders WHERE id = {ph}', (recorder_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404

        # Ownership check ‚Äî only owner or admin can clone
        clone_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
            clone_user_id = current_user_id
            original_temp = dict(row)
            owner_id = original_temp.get('user_id')
            current_user = get_current_user()
            is_admin = current_user and current_user.is_admin
            if current_user_id and owner_id and owner_id != current_user_id and not is_admin:
                conn.close()
                return jsonify({'success': False, 'error': 'You can only clone your own recorders'}), 403

        original = dict(row)
        
        # Generate new webhook token
        import secrets
        webhook_token = secrets.token_urlsafe(16)
        
        # Create cloned recorder with modified name
        new_name = f"{original['name']} (Copy)"
        
        _vals = ', '.join([ph] * 34)
        cursor.execute(f'''
            INSERT INTO recorders (
                name, strategy_type, symbol, demo_account_id, account_id,
                initial_position_size, add_position_size,
                tp_units, trim_units, tp_targets,
                sl_enabled, sl_amount, sl_units, sl_type,
                avg_down_enabled, avg_down_amount, avg_down_point, avg_down_units,
                add_delay, max_contracts_per_trade, option_premium_filter, direction_filter,
                time_filter_1_start, time_filter_1_stop, time_filter_2_start, time_filter_2_stop,
                signal_cooldown, max_signals_per_session, max_daily_loss, auto_flat_after_cutoff,
                notes, recording_enabled, webhook_token, user_id
            ) VALUES ({_vals})
        ''', (
            new_name,
            original['strategy_type'],
            original['symbol'],
            original['demo_account_id'],
            original['account_id'],
            original['initial_position_size'],
            original['add_position_size'],
            original['tp_units'],
            original['trim_units'],
            original['tp_targets'],
            original['sl_enabled'],
            original['sl_amount'],
            original['sl_units'],
            original['sl_type'],
            original['avg_down_enabled'],
            original['avg_down_amount'],
            original['avg_down_point'],
            original['avg_down_units'],
            original['add_delay'],
            original['max_contracts_per_trade'],
            original['option_premium_filter'],
            original['direction_filter'],
            original['time_filter_1_start'],
            original['time_filter_1_stop'],
            original['time_filter_2_start'],
            original['time_filter_2_stop'],
            original['signal_cooldown'],
            original['max_signals_per_session'],
            original['max_daily_loss'],
            original['auto_flat_after_cutoff'],
            original['notes'],
            original['recording_enabled'],
            webhook_token,
            clone_user_id
        ))
        
        new_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        logger.info(f"Cloned recorder {recorder_id} -> {new_id}: {new_name}")
        
        return jsonify({
            'success': True,
            'message': f'Recorder cloned successfully',
            'recorder_id': new_id,
            'name': new_name
        })
    except Exception as e:
        logger.error(f"Error cloning recorder {recorder_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>/start', methods=['POST'])
def api_start_recorder(recorder_id):
    """Start recording for a recorder"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'

        cursor.execute(f'SELECT name, is_recording FROM recorders WHERE id = {ph}', (recorder_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404

        name, is_recording = row
        if is_recording:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder is already running'}), 400

        cursor.execute(f'UPDATE recorders SET is_recording = 1, updated_at = CURRENT_TIMESTAMP WHERE id = {ph}', (recorder_id,))
        conn.commit()
        conn.close()
        
        logger.info(f"Started recording: {name} (ID: {recorder_id})")
        
        return jsonify({
            'success': True,
            'message': f'Recording started for "{name}"'
        })
    except Exception as e:
        logger.error(f"Error starting recorder {recorder_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>/stop', methods=['POST'])
def api_stop_recorder(recorder_id):
    """Stop recording for a recorder"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'

        cursor.execute(f'SELECT name, is_recording FROM recorders WHERE id = {ph}', (recorder_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404

        name, is_recording = row
        if not is_recording:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder is not running'}), 400

        cursor.execute(f'UPDATE recorders SET is_recording = 0, updated_at = CURRENT_TIMESTAMP WHERE id = {ph}', (recorder_id,))
        conn.commit()
        conn.close()
        
        logger.info(f"Stopped recording: {name} (ID: {recorder_id})")
        
        return jsonify({
            'success': True,
            'message': f'Recording stopped for "{name}"'
        })
    except Exception as e:
        logger.error(f"Error stopping recorder {recorder_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>/reset-history', methods=['POST'])
def api_reset_recorder_history(recorder_id):
    """Reset trade history for a recorder (delete all trades and signals)"""
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Verify recorder exists
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f'SELECT name FROM recorders WHERE id = {ph}', (recorder_id,))
        row = cursor.fetchone()
        if not row:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404

        name = row['name']

        # Delete all trades for this recorder
        cursor.execute(f'DELETE FROM recorded_trades WHERE recorder_id = {ph}', (recorder_id,))
        trades_deleted = cursor.rowcount

        # Delete all signals for this recorder
        cursor.execute(f'DELETE FROM recorded_signals WHERE recorder_id = {ph}', (recorder_id,))
        signals_deleted = cursor.rowcount

        # Delete all positions for this recorder (Trade Manager style tracking)
        cursor.execute(f'DELETE FROM recorder_positions WHERE recorder_id = {ph}', (recorder_id,))
        positions_deleted = cursor.rowcount
        
        conn.commit()
        conn.close()
        
        logger.info(f"Reset history for recorder '{name}' (ID: {recorder_id}): {trades_deleted} trades, {signals_deleted} signals, {positions_deleted} positions deleted")
        
        return jsonify({
            'success': True,
            'message': f'Trade history reset for "{name}". Deleted {trades_deleted} trades, {signals_deleted} signals, and {positions_deleted} positions.',
            'trades_deleted': trades_deleted,
            'signals_deleted': signals_deleted,
            'positions_deleted': positions_deleted
        })
    except Exception as e:
        logger.error(f"Error resetting history for recorder {recorder_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>/webhook', methods=['GET'])
def api_get_recorder_webhook(recorder_id):
    """Get webhook details for a recorder"""
    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        if is_postgres:
            cursor = conn.cursor()
        else:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
        
        cursor.execute(f'SELECT name, webhook_token FROM recorders WHERE id = {placeholder}', (recorder_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404
        
        # Handle both SQLite Row and PostgreSQL tuple
        if is_postgres:
            name = row[0]
            webhook_token = row[1]
        else:
            name = row['name']
            webhook_token = row['webhook_token']
        
        # Build webhook URL - use custom domain if configured, fall back to request host
        custom_domain = os.environ.get('WEBHOOK_BASE_URL', '').rstrip('/')
        if custom_domain:
            base_url = custom_domain
        else:
            base_url = request.host_url.rstrip('/')
            # Force HTTPS (Railway uses edge proxy that may report HTTP internally)
            if base_url.startswith('http://'):
                base_url = base_url.replace('http://', 'https://', 1)
        webhook_url = f"{base_url}/webhook/{webhook_token}"
        
        # Simple indicator alert message (user specifies buy or sell)
        indicator_buy_message = json.dumps({
            "recorder": name,
            "action": "buy",
            "ticker": "{{ticker}}",
            "price": "{{close}}"
        }, indent=2)
        
        indicator_sell_message = json.dumps({
            "recorder": name,
            "action": "sell",
            "ticker": "{{ticker}}",
            "price": "{{close}}"
        }, indent=2)
        
        # Pine Script strategy alert message (action auto-filled by TradingView)
        strategy_message = json.dumps({
            "recorder": name,
            "action": "{{strategy.order.action}}",
            "ticker": "{{ticker}}",
            "price": "{{close}}",
            "contracts": "{{strategy.order.contracts}}",
            "position_size": "{{strategy.position_size}}",
            "market_position": "{{strategy.market_position}}"
        }, indent=2)
        
        return jsonify({
            'success': True,
            'webhook_url': webhook_url,
            'name': name,
            'alerts': {
                'indicator_buy': indicator_buy_message,
                'indicator_sell': indicator_sell_message,
                'strategy_message': strategy_message
            }
        })
    except Exception as e:
        logger.error(f"Error getting webhook for recorder {recorder_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/recorders/<int:recorder_id>/inverse', methods=['POST'])
def api_set_inverse_signals(recorder_id):
    """Enable/disable inverse signals for a recorder"""
    try:
        data = request.get_json() or {}
        inverse_raw = data.get('inverse_signals', 0)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        
        # Get webhook_token so we can clear the cache
        cursor.execute(f'SELECT webhook_token FROM recorders WHERE id = {ph}', (recorder_id,))
        row = cursor.fetchone()
        webhook_token = None
        if row:
            webhook_token = row[0] if isinstance(row, tuple) else row.get('webhook_token')
        
        # PostgreSQL needs boolean, SQLite needs integer
        if is_postgres:
            inverse_enabled = bool(inverse_raw)
        else:
            inverse_enabled = 1 if inverse_raw else 0
        
        cursor.execute(f'''
            UPDATE recorders SET inverse_signals = {ph}, updated_at = CURRENT_TIMESTAMP
            WHERE id = {ph}
        ''', (inverse_enabled, recorder_id))
        
        conn.commit()
        conn.close()
        
        # CRITICAL: Clear the recorder cache so next webhook uses updated value
        if webhook_token and webhook_token in recorder_cache:
            del recorder_cache[webhook_token]
            if webhook_token in recorder_cache_time:
                del recorder_cache_time[webhook_token]
            logger.info(f"üßπ Cleared cache for recorder {recorder_id} (token: {webhook_token[:8]}...)")
        
        logger.info(f"üîÑ Recorder {recorder_id} inverse_signals set to {inverse_enabled}")
        return jsonify({'success': True, 'inverse_signals': inverse_enabled})
    except Exception as e:
        logger.error(f"Error setting inverse signals for recorder {recorder_id}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

# ============================================================
# TRADERS API ENDPOINTS (links recorders to accounts)
# ============================================================

@app.route('/api/traders', methods=['GET'])
def api_get_traders():
    """Get all traders (recorder-account links) with joined data and risk settings"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Filter by user_id if logged in - ONLY show current user's traders
        if USER_AUTH_AVAILABLE and is_logged_in():
            user_id = get_current_user_id()
            if is_postgres:
                cursor.execute('''
                    SELECT
                        t.id,
                        t.recorder_id,
                        t.account_id,
                        t.subaccount_id,
                        t.subaccount_name,
                        t.is_demo,
                        t.enabled,
                        t.created_at,
                        t.user_id,
                        t.initial_position_size as trader_position_size,
                        t.add_position_size as trader_add_position_size,
                        t.tp_targets as trader_tp_targets,
                        t.sl_enabled as trader_sl_enabled,
                        t.sl_amount as trader_sl_amount,
                        t.sl_units as trader_sl_units,
                        t.max_daily_loss as trader_max_daily_loss,
                        t.add_delay,
                        t.signal_count,
                        r.name as recorder_name,
                        r.ticker as symbol,
                        r.initial_position_size as recorder_position_size,
                        a.name as account_name,
                        a.broker
                    FROM traders t
                    LEFT JOIN recorders r ON t.recorder_id = r.id
                    LEFT JOIN accounts a ON t.account_id = a.id
                    WHERE t.user_id = %s OR a.user_id = %s
                    ORDER BY t.created_at DESC
                ''', (user_id, user_id))
            else:
                cursor.execute('''
                    SELECT
                        t.id,
                        t.recorder_id,
                        t.account_id,
                        t.subaccount_id,
                        t.subaccount_name,
                        t.is_demo,
                        t.enabled,
                        t.created_at,
                        t.user_id,
                        t.initial_position_size as trader_position_size,
                        t.add_position_size as trader_add_position_size,
                        t.tp_targets as trader_tp_targets,
                        t.sl_enabled as trader_sl_enabled,
                        t.sl_amount as trader_sl_amount,
                        t.sl_units as trader_sl_units,
                        t.max_daily_loss as trader_max_daily_loss,
                        t.add_delay,
                        t.signal_count,
                        r.name as recorder_name,
                        r.strategy_type,
                        r.initial_position_size as recorder_position_size,
                        r.symbol,
                        a.name as account_name,
                        a.broker
                    FROM traders t
                    JOIN recorders r ON t.recorder_id = r.id
                    JOIN accounts a ON t.account_id = a.id
                    WHERE t.user_id = ? OR a.user_id = ?
                    ORDER BY t.created_at DESC
                ''', (user_id, user_id))
        else:
            # If auth is available but user isn't logged in, reject ‚Äî don't leak all traders
            if USER_AUTH_AVAILABLE:
                conn.close()
                return jsonify({'success': False, 'error': 'Not authenticated'}), 401
            if is_postgres:
                cursor.execute('''
                    SELECT
                        t.id,
                        t.recorder_id,
                        t.account_id,
                        t.subaccount_id,
                        t.subaccount_name,
                        t.is_demo,
                        t.enabled,
                        t.created_at,
                        t.initial_position_size as trader_position_size,
                        t.add_position_size as trader_add_position_size,
                        t.tp_targets as trader_tp_targets,
                        t.sl_enabled as trader_sl_enabled,
                        t.sl_amount as trader_sl_amount,
                        t.sl_units as trader_sl_units,
                        t.max_daily_loss as trader_max_daily_loss,
                        t.add_delay,
                        t.signal_count,
                        r.name as recorder_name,
                        r.ticker as symbol,
                        r.initial_position_size as recorder_position_size,
                        a.name as account_name,
                        a.broker
                    FROM traders t
                    LEFT JOIN recorders r ON t.recorder_id = r.id
                    LEFT JOIN accounts a ON t.account_id = a.id
                    ORDER BY t.created_at DESC
                ''')
            else:
                cursor.execute('''
                    SELECT
                        t.id,
                        t.recorder_id,
                        t.account_id,
                        t.subaccount_id,
                        t.subaccount_name,
                        t.is_demo,
                        t.enabled,
                        t.created_at,
                        t.initial_position_size as trader_position_size,
                        t.add_position_size as trader_add_position_size,
                        t.tp_targets as trader_tp_targets,
                        t.sl_enabled as trader_sl_enabled,
                        t.sl_amount as trader_sl_amount,
                        t.sl_units as trader_sl_units,
                        t.max_daily_loss as trader_max_daily_loss,
                        t.add_delay,
                        t.signal_count,
                        r.name as recorder_name,
                        r.strategy_type,
                        r.initial_position_size as recorder_position_size,
                        r.symbol,
                        a.name as account_name,
                        a.broker
                    FROM traders t
                    JOIN recorders r ON t.recorder_id = r.id
                    JOIN accounts a ON t.account_id = a.id
                    ORDER BY t.created_at DESC
                ''')

        rows = cursor.fetchall()
        traders = []
        
        # PostgreSQL columns for the simplified query
        pg_columns = ['id', 'recorder_id', 'account_id', 'subaccount_id', 'subaccount_name',
                      'is_demo', 'enabled', 'created_at', 'trader_position_size',
                      'add_delay', 'signal_count',
                      'recorder_name', 'symbol', 'account_name', 'broker']
        
        for row in rows:
            # Convert to dict
            if hasattr(row, 'keys'):
                row_dict = dict(row)
            else:
                row_dict = dict(zip(pg_columns[:len(row)], row))
            
            is_demo = bool(row_dict.get('is_demo')) if row_dict.get('is_demo') is not None else None
            env_label = "DEMO" if is_demo else "LIVE" if is_demo is not None else ""
            account_name = row_dict.get('account_name') or 'Unknown'
            subaccount_name = row_dict.get('subaccount_name')
            display_account = f"{account_name} {env_label} ({subaccount_name})" if subaccount_name else account_name
            
            position_size = row_dict.get('trader_position_size') or row_dict.get('recorder_position_size') or 1
            
            traders.append({
                'id': row_dict.get('id'),
                'recorder_id': row_dict.get('recorder_id'),
                'account_id': row_dict.get('account_id'),
                'subaccount_id': row_dict.get('subaccount_id'),
                'subaccount_name': subaccount_name,
                'is_demo': is_demo,
                'enabled': bool(row_dict.get('enabled')),
                'created_at': str(row_dict.get('created_at', '')),
                'recorder_name': row_dict.get('recorder_name'),
                'name': row_dict.get('recorder_name'),
                'strategy_type': row_dict.get('strategy_type', 'Futures'),
                'symbol': row_dict.get('symbol'),
                'account_name': account_name,
                'display_account': display_account,
                'broker': row_dict.get('broker'),
                'initial_position_size': position_size,
                'position_size': position_size,  # For backward compatibility
                'add_position_size': row_dict.get('trader_add_position_size'),
                'tp_targets': [],
                'sl_enabled': bool(row_dict.get('trader_sl_enabled')) if row_dict.get('trader_sl_enabled') is not None else False,
                'sl_amount': row_dict.get('trader_sl_amount'),
                'sl_units': row_dict.get('trader_sl_units'),
                'max_daily_loss': row_dict.get('trader_max_daily_loss'),
                'add_delay': row_dict.get('add_delay', 1),
                'signal_count': row_dict.get('signal_count', 0)
            })
        
        conn.close()
        return jsonify({'success': True, 'traders': traders})
        
    except Exception as e:
        logger.error(f"Error getting traders: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

def validate_trader_risk_settings(data, context='save'):
    """Validate risk settings for trader create/update. Returns list of error strings (empty = valid).
    Warnings are returned as {'warnings': [...]} when settings are risky but technically allowed."""
    errors = []
    warnings = []

    # --- TP Targets ---
    tp_targets = data.get('tp_targets')
    if tp_targets is not None:
        if isinstance(tp_targets, str):
            try:
                tp_targets = json.loads(tp_targets)
            except (json.JSONDecodeError, TypeError):
                errors.append('tp_targets must be a valid JSON array')
                tp_targets = None
        if tp_targets is not None:
            if not isinstance(tp_targets, list):
                errors.append('tp_targets must be a list')
            elif len(tp_targets) > 0:
                for i, target in enumerate(tp_targets):
                    if not isinstance(target, dict):
                        errors.append(f'tp_targets[{i}] must be an object with ticks and trim')
                        continue
                    ticks = target.get('ticks', 0)
                    trim = target.get('trim', 0)
                    try:
                        ticks = float(ticks or 0)
                    except (ValueError, TypeError):
                        errors.append(f'tp_targets[{i}].ticks must be a number')
                        continue
                    if ticks < 0:
                        errors.append(f'tp_targets[{i}].ticks cannot be negative (got {ticks})')
                    try:
                        trim = float(trim or 0)
                    except (ValueError, TypeError):
                        errors.append(f'tp_targets[{i}].trim must be a number')
                        continue
                    if trim < 0 or trim > 100:
                        errors.append(f'tp_targets[{i}].trim must be between 0 and 100 (got {trim})')

    # --- Position Sizes (0 = use default/none, so allow 0+) ---
    for field in ['initial_position_size', 'add_position_size']:
        val = data.get(field)
        if val is not None:
            try:
                val = int(val)
                if val < 0:
                    errors.append(f'{field} cannot be negative (got {val})')
            except (ValueError, TypeError):
                errors.append(f'{field} must be a non-negative integer')

    # --- SL Settings ---
    sl_enabled = data.get('sl_enabled')
    sl_amount = data.get('sl_amount')
    sl_type = data.get('sl_type')

    if sl_amount is not None:
        try:
            sl_amount = float(sl_amount)
            if sl_amount < 0:
                errors.append(f'sl_amount cannot be negative (got {sl_amount})')
        except (ValueError, TypeError):
            errors.append('sl_amount must be a number')

    if sl_type is not None and sl_type not in ('Fixed', 'Trail', 'Trailing'):
        errors.append(f"sl_type must be 'Fixed' or 'Trail' (got '{sl_type}')")

    # --- Trailing Stop Consistency ---
    trail_trigger = data.get('trail_trigger')
    trail_freq = data.get('trail_freq')
    if trail_trigger is not None:
        try:
            trail_trigger = int(trail_trigger)
            if trail_trigger < 0:
                errors.append(f'trail_trigger cannot be negative (got {trail_trigger})')
        except (ValueError, TypeError):
            errors.append('trail_trigger must be a non-negative integer')
    if trail_freq is not None:
        try:
            trail_freq = int(trail_freq)
            if trail_freq < 0:
                errors.append(f'trail_freq cannot be negative (got {trail_freq})')
        except (ValueError, TypeError):
            errors.append('trail_freq must be a non-negative integer')

    # --- Break-Even Settings ---
    be_enabled = data.get('break_even_enabled')
    be_ticks = data.get('break_even_ticks')
    if be_ticks is not None:
        try:
            be_ticks = int(be_ticks)
            if be_ticks < 0:
                errors.append(f'break_even_ticks cannot be negative (got {be_ticks})')
        except (ValueError, TypeError):
            errors.append('break_even_ticks must be a non-negative integer')

    # --- Max Daily Loss ---
    max_loss = data.get('max_daily_loss')
    if max_loss is not None:
        try:
            max_loss = float(max_loss)
            if max_loss < 0:
                errors.append(f'max_daily_loss cannot be negative (got {max_loss})')
        except (ValueError, TypeError):
            errors.append('max_daily_loss must be a non-negative number')

    return errors


@app.route('/api/traders', methods=['POST'])
def api_create_trader():
    """Create a new trader (link recorder to account with subaccount info and risk settings)"""
    try:
        data = request.get_json()
        recorder_id = data.get('recorder_id')
        account_id = data.get('account_id')
        subaccount_id = data.get('subaccount_id')  # Tradovate subaccount ID (e.g., 26029294)
        subaccount_name = data.get('subaccount_name')  # e.g., "DEMO4419847-2" or "1381296"
        is_demo = data.get('is_demo')  # True for demo, False for live
        
        # Risk settings (optional - will use recorder defaults if not provided)
        initial_position_size = data.get('initial_position_size')
        add_position_size = data.get('add_position_size')
        tp_targets = data.get('tp_targets')  # JSON string or list
        sl_enabled = data.get('sl_enabled')
        sl_amount = data.get('sl_amount')
        sl_units = data.get('sl_units')
        max_daily_loss = data.get('max_daily_loss')
        time_filter_1_enabled = data.get('time_filter_1_enabled', False)
        time_filter_1_start = data.get('time_filter_1_start', '')
        time_filter_1_stop = data.get('time_filter_1_stop', '')
        time_filter_2_enabled = data.get('time_filter_2_enabled', False)
        time_filter_2_start = data.get('time_filter_2_start', '')
        time_filter_2_stop = data.get('time_filter_2_stop', '')
        inverse_signals = data.get('inverse_signals', 0)

        # Additional risk settings (inherit from recorder if not provided)
        sl_type = data.get('sl_type')
        tp_units = data.get('tp_units')
        trim_units = data.get('trim_units')
        break_even_enabled = data.get('break_even_enabled')
        break_even_ticks = data.get('break_even_ticks')
        break_even_offset = data.get('break_even_offset')
        avg_down_enabled = data.get('avg_down_enabled')
        avg_down_amount = data.get('avg_down_amount')
        avg_down_point = data.get('avg_down_point')
        avg_down_units = data.get('avg_down_units')
        trail_trigger = data.get('trail_trigger')
        trail_freq = data.get('trail_freq')
        add_delay = data.get('add_delay')
        dca_enabled = data.get('dca_enabled')

        if not recorder_id or not account_id:
            return jsonify({'success': False, 'error': 'recorder_id and account_id are required'}), 400
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Verify recorder exists AND get its settings to inherit
        if is_postgres:
            cursor.execute('''
                SELECT id, name, initial_position_size, add_position_size, tp_targets,
                       sl_enabled, sl_amount, sl_units, max_daily_loss,
                       sl_type, tp_units, trim_units,
                       break_even_enabled, break_even_ticks, break_even_offset,
                       avg_down_enabled, avg_down_amount, avg_down_point, avg_down_units,
                       trail_trigger, trail_freq, add_delay
                FROM recorders WHERE id = %s
            ''', (recorder_id,))
        else:
            cursor.execute('''
                SELECT id, name, initial_position_size, add_position_size, tp_targets,
                       sl_enabled, sl_amount, sl_units, max_daily_loss,
                       sl_type, tp_units, trim_units,
                       break_even_enabled, break_even_ticks, break_even_offset,
                       avg_down_enabled, avg_down_amount, avg_down_point, avg_down_units,
                       trail_trigger, trail_freq, add_delay
                FROM recorders WHERE id = ?
            ''', (recorder_id,))
        recorder = cursor.fetchone()
        if not recorder:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404
        
        # Extract recorder settings (handle both dict and tuple results)
        if isinstance(recorder, dict):
            rec_initial = recorder.get('initial_position_size', 1)
            rec_add = recorder.get('add_position_size', 1)
            rec_tp = recorder.get('tp_targets', '[]')
            rec_sl_enabled = recorder.get('sl_enabled', False)
            rec_sl_amount = recorder.get('sl_amount', 0)
            rec_sl_units = recorder.get('sl_units', 'Ticks')
            rec_max_loss = recorder.get('max_daily_loss', 500)
            rec_sl_type = recorder.get('sl_type', 'Fixed')
            rec_tp_units = recorder.get('tp_units', 'Ticks')
            rec_trim_units = recorder.get('trim_units', 'Contracts')
            rec_break_even_enabled = recorder.get('break_even_enabled', False)
            rec_break_even_ticks = recorder.get('break_even_ticks', 10)
            rec_break_even_offset = recorder.get('break_even_offset', 0)
            rec_avg_down_enabled = recorder.get('avg_down_enabled', False)
            rec_avg_down_amount = recorder.get('avg_down_amount', 0)
            rec_avg_down_point = recorder.get('avg_down_point', 0)
            rec_avg_down_units = recorder.get('avg_down_units', 'Ticks')
            rec_trail_trigger = recorder.get('trail_trigger', 0)
            rec_trail_freq = recorder.get('trail_freq', 0)
            rec_add_delay = recorder.get('add_delay', 1)
        else:
            rec_initial = recorder[2] if recorder[2] is not None else 1
            rec_add = recorder[3] if recorder[3] is not None else 1
            rec_tp = recorder[4] if recorder[4] is not None else '[]'
            rec_sl_enabled = recorder[5] if recorder[5] is not None else False
            rec_sl_amount = recorder[6] if recorder[6] is not None else 0
            rec_sl_units = recorder[7] if recorder[7] is not None else 'Ticks'
            rec_max_loss = recorder[8] if recorder[8] is not None else 500
            rec_sl_type = recorder[9] if recorder[9] is not None else 'Fixed'
            rec_tp_units = recorder[10] if recorder[10] is not None else 'Ticks'
            rec_trim_units = recorder[11] if recorder[11] is not None else 'Contracts'
            rec_break_even_enabled = recorder[12] if recorder[12] is not None else False
            rec_break_even_ticks = recorder[13] if recorder[13] is not None else 10
            rec_break_even_offset = recorder[14] if recorder[14] is not None else 0
            rec_avg_down_enabled = recorder[15] if recorder[15] is not None else False
            rec_avg_down_amount = recorder[16] if recorder[16] is not None else 0
            rec_avg_down_point = recorder[17] if recorder[17] is not None else 0
            rec_avg_down_units = recorder[18] if recorder[18] is not None else 'Ticks'
            rec_trail_trigger = recorder[19] if recorder[19] is not None else 0
            rec_trail_freq = recorder[20] if recorder[20] is not None else 0
            rec_add_delay = recorder[21] if recorder[21] is not None else 1

        # INHERIT from recorder if user didn't provide custom values
        if initial_position_size is None:
            initial_position_size = rec_initial
        if add_position_size is None:
            add_position_size = rec_add
        if tp_targets is None:
            tp_targets = rec_tp
        if sl_enabled is None:
            sl_enabled = rec_sl_enabled
        if sl_amount is None:
            sl_amount = rec_sl_amount
        if sl_units is None:
            sl_units = rec_sl_units
        if max_daily_loss is None:
            max_daily_loss = rec_max_loss
        if sl_type is None:
            sl_type = rec_sl_type
        if tp_units is None:
            tp_units = rec_tp_units
        if trim_units is None:
            trim_units = rec_trim_units
        if break_even_enabled is None:
            break_even_enabled = rec_break_even_enabled
        if break_even_ticks is None:
            break_even_ticks = rec_break_even_ticks
        if break_even_offset is None:
            break_even_offset = rec_break_even_offset
        if avg_down_enabled is None:
            avg_down_enabled = rec_avg_down_enabled
        if avg_down_amount is None:
            avg_down_amount = rec_avg_down_amount
        if avg_down_point is None:
            avg_down_point = rec_avg_down_point
        if avg_down_units is None:
            avg_down_units = rec_avg_down_units
        if trail_trigger is None:
            trail_trigger = rec_trail_trigger
        if trail_freq is None:
            trail_freq = rec_trail_freq
        if add_delay is None:
            add_delay = rec_add_delay
        if dca_enabled is None:
            # Frontend sends 'avg_down_enabled' ‚Äî use that, then fall back to recorder
            dca_enabled = bool(avg_down_enabled) if avg_down_enabled is not None else bool(rec_avg_down_enabled)
        
        # Validate risk settings AFTER inheritance (catch bad recorder defaults too)
        validation_data = {
            'initial_position_size': initial_position_size,
            'add_position_size': add_position_size,
            'tp_targets': tp_targets,
            'sl_enabled': sl_enabled,
            'sl_amount': sl_amount,
            'sl_units': sl_units,
            'max_daily_loss': max_daily_loss,
        }
        validation_errors = validate_trader_risk_settings(validation_data)
        if validation_errors:
            conn.close()
            return jsonify({'success': False, 'error': 'Invalid risk settings', 'validation_errors': validation_errors}), 400

        # Verify account exists AND user owns it
        if is_postgres:
            cursor.execute('SELECT id, name, user_id FROM accounts WHERE id = %s', (account_id,))
        else:
            cursor.execute('SELECT id, name, user_id FROM accounts WHERE id = ?', (account_id,))
        account = cursor.fetchone()
        if not account:
            conn.close()
            return jsonify({'success': False, 'error': 'Account not found'}), 404

        # Ownership check ‚Äî only account owner or admin can create traders
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id_check = get_current_user_id()
            acct_owner_id = account[2] if isinstance(account, tuple) else account.get('user_id')
            current_user_check = get_current_user()
            is_admin_check = current_user_check and current_user_check.is_admin
            if current_user_id_check and acct_owner_id and acct_owner_id != current_user_id_check and not is_admin_check:
                conn.close()
                return jsonify({'success': False, 'error': 'You can only create traders on your own accounts'}), 403

        # Check if link already exists
        if is_postgres:
            if subaccount_id:
                cursor.execute('SELECT id FROM traders WHERE recorder_id = %s AND account_id = %s AND subaccount_id = %s', 
                              (recorder_id, account_id, subaccount_id))
            else:
                cursor.execute('SELECT id FROM traders WHERE recorder_id = %s AND account_id = %s', (recorder_id, account_id))
        else:
            if subaccount_id:
                cursor.execute('SELECT id FROM traders WHERE recorder_id = ? AND account_id = ? AND subaccount_id = ?', 
                              (recorder_id, account_id, subaccount_id))
            else:
                cursor.execute('SELECT id FROM traders WHERE recorder_id = ? AND account_id = ?', (recorder_id, account_id))
        existing = cursor.fetchone()
        if existing:
            conn.close()
            return jsonify({'success': False, 'error': 'This recorder is already linked to this account'}), 400
        
        # Get current user_id
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        # Build enabled_accounts JSON from the account data provided
        create_multiplier = float(data.get('multiplier', 1.0)) if data.get('multiplier') else 1.0
        if create_multiplier <= 0:
            create_multiplier = 1.0
        initial_enabled_accounts = json.dumps([{
            'account_id': account_id,
            'account_name': account.get('name', ''),
            'subaccount_id': subaccount_id,
            'subaccount_name': subaccount_name,
            'is_demo': is_demo,
            'max_contracts': 0,
            'custom_ticker': '',
            'multiplier': create_multiplier
        }])

        # Create the trader link - DEFAULT TO DISABLED (user must explicitly enable)
        # This prevents accidental live trading on new setups
        if is_postgres:
            cursor.execute('''
                INSERT INTO traders (
                    recorder_id, account_id, subaccount_id, subaccount_name, is_demo, enabled,
                    initial_position_size, add_position_size, tp_targets, sl_enabled, sl_amount, sl_units, max_daily_loss,
                    user_id, enabled_accounts,
                    time_filter_1_enabled, time_filter_1_start, time_filter_1_stop,
                    time_filter_2_enabled, time_filter_2_start, time_filter_2_stop,
                    inverse_signals,
                    sl_type, tp_units, trim_units,
                    break_even_enabled, break_even_ticks, break_even_offset,
                    avg_down_enabled, avg_down_amount, avg_down_point, avg_down_units,
                    trail_trigger, trail_freq, add_delay, dca_enabled
                )
                VALUES (%s, %s, %s, %s, %s, false, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id
            ''', (recorder_id, account_id, subaccount_id, subaccount_name, bool(is_demo),
                  initial_position_size, add_position_size, tp_targets, bool(sl_enabled), sl_amount, sl_units, max_daily_loss,
                  current_user_id, initial_enabled_accounts,
                  bool(time_filter_1_enabled), time_filter_1_start, time_filter_1_stop,
                  bool(time_filter_2_enabled), time_filter_2_start, time_filter_2_stop,
                  bool(inverse_signals),
                  sl_type, tp_units, trim_units,
                  bool(break_even_enabled), int(break_even_ticks), int(break_even_offset),
                  bool(avg_down_enabled), int(avg_down_amount), float(avg_down_point), avg_down_units,
                  int(trail_trigger), int(trail_freq), int(add_delay), bool(dca_enabled)))
            result = cursor.fetchone()
            if result:
                trader_id = result.get('id') if isinstance(result, dict) else result[0]
            else:
                trader_id = None
        else:
            # SQLite: enabled = 0 (disabled by default)
            cursor.execute('''
                INSERT INTO traders (
                    recorder_id, account_id, subaccount_id, subaccount_name, is_demo, enabled,
                    initial_position_size, add_position_size, tp_targets, sl_enabled, sl_amount, sl_units, max_daily_loss,
                    user_id, enabled_accounts,
                    time_filter_1_enabled, time_filter_1_start, time_filter_1_stop,
                    time_filter_2_enabled, time_filter_2_start, time_filter_2_stop,
                    inverse_signals,
                    sl_type, tp_units, trim_units,
                    break_even_enabled, break_even_ticks, break_even_offset,
                    avg_down_enabled, avg_down_amount, avg_down_point, avg_down_units,
                    trail_trigger, trail_freq, add_delay, dca_enabled
                )
                VALUES (?, ?, ?, ?, ?, 0, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,
                        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (recorder_id, account_id, subaccount_id, subaccount_name, 1 if is_demo else 0,
                  initial_position_size, add_position_size, tp_targets, sl_enabled, sl_amount, sl_units, max_daily_loss,
                  current_user_id, initial_enabled_accounts,
                  1 if time_filter_1_enabled else 0, time_filter_1_start, time_filter_1_stop,
                  1 if time_filter_2_enabled else 0, time_filter_2_start, time_filter_2_stop,
                  1 if inverse_signals else 0,
                  sl_type, tp_units, trim_units,
                  1 if break_even_enabled else 0, int(break_even_ticks), int(break_even_offset),
                  1 if avg_down_enabled else 0, int(avg_down_amount), float(avg_down_point), avg_down_units,
                  int(trail_trigger), int(trail_freq), int(add_delay), 1 if dca_enabled else 0))
            trader_id = cursor.lastrowid
        
        conn.commit()
        conn.close()

        # Check subaccount ownership for abuse detection (flag-only, non-fatal)
        if subaccount_id and current_user_id:
            try:
                from subaccount_abuse_detection import check_subaccount_ownership, register_subaccounts
                is_shared, conflicting = check_subaccount_ownership(subaccount_id, current_user_id, broker='Tradovate')
                if is_shared:
                    logger.warning(f"SUBACCOUNT ABUSE: trader {trader_id} uses subaccount {subaccount_id} also owned by users {conflicting}")
                # Also register this subaccount in the registry
                register_subaccounts(current_user_id, account_id, [{'id': subaccount_id, 'name': subaccount_name, 'environment': 'demo' if is_demo else 'live'}], broker='Tradovate')
            except Exception as _chk_err:
                logger.debug(f"Subaccount abuse check skipped on trader create: {_chk_err}")

        env_label = "DEMO" if is_demo else "LIVE"
        display_name = f"{account['name']} {env_label} ({subaccount_name})" if subaccount_name else account['name']
        logger.info(f"Created trader {trader_id}: recorder '{recorder['name']}' -> {display_name} | Position: {initial_position_size}, SL: {sl_amount} {sl_units}")

        return jsonify({
            'success': True,
            'trader_id': trader_id,
            'message': f"Linked '{recorder['name']}' to '{display_name}'"
        })
        
    except Exception as e:
        logger.error(f"Error creating trader: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/traders/<int:trader_id>', methods=['PUT'])
def api_update_trader(trader_id):
    """Update a trader (enable/disable and risk settings)"""
    try:
        data = request.get_json()
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Check trader exists
        cursor.execute(f'SELECT id FROM traders WHERE id = {placeholder}', (trader_id,))
        if not cursor.fetchone():
            conn.close()
            return jsonify({'success': False, 'error': 'Trader not found'}), 404

        # Validate risk settings before saving
        validation_errors = validate_trader_risk_settings(data)
        if validation_errors:
            conn.close()
            return jsonify({'success': False, 'error': 'Invalid risk settings', 'validation_errors': validation_errors}), 400

        # Build dynamic update query based on provided fields
        updates = []
        params = []
        
        # Update enabled status if provided
        if 'enabled' in data:
            # PostgreSQL needs boolean True/False, SQLite needs 1/0
            enabled = bool(data['enabled']) if is_postgres else (1 if data['enabled'] else 0)
            updates.append(f'enabled = {placeholder}')
            params.append(enabled)
        
        # Update risk settings if provided (None/0 = fall back to recorder defaults)
        if 'initial_position_size' in data:
            val = data['initial_position_size']
            val = int(val) if val else None
            updates.append(f'initial_position_size = {placeholder}')
            params.append(val)

        if 'add_position_size' in data:
            val = data['add_position_size']
            val = int(val) if val else None
            updates.append(f'add_position_size = {placeholder}')
            params.append(val)
        
        if 'tp_targets' in data:
            tp_targets = data['tp_targets']
            # Convert to JSON string if it's a list
            if isinstance(tp_targets, list):
                tp_targets = json.dumps(tp_targets)
            updates.append(f'tp_targets = {placeholder}')
            params.append(tp_targets)
        
        if 'sl_enabled' in data:
            updates.append(f'sl_enabled = {placeholder}')
            params.append(bool(data['sl_enabled']) if is_postgres else (1 if data['sl_enabled'] else 0))
        
        if 'sl_amount' in data:
            updates.append(f'sl_amount = {placeholder}')
            params.append(float(data['sl_amount']))
        
        if 'sl_units' in data:
            updates.append(f'sl_units = {placeholder}')
            params.append(data['sl_units'])
        
        # SL Type (Fixed/Trailing)
        if 'sl_type' in data:
            updates.append(f'sl_type = {placeholder}')
            params.append(data['sl_type'])
        
        # Break-Even settings
        if 'break_even_enabled' in data:
            updates.append(f'break_even_enabled = {placeholder}')
            params.append(bool(data['break_even_enabled']) if is_postgres else (1 if data['break_even_enabled'] else 0))
        
        if 'break_even_ticks' in data:
            updates.append(f'break_even_ticks = {placeholder}')
            params.append(int(data['break_even_ticks']))
        
        # Averaging Down settings
        if 'avg_down_enabled' in data:
            updates.append(f'avg_down_enabled = {placeholder}')
            params.append(bool(data['avg_down_enabled']) if is_postgres else (1 if data['avg_down_enabled'] else 0))
        
        if 'avg_down_amount' in data:
            updates.append(f'avg_down_amount = {placeholder}')
            params.append(int(data['avg_down_amount']))
        
        if 'avg_down_point' in data:
            updates.append(f'avg_down_point = {placeholder}')
            params.append(float(data['avg_down_point']))
        
        if 'avg_down_units' in data:
            updates.append(f'avg_down_units = {placeholder}')
            params.append(data['avg_down_units'])

        # DCA Mode - PROTECTED SETTING - controls DCA logic directly
        # Frontend sends 'avg_down_enabled', backend stores as 'dca_enabled'
        dca_value = data.get('dca_enabled') if 'dca_enabled' in data else data.get('avg_down_enabled') if 'avg_down_enabled' in data else None
        if dca_value is not None:
            updates.append(f'dca_enabled = {placeholder}')
            params.append(bool(dca_value) if is_postgres else (1 if dca_value else 0))
            logger.info(f"üîÑ Setting dca_enabled={dca_value} for trader {trader_id}")

        # TP/Trim units
        if 'tp_units' in data:
            updates.append(f'tp_units = {placeholder}')
            params.append(data['tp_units'])
        
        if 'trim_units' in data:
            updates.append(f'trim_units = {placeholder}')
            params.append(data['trim_units'])
        
        # Time filter settings
        if 'time_filter_1_enabled' in data:
            updates.append(f'time_filter_1_enabled = {placeholder}')
            params.append(bool(data['time_filter_1_enabled']) if is_postgres else (1 if data['time_filter_1_enabled'] else 0))
        
        if 'time_filter_1_start' in data:
            updates.append(f'time_filter_1_start = {placeholder}')
            params.append(data['time_filter_1_start'])
        
        if 'time_filter_1_stop' in data:
            updates.append(f'time_filter_1_stop = {placeholder}')
            params.append(data['time_filter_1_stop'])
        
        if 'time_filter_2_enabled' in data:
            updates.append(f'time_filter_2_enabled = {placeholder}')
            params.append(bool(data['time_filter_2_enabled']) if is_postgres else (1 if data['time_filter_2_enabled'] else 0))
        
        if 'time_filter_2_start' in data:
            updates.append(f'time_filter_2_start = {placeholder}')
            params.append(data['time_filter_2_start'])
        
        if 'time_filter_2_stop' in data:
            updates.append(f'time_filter_2_stop = {placeholder}')
            params.append(data['time_filter_2_stop'])
        
        if 'max_daily_loss' in data:
            max_daily_loss_value = float(data['max_daily_loss'])
            updates.append(f'max_daily_loss = {placeholder}')
            params.append(max_daily_loss_value)
            logger.info(f"üí∞ Setting max_daily_loss={max_daily_loss_value} for trader {trader_id}")

        # Signal delay (Nth signal filter) - take every Nth signal
        if 'add_delay' in data:
            updates.append(f'add_delay = {placeholder}')
            params.append(int(data['add_delay']) if data['add_delay'] else 1)
            logger.info(f"üìù Adding add_delay={data['add_delay']} to trader {trader_id} update")

        # Break-even offset
        if 'break_even_offset' in data:
            updates.append(f'break_even_offset = {placeholder}')
            params.append(int(data['break_even_offset']))

        # Trailing stop settings
        if 'trail_trigger' in data:
            updates.append(f'trail_trigger = {placeholder}')
            params.append(int(data['trail_trigger']))

        if 'trail_freq' in data:
            updates.append(f'trail_freq = {placeholder}')
            params.append(int(data['trail_freq']))

        # Signal cooldown
        if 'signal_cooldown' in data:
            updates.append(f'signal_cooldown = {placeholder}')
            params.append(int(data['signal_cooldown']))

        # Max signals per session
        if 'max_signals_per_session' in data:
            updates.append(f'max_signals_per_session = {placeholder}')
            params.append(int(data['max_signals_per_session']))

        # Auto flat after cutoff
        if 'auto_flat_after_cutoff' in data:
            updates.append(f'auto_flat_after_cutoff = {placeholder}')
            params.append(bool(data['auto_flat_after_cutoff']) if is_postgres else (1 if data['auto_flat_after_cutoff'] else 0))

        # Inverse signals
        if 'inverse_signals' in data:
            updates.append(f'inverse_signals = {placeholder}')
            params.append(bool(data['inverse_signals']) if is_postgres else (1 if data['inverse_signals'] else 0))

        # Update account routing if provided
        if 'enabled_accounts' in data:
            enabled_accounts = data['enabled_accounts']
            logger.info(f"üì• [SAVE] Received enabled_accounts for trader {trader_id}: type={type(enabled_accounts)}, length={len(enabled_accounts) if isinstance(enabled_accounts, list) else 'N/A'}")
            if isinstance(enabled_accounts, list):
                logger.info(f"   Raw data: {json.dumps(enabled_accounts, indent=2)}")
            
            # CRITICAL: Ensure multipliers are properly formatted as floats
            if isinstance(enabled_accounts, list):
                for idx, acct in enumerate(enabled_accounts):
                    # Always ensure multiplier exists and is a float
                    account_name = acct.get('account_name', 'Unknown')
                    account_id = acct.get('account_id', 'N/A')
                    subaccount_id = acct.get('subaccount_id', 'N/A')
                    
                    if 'multiplier' not in acct:
                        acct['multiplier'] = 1.0
                        logger.warning(f"  ‚ö†Ô∏è [{idx}] Account {account_name} (id={account_id}, subaccount_id={subaccount_id}): multiplier NOT PROVIDED, defaulting to 1.0")
                    else:
                        # Ensure multiplier is a float, not a string
                        try:
                            multiplier_val = acct['multiplier']
                            original_type = type(multiplier_val)
                            acct['multiplier'] = float(multiplier_val) if multiplier_val is not None and multiplier_val != '' else 1.0
                            if acct['multiplier'] <= 0:
                                acct['multiplier'] = 1.0
                                logger.warning(f"  ‚ö†Ô∏è [{idx}] Account {account_name} (id={account_id}, subaccount_id={subaccount_id}): Invalid multiplier (<=0), defaulting to 1.0")
                            else:
                                logger.info(f"  ‚úÖ [{idx}] Account {account_name} (id={account_id}, subaccount_id={subaccount_id}): multiplier={acct['multiplier']} (was {multiplier_val}, type={original_type})")
                        except (ValueError, TypeError) as e:
                            acct['multiplier'] = 1.0
                            logger.error(f"  ‚ùå [{idx}] Account {account_name} (id={account_id}, subaccount_id={subaccount_id}): Invalid multiplier (value={acct.get('multiplier', 'N/A')}), defaulting to 1.0: {e}")
            
            # Convert to JSON string if it's a list
            if isinstance(enabled_accounts, list):
                # Final verification: ensure all multipliers are floats
                for acct in enabled_accounts:
                    if 'multiplier' in acct:
                        try:
                            acct['multiplier'] = float(acct['multiplier'])
                        except (ValueError, TypeError):
                            acct['multiplier'] = 1.0
                enabled_accounts_json = json.dumps(enabled_accounts, ensure_ascii=False)
                logger.info(f"üíæ [SAVE] Final JSON to save: {enabled_accounts_json[:500]}")
                enabled_accounts = enabled_accounts_json
            updates.append(f'enabled_accounts = {placeholder}')
            params.append(enabled_accounts)
            logger.info(f"‚úÖ [SAVE] Will update enabled_accounts field with {len(json.loads(enabled_accounts) if isinstance(enabled_accounts, str) else enabled_accounts)} accounts")
        
        # Execute update if there are fields to update (with retry for db locks)
        logger.info(f"üîç DEBUG: updates list has {len(updates)} items: {updates}")
        logger.info(f"üîç DEBUG: params has {len(params)} items")
        if updates:
            params.append(trader_id)
            query = f"UPDATE traders SET {', '.join(updates)} WHERE id = {placeholder}"

            # Log what we're updating for debugging
            logger.info(f"üìù Updating trader {trader_id} with {len(updates)} fields")
            logger.info(f"üìù SQL: {query}")
            if 'enabled_accounts' in data:
                logger.info(f"   - enabled_accounts will be updated (contains multipliers)")
            
            for attempt in range(5):
                try:
                    cursor.execute(query, params)
                    break
                except Exception as e:
                    if 'locked' in str(e) and attempt < 4:
                        import time
                        time.sleep(0.2 * (attempt + 1))
                        continue
                    raise
            
            # Verify the update worked by reading it back
            cursor.execute(f'SELECT enabled_accounts FROM traders WHERE id = {placeholder}', (trader_id,))
            verify_row = cursor.fetchone()
            if verify_row:
                verify_enabled_accounts = verify_row[0]
                logger.info(f"‚úÖ Verified: enabled_accounts saved successfully")
                if verify_enabled_accounts:
                    try:
                        verify_parsed = json.loads(verify_enabled_accounts) if isinstance(verify_enabled_accounts, str) else verify_enabled_accounts
                        logger.info(f"‚úÖ VERIFIED: enabled_accounts saved with {len(verify_parsed)} accounts")
                        for acct in verify_parsed:
                            logger.info(f"   ‚úì Account {acct.get('account_name')} (subaccount_id={acct.get('subaccount_id')}): multiplier={acct.get('multiplier', 'NOT SET')}, max_contracts={acct.get('max_contracts', 0)}")
                    except Exception as verify_err:
                        logger.error(f"‚ùå Error verifying enabled_accounts: {verify_err}")
                        import traceback
                        logger.error(traceback.format_exc())
                else:
                    logger.warning(f"‚ö†Ô∏è VERIFICATION FAILED: enabled_accounts is None or empty after save!")
        
        # CRITICAL: DO NOT update the recorder table TRADING SETTINGS when updating a trader!
        # The recorder is the baseline/master settings and should NEVER be changed by trader edits.
        # Traders inherit from the recorder but can override settings in the traders table.
        # EXCEPTION: Privacy and tier are recorder-level admin/owner controls, not trading settings.

        # Update recorder privacy if provided (owner or admin only)
        if 'recorder_is_private' in data or 'required_tier' in data:
            # Get the recorder_id and ownership info for this trader
            cursor.execute(f'SELECT recorder_id FROM traders WHERE id = {placeholder}', (trader_id,))
            rec_row = cursor.fetchone()
            if rec_row:
                rec_id = rec_row[0] if isinstance(rec_row, tuple) else rec_row.get('recorder_id')
                # Get recorder owner
                cursor.execute(f'SELECT user_id FROM recorders WHERE id = {placeholder}', (rec_id,))
                rec_owner_row = cursor.fetchone()
                rec_owner_id = (rec_owner_row[0] if isinstance(rec_owner_row, tuple) else rec_owner_row.get('user_id')) if rec_owner_row else None

                # Get current user info
                current_user_id = None
                current_is_admin = False
                if USER_AUTH_AVAILABLE and is_logged_in():
                    current_user_id = get_current_user_id()
                    user = get_current_user()
                    if user and user.is_admin:
                        current_is_admin = True

                # is_private: owner or admin can change
                if 'recorder_is_private' in data:
                    is_owner = (current_user_id is not None and rec_owner_id is not None and int(current_user_id) == int(rec_owner_id))
                    if is_owner or current_is_admin:
                        priv_val = bool(data['recorder_is_private']) if is_postgres else (1 if data['recorder_is_private'] else 0)
                        cursor.execute(f'UPDATE recorders SET is_private = {placeholder} WHERE id = {placeholder}', (priv_val, rec_id))
                        logger.info(f"Updated recorder {rec_id} is_private={data['recorder_is_private']}")

                # required_tier: admin only
                if 'required_tier' in data:
                    if current_is_admin:
                        tier_val = data['required_tier'] if data['required_tier'] in ('public', 'premium', 'elite') else 'public'
                        cursor.execute(f'UPDATE recorders SET required_tier = {placeholder} WHERE id = {placeholder}', (tier_val, rec_id))
                        logger.info(f"Updated recorder {rec_id} required_tier={tier_val}")

        conn.commit()

        # ============================================================
        # üîç VERIFY max_daily_loss was saved correctly
        # ============================================================
        if 'max_daily_loss' in data:
            try:
                verify_cursor = conn.cursor()
                verify_cursor.execute(f'SELECT max_daily_loss FROM traders WHERE id = {placeholder}', (trader_id,))
                verify_row = verify_cursor.fetchone()
                if verify_row:
                    saved_value = verify_row[0] if isinstance(verify_row, tuple) else verify_row.get('max_daily_loss')
                    logger.info(f"‚úÖ [VERIFY] max_daily_loss saved as {saved_value} for trader {trader_id}")
                else:
                    logger.error(f"‚ùå [VERIFY] Could not read trader {trader_id} after commit!")
            except Exception as e:
                logger.error(f"‚ùå [VERIFY] Error verifying max_daily_loss: {e}")

        # ============================================================
        # üîç FINAL VERIFICATION: Read from fresh connection to confirm persistence
        # ============================================================
        if 'enabled_accounts' in data:
            try:
                verify_conn = get_db_connection()
                verify_cursor = verify_conn.cursor()
                verify_cursor.execute(f'SELECT enabled_accounts FROM traders WHERE id = {placeholder}', (trader_id,))
                final_verify = verify_cursor.fetchone()
                verify_conn.close()
                
                if final_verify:
                    final_enabled_accounts = final_verify[0]
                    if final_enabled_accounts:
                        try:
                            final_parsed = json.loads(final_enabled_accounts) if isinstance(final_enabled_accounts, str) else final_enabled_accounts
                            logger.info(f"üîç [FINAL VERIFY] Fresh DB read: {len(final_parsed)} accounts with multipliers:")
                            for acct in final_parsed:
                                mult = acct.get('multiplier', 'NOT SET')
                                logger.info(f"   ‚Üí {acct.get('account_name')}: multiplier={mult} (type={type(mult)})")
                        except Exception as e:
                            logger.error(f"‚ùå [FINAL VERIFY] Error parsing: {e}")
                    else:
                        logger.error(f"‚ùå [FINAL VERIFY] enabled_accounts is empty after commit!")
                else:
                    logger.error(f"‚ùå [FINAL VERIFY] Could not read trader {trader_id} after commit!")
            except Exception as e:
                logger.error(f"‚ùå [FINAL VERIFY] Error in final verification: {e}")
                import traceback
                logger.error(traceback.format_exc())
        
        # ============================================================
        # üîÑ CRITICAL: Refresh tokens when strategy is ENABLED
        # ============================================================
        # When user comes back after being away, tokens may have expired.
        # Force refresh all linked account tokens to ensure trading works.
        # ============================================================
        if 'enabled' in data and data['enabled']:
            logger.info(f"üîÑ Strategy enabled - validating/refreshing account tokens...")
            try:
                # Get accounts linked to this trader
                cursor.execute(f'''
                    SELECT enabled_accounts FROM traders WHERE id = {placeholder}
                ''', (trader_id,))
                trader_row = cursor.fetchone()
                if trader_row and trader_row[0]:
                    enabled_accounts = json.loads(trader_row[0]) if isinstance(trader_row[0], str) else trader_row[0]
                    account_ids = set()
                    for acct in enabled_accounts:
                        if 'account_id' in acct:
                            account_ids.add(acct['account_id'])
                    
                    logger.info(f"üîÑ Refreshing tokens for {len(account_ids)} account(s)...")
                    for account_id in account_ids:
                        # Force token refresh (bypasses rate limit for this critical path)
                        try:
                            refreshed = try_refresh_tradovate_token(account_id)
                            if refreshed:
                                logger.info(f"‚úÖ Token refreshed for account {account_id}")
                            else:
                                # Check if token is still valid
                                cursor.execute(f'SELECT token_expires_at FROM accounts WHERE id = {placeholder}', (account_id,))
                                exp_row = cursor.fetchone()
                                if exp_row and exp_row[0]:
                                    from datetime import datetime
                                    try:
                                        exp_time = datetime.strptime(str(exp_row[0]).split('.')[0], '%Y-%m-%d %H:%M:%S')
                                        if exp_time > datetime.now():
                                            logger.info(f"‚úÖ Token for account {account_id} still valid (expires {exp_row[0]})")
                                        else:
                                            logger.warning(f"‚ö†Ô∏è Token for account {account_id} EXPIRED - user may need to re-authenticate")
                                    except:
                                        pass
                        except Exception as refresh_err:
                            logger.warning(f"‚ö†Ô∏è Could not refresh token for account {account_id}: {refresh_err}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error during token refresh on enable: {e}")
        
        conn.close()
        
        logger.info(f"Updated trader {trader_id}: {list(data.keys())}")
        if 'enabled_accounts' in data:
            logger.info(f"  - Account routing: {len(data['enabled_accounts']) if isinstance(data['enabled_accounts'], list) else 'N/A'} accounts enabled")
        return jsonify({'success': True, 'message': 'Trader and recorder settings updated'})
        
    except Exception as e:
        logger.error(f"‚ùå Error updating trader {trader_id}: {e}")
        import traceback
        error_trace = traceback.format_exc()
        logger.error(error_trace)
        return jsonify({'success': False, 'error': str(e), 'traceback': error_trace}), 500

@app.route('/api/traders/<int:trader_id>', methods=['DELETE'])
def api_delete_trader(trader_id):
    """Delete a trader (unlink recorder from account)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Check trader exists
        cursor.execute(f'SELECT id FROM traders WHERE id = {placeholder}', (trader_id,))
        if not cursor.fetchone():
            conn.close()
            return jsonify({'success': False, 'error': 'Trader not found'}), 404
        
        cursor.execute(f'DELETE FROM traders WHERE id = {placeholder}', (trader_id,))
        conn.commit()
        conn.close()
        
        logger.info(f"Deleted trader {trader_id}")
        return jsonify({'success': True, 'message': 'Trader deleted'})
        
    except Exception as e:
        logger.error(f"Error deleting trader {trader_id}: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/traders/cleanup-duplicates', methods=['POST'])
def api_cleanup_duplicate_traders():
    """Remove duplicate traders, keeping the one with the lowest ID (oldest)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()

        # Find all duplicates (same recorder_id, account_id, subaccount_id)
        if is_postgres:
            cursor.execute('''
                SELECT recorder_id, account_id, subaccount_id, COUNT(*) as cnt,
                       MIN(id) as keep_id, array_agg(id ORDER BY id) as all_ids
                FROM traders
                WHERE subaccount_id IS NOT NULL
                GROUP BY recorder_id, account_id, subaccount_id
                HAVING COUNT(*) > 1
            ''')
        else:
            cursor.execute('''
                SELECT recorder_id, account_id, subaccount_id, COUNT(*) as cnt,
                       MIN(id) as keep_id, GROUP_CONCAT(id) as all_ids
                FROM traders
                WHERE subaccount_id IS NOT NULL
                GROUP BY recorder_id, account_id, subaccount_id
                HAVING COUNT(*) > 1
            ''')

        duplicates = cursor.fetchall()
        deleted_count = 0
        deleted_ids = []

        for dup in duplicates:
            if isinstance(dup, dict):
                keep_id = dup['keep_id']
                all_ids = dup['all_ids']
                recorder_id = dup['recorder_id']
                account_id = dup['account_id']
                subaccount_id = dup['subaccount_id']
            else:
                keep_id = dup[4]
                all_ids = dup[5]
                recorder_id = dup[0]
                account_id = dup[1]
                subaccount_id = dup[2]

            # Parse all_ids (PostgreSQL returns list, SQLite returns comma-separated string)
            if isinstance(all_ids, str):
                ids_to_check = [int(x) for x in all_ids.split(',')]
            else:
                ids_to_check = list(all_ids)

            # Delete all except the one to keep (lowest ID)
            ids_to_delete = [id for id in ids_to_check if id != keep_id]

            for del_id in ids_to_delete:
                if is_postgres:
                    cursor.execute('DELETE FROM traders WHERE id = %s', (del_id,))
                else:
                    cursor.execute('DELETE FROM traders WHERE id = ?', (del_id,))
                deleted_ids.append(del_id)
                deleted_count += 1
                logger.info(f"Deleted duplicate trader {del_id} (keeping {keep_id} for recorder={recorder_id}, account={account_id}, subaccount={subaccount_id})")

        conn.commit()
        conn.close()

        return jsonify({
            'success': True,
            'message': f'Cleaned up {deleted_count} duplicate traders',
            'deleted_count': deleted_count,
            'deleted_ids': deleted_ids
        })

    except Exception as e:
        logger.error(f"Error cleaning up duplicate traders: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/traders/<int:trader_id>/toggle', methods=['POST'])
def api_toggle_trader(trader_id):
    """Toggle a single trader's enabled status (per-account control)."""
    try:
        # Auth check: require login when auth is available
        current_user_id = None
        if USER_AUTH_AVAILABLE:
            if not is_logged_in():
                return jsonify({'success': False, 'error': 'Not authenticated'}), 401
            current_user_id = get_current_user_id()

        data = request.get_json() or {}
        enabled = data.get('enabled', False)

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        # Get trader info with ownership check
        cursor.execute(f'''
            SELECT t.id, t.user_id, r.name as recorder_name, a.name as account_name, a.user_id as account_owner_id
            FROM traders t
            JOIN recorders r ON t.recorder_id = r.id
            JOIN accounts a ON t.account_id = a.id
            WHERE t.id = {placeholder}
        ''', (trader_id,))

        trader = cursor.fetchone()
        if not trader:
            conn.close()
            return jsonify({'success': False, 'error': 'Trader not found'}), 404

        # Ownership check: user must own the trader or the account
        if current_user_id:
            if hasattr(trader, 'keys'):
                trader_owner = trader['user_id']
                account_owner = trader['account_owner_id']
            else:
                trader_owner = trader[1]
                account_owner = trader[4]
            if current_user_id != trader_owner and current_user_id != account_owner:
                conn.close()
                return jsonify({'success': False, 'error': 'Not authorized'}), 403
        
        # Get names for logging
        if hasattr(trader, 'keys'):
            recorder_name = trader['recorder_name']
            account_name = trader['account_name']
        else:
            recorder_name = trader[2]
            account_name = trader[3]
        
        # Update trader enabled status
        enabled_value = bool(enabled) if is_postgres else (1 if enabled else 0)
        cursor.execute(f'''
            UPDATE traders SET enabled = {placeholder} WHERE id = {placeholder}
        ''', (enabled_value, trader_id))
        
        conn.commit()
        conn.close()
        
        action = 'enabled' if enabled else 'disabled'
        logger.info(f"üìä {action.upper()} trader #{trader_id}: {recorder_name} ‚Üí {account_name}")
        
        return jsonify({
            'success': True,
            'message': f'{recorder_name} on {account_name} {action}',
            'trader_id': trader_id,
            'enabled': enabled
        })
        
    except Exception as e:
        logger.error(f"Error toggling trader {trader_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/traders/<int:trader_id>/close-position', methods=['POST'])
def api_close_trader_position(trader_id):
    """Close position for a single trader (one account only)."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Get trader info
        cursor.execute(f'''
            SELECT t.id, t.recorder_id, r.name as recorder_name, a.name as account_name
            FROM traders t
            JOIN recorders r ON t.recorder_id = r.id
            JOIN accounts a ON t.account_id = a.id
            WHERE t.id = {placeholder}
        ''', (trader_id,))
        
        trader = cursor.fetchone()
        if not trader:
            conn.close()
            return jsonify({'success': False, 'error': 'Trader not found'}), 404
        
        if hasattr(trader, 'keys'):
            recorder_id = trader['recorder_id']
            recorder_name = trader['recorder_name']
            account_name = trader['account_name']
        else:
            recorder_id = trader[1]
            recorder_name = trader[2]
            account_name = trader[3]
        
        # Close any open trades for this recorder
        cursor.execute(f'''
            UPDATE recorded_trades 
            SET status = 'closed', exit_reason = 'manual_close', exit_time = CURRENT_TIMESTAMP
            WHERE recorder_id = {placeholder} AND status = 'open'
        ''', (recorder_id,))
        closed_count = cursor.rowcount
        
        conn.commit()
        conn.close()
        
        logger.info(f"üìä Closed {closed_count} position(s) for {recorder_name} on {account_name}")
        
        return jsonify({
            'success': True,
            'message': f'Closed {closed_count} position(s) for {recorder_name} on {account_name}',
            'closed_count': closed_count
        })
        
    except Exception as e:
        logger.error(f"Error closing position for trader {trader_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/traders/<int:trader_id>/test-connection', methods=['POST'])
def api_test_trader_connection(trader_id):
    """Test if the trader's linked account connection is working"""
    import asyncio
    
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get trader with account info - CRITICAL: Include environment for demo/live detection
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        cursor.execute(f'''
            SELECT t.*, a.tradovate_token, a.username, a.password, a.id as account_id,
                   a.name as account_name, t.subaccount_name, t.is_demo, a.environment
            FROM traders t
            JOIN accounts a ON t.account_id = a.id
            WHERE t.id = {placeholder}
        ''', (trader_id,))
        trader_row = cursor.fetchone()
        
        if not trader_row:
            conn.close()
            return jsonify({'success': False, 'error': 'Trader not found'}), 404
        
        trader = dict(trader_row)
        conn.close()
        
        account_name = trader.get('account_name', 'Unknown')
        subaccount_name = trader.get('subaccount_name', '')
        # CRITICAL FIX: Use environment as source of truth for demo vs live
        env = (trader.get('environment') or 'demo').lower()
        is_demo = env != 'live'
        tradovate_token = trader.get('tradovate_token')
        username = trader.get('username')
        password = trader.get('password')
        
        logger.info(f"üîó Testing connection for trader {trader_id} ({account_name} / {subaccount_name})")
        
        # Test 1: Check if we have credentials
        if not tradovate_token and not (username and password):
            return jsonify({
                'success': True,
                'connected': False,
                'error': 'No OAuth token or API credentials found. Please re-authenticate.'
            })
        
        # Test 2: Try to validate token or authenticate
        async def test_connection():
            from phantom_scraper.tradovate_integration import TradovateIntegration
            from tradovate_api_access import TradovateAPIAccess
            
            # Try OAuth token first
            if tradovate_token:
                try:
                    tradovate = TradovateIntegration(demo=is_demo)
                    await tradovate.__aenter__()
                    tradovate.access_token = tradovate_token
                    
                    # Try to get positions (simple API test)
                    positions = await tradovate.get_positions(account_id=trader.get('subaccount_id'))
                    await tradovate.__aexit__(None, None, None)
                    
                    if positions is not None:
                        return {'connected': True, 'method': 'OAuth', 'positions': len(positions)}
                except Exception as e:
                    logger.warning(f"OAuth test failed: {e}")
            
            # Try API Access
            if username and password:
                try:
                    api_access = TradovateAPIAccess(demo=is_demo)
                    result = await api_access.login(username=username, password=password)
                    if result.get('success'):
                        return {'connected': True, 'method': 'API Access'}
                    else:
                        return {'connected': False, 'error': result.get('error', 'Login failed')}
                except Exception as e:
                    return {'connected': False, 'error': str(e)}
            
            return {'connected': False, 'error': 'No valid authentication method'}
        
        # Run async test
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(test_connection())
            loop.close()
        except RuntimeError:
            result = asyncio.run(test_connection())
        
        if result.get('connected'):
            logger.info(f"‚úÖ Connection test PASSED for {account_name} ({result.get('method')})")
        else:
            logger.warning(f"‚ö†Ô∏è Connection test FAILED for {account_name}: {result.get('error')}")
        
        return jsonify({
            'success': True,
            'connected': result.get('connected', False),
            'method': result.get('method'),
            'error': result.get('error'),
            'account_name': account_name,
            'subaccount_name': subaccount_name
        })
        
    except Exception as e:
        logger.error(f"Error testing connection for trader {trader_id}: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================
# OPERATIONAL TOOLS - Broker state, order management, webhook retry
# (Restored from backup-feb5-before-revert branch)
# ============================================================

@app.route('/api/traders/<int:trader_id>/broker-state', methods=['GET'])
def api_get_broker_state(trader_id):
    """Get actual broker positions and orders for a trader - for debugging"""
    import asyncio

    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        cursor.execute(f'''
            SELECT t.*, a.tradovate_token, a.environment, a.name as account_name
            FROM traders t
            JOIN accounts a ON t.account_id = a.id
            WHERE t.id = {placeholder}
        ''', (trader_id,))
        trader_row = cursor.fetchone()
        conn.close()

        if not trader_row:
            return jsonify({'success': False, 'error': 'Trader not found'}), 404

        trader = dict(trader_row)
        token = trader.get('tradovate_token')
        env = (trader.get('environment') or 'demo').lower()
        is_demo = env != 'live'
        subaccount_id = trader.get('subaccount_id')

        if not token:
            return jsonify({'success': False, 'error': 'No token'})

        async def query_broker():
            from phantom_scraper.tradovate_integration import TradovateIntegration

            tradovate = TradovateIntegration(demo=is_demo)
            await tradovate.__aenter__()
            tradovate.access_token = token

            positions = await tradovate.get_positions(account_id=subaccount_id)
            open_positions = []
            for p in (positions or []):
                if p.get('netPos', 0) != 0:
                    open_positions.append({
                        'symbol': p.get('contractId'),
                        'side': 'LONG' if p.get('netPos') > 0 else 'SHORT',
                        'qty': abs(p.get('netPos')),
                        'avg_price': p.get('netPrice')
                    })

            orders = await tradovate.get_orders_with_details(account_id=str(subaccount_id))
            working_orders = []
            tp_count = 0
            sl_count = 0
            for o in (orders or []):
                status = str(o.get('ordStatus', '')).upper()
                if status in ['WORKING', 'ACCEPTED', 'PENDINGNEW', 'NEW']:
                    otype = str(o.get('orderType', '')).upper()
                    order_info = {
                        'id': o.get('id'),
                        'action': o.get('action'),
                        'type': otype,
                        'price': o.get('price'),
                        'qty': o.get('orderQty'),
                        'symbol': o.get('symbol')
                    }
                    working_orders.append(order_info)
                    if otype == 'LIMIT':
                        tp_count += 1
                    elif otype in ['STOP', 'STOPLIMIT']:
                        sl_count += 1

            await tradovate.__aexit__(None, None, None)

            return {
                'positions': open_positions,
                'orders': working_orders,
                'tp_count': tp_count,
                'sl_count': sl_count
            }

        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(query_broker())
            loop.close()
        except RuntimeError:
            result = asyncio.run(query_broker())

        return jsonify({
            'success': True,
            'trader_id': trader_id,
            'account': trader.get('account_name'),
            'subaccount': trader.get('subaccount_name'),
            **result
        })

    except Exception as e:
        logger.error(f"Error getting broker state: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/account/<int:account_id>/broker-state', methods=['GET'])
def api_account_broker_state(account_id):
    """Query broker for account positions and working orders - for debugging"""
    import requests as sync_requests

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(f'''
            SELECT id, name, tradovate_token, environment, broker
            FROM accounts WHERE id = {placeholder}
        ''', (account_id,))
        account = cursor.fetchone()

        if not account:
            return jsonify({'success': False, 'error': 'Account not found'}), 404

        account = dict(account)
        token = account.get('tradovate_token')
        env = (account.get('environment') or 'demo').lower()
        name = account.get('name')

        if not token:
            return jsonify({'success': False, 'error': 'No token for account'}), 400

        cursor.execute(f'''
            SELECT DISTINCT subaccount_id, subaccount_name
            FROM traders
            WHERE account_id = {placeholder} AND subaccount_id IS NOT NULL
        ''', (account_id,))
        subaccounts = cursor.fetchall()
        conn.close()

        base_url = f"https://{env}.tradovateapi.com/v1"
        headers = {"Authorization": f"Bearer {token}"}

        result = {
            'success': True,
            'account_id': account_id,
            'account_name': name,
            'environment': env,
            'subaccounts': []
        }

        pos_resp = sync_requests.get(f"{base_url}/position/list", headers=headers, timeout=10)
        if pos_resp.status_code != 200:
            return jsonify({'success': False, 'error': f'Broker auth failed: {pos_resp.status_code}'}), 401

        positions = pos_resp.json()
        orders_resp = sync_requests.get(f"{base_url}/order/list", headers=headers, timeout=10)
        orders = orders_resp.json() if orders_resp.status_code == 200 else []

        for sub in subaccounts:
            sub = dict(sub)
            sub_id = sub.get('subaccount_id')
            sub_name = sub.get('subaccount_name')

            sub_result = {
                'subaccount_id': sub_id,
                'subaccount_name': sub_name,
                'positions': [],
                'working_orders': []
            }

            for p in positions:
                if p.get('accountId') == sub_id and p.get('netPos', 0) != 0:
                    sub_result['positions'].append({
                        'side': 'LONG' if p.get('netPos') > 0 else 'SHORT',
                        'quantity': abs(p.get('netPos')),
                        'avg_price': p.get('netPrice'),
                        'contract_id': p.get('contractId')
                    })

            for o in orders:
                if o.get('accountId') == sub_id and str(o.get('ordStatus', '')).upper() in ['WORKING', 'ACCEPTED', 'PENDINGNEW', 'NEW']:
                    try:
                        ver_resp = sync_requests.get(
                            f"{base_url}/orderVersion/deps",
                            params={"masterid": o.get('id')},
                            headers=headers,
                            timeout=5
                        )
                        if ver_resp.status_code == 200:
                            versions = ver_resp.json()
                            if versions:
                                v = versions[-1]
                                order_type = v.get('orderType', 'Unknown')
                                label = 'TP' if order_type == 'Limit' else ('SL' if order_type in ['Stop', 'StopLimit'] else order_type)
                                sub_result['working_orders'].append({
                                    'id': o.get('id'),
                                    'type': label,
                                    'action': v.get('action'),
                                    'qty': v.get('orderQty'),
                                    'price': v.get('price')
                                })
                    except Exception:
                        pass

            result['subaccounts'].append(sub_result)

        return jsonify(result)

    except Exception as e:
        logger.error(f"Error querying broker state: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/account/<int:account_id>/cancel-invalid-orders', methods=['POST'])
def api_cancel_invalid_orders(account_id):
    """Cancel all working orders with invalid prices (negative or null)"""
    import requests as sync_requests

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(f'''
            SELECT id, name, tradovate_token, environment
            FROM accounts WHERE id = {placeholder}
        ''', (account_id,))
        account = cursor.fetchone()

        if not account:
            return jsonify({'success': False, 'error': 'Account not found'}), 404

        account = dict(account)
        token = account.get('tradovate_token')
        env = (account.get('environment') or 'demo').lower()

        if not token:
            return jsonify({'success': False, 'error': 'No token'}), 400

        cursor.execute(f'''
            SELECT DISTINCT subaccount_id FROM traders
            WHERE account_id = {placeholder} AND subaccount_id IS NOT NULL
        ''', (account_id,))
        subaccounts = [dict(r)['subaccount_id'] for r in cursor.fetchall()]
        conn.close()

        base_url = f"https://{env}.tradovateapi.com/v1"
        headers = {"Authorization": f"Bearer {token}"}

        cancelled = []

        orders_resp = sync_requests.get(f"{base_url}/order/list", headers=headers, timeout=10)
        if orders_resp.status_code != 200:
            return jsonify({'success': False, 'error': 'Broker auth failed'}), 401

        for o in orders_resp.json():
            if o.get('accountId') not in subaccounts:
                continue
            if str(o.get('ordStatus', '')).upper() not in ['WORKING', 'ACCEPTED', 'PENDINGNEW', 'NEW']:
                continue

            oid = o.get('id')
            try:
                ver_resp = sync_requests.get(f"{base_url}/orderVersion/deps", params={"masterid": oid}, headers=headers, timeout=5)
                if ver_resp.status_code == 200:
                    versions = ver_resp.json()
                    if versions:
                        v = versions[-1]
                        price = v.get('price')
                        stop_price = v.get('stopPrice')
                        order_type = v.get('orderType', '')

                        is_invalid = False
                        if price is not None and price <= 0:
                            is_invalid = True
                        if stop_price is not None and stop_price <= 0:
                            is_invalid = True
                        if order_type == 'Limit' and price is None:
                            is_invalid = True
                        if order_type in ['Stop', 'StopLimit'] and stop_price is None:
                            is_invalid = True

                        if is_invalid:
                            sync_requests.post(f"{base_url}/order/cancelorder", headers=headers, json={"orderId": oid}, timeout=10)
                            cancelled.append({'id': oid, 'price': price, 'stop_price': stop_price})
            except Exception:
                pass

        return jsonify({'success': True, 'cancelled_count': len(cancelled), 'cancelled': cancelled})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/account/<int:account_id>/cancel-order/<int:order_id>', methods=['POST'])
def api_cancel_specific_order(account_id, order_id):
    """Cancel a specific order by ID"""
    import requests as sync_requests

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(f'SELECT tradovate_token, environment FROM accounts WHERE id = {placeholder}', (account_id,))
        account = cursor.fetchone()
        conn.close()

        if not account:
            return jsonify({'success': False, 'error': 'Account not found'}), 404

        account = dict(account)
        token = account.get('tradovate_token')
        env = (account.get('environment') or 'demo').lower()

        base_url = f"https://{env}.tradovateapi.com/v1"
        headers = {"Authorization": f"Bearer {token}"}

        resp = sync_requests.post(f"{base_url}/order/cancelorder", headers=headers, json={"orderId": order_id}, timeout=10)

        if resp.status_code == 200:
            return jsonify({'success': True, 'cancelled_order_id': order_id})
        else:
            return jsonify({'success': False, 'error': resp.text}), 400

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/webhooks/retry-failed', methods=['POST'])
@admin_or_api_key_required
def api_retry_failed_webhooks():
    """Retry recently failed webhooks by resetting them to 'pending'.
    Optional: ?minutes=N to retry webhooks from the last N minutes (default: 30)"""
    try:
        minutes = request.args.get('minutes', 30, type=int)
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()

        if is_postgres:
            cursor.execute('''
                UPDATE incoming_webhooks
                SET status = 'pending',
                    retry_count = retry_count + 1,
                    error_message = 'Retry via API',
                    updated_at = NOW()
                WHERE status = 'failed'
                AND created_at > NOW() - INTERVAL '%s minutes'
            ''' % minutes)
        else:
            cursor.execute('''
                UPDATE incoming_webhooks
                SET status = 'pending',
                    retry_count = retry_count + 1,
                    error_message = 'Retry via API',
                    updated_at = datetime('now')
                WHERE status = 'failed'
                AND created_at > datetime('now', '-%s minutes')
            ''' % minutes)

        retry_count = cursor.rowcount
        conn.commit()
        conn.close()

        logger.info(f"RETRY FAILED: {retry_count} failed webhooks reset to 'pending' (last {minutes} min)")

        return jsonify({
            'success': True,
            'message': f'Retrying {retry_count} failed webhooks from last {minutes} minutes',
            'retry_count': retry_count
        })

    except Exception as e:
        logger.error(f"Error retrying failed webhooks: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================
# HIGH-PERFORMANCE SIGNAL PROCESSOR (Background Thread)
# ============================================================

# Cache for recorder lookups (avoid repeated DB queries)
recorder_cache = {}
recorder_cache_time = {}
CACHE_TTL = 60  # Cache recorder info for 60 seconds

def get_cached_recorder(webhook_token):
    """Get recorder from cache or database"""
    now = time.time()
    if webhook_token in recorder_cache:
        if now - recorder_cache_time.get(webhook_token, 0) < CACHE_TTL:
            return recorder_cache[webhook_token]
    
    conn = get_db_connection()
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    if is_using_postgres():
        cursor.execute('SELECT * FROM recorders WHERE webhook_token = %s AND recording_enabled = true', (webhook_token,))
    else:
        cursor.execute('SELECT * FROM recorders WHERE webhook_token = ? AND recording_enabled = 1', (webhook_token,))
    row = cursor.fetchone()
    conn.close()
    
    if row:
        recorder = dict(row)
        recorder_cache[webhook_token] = recorder
        recorder_cache_time[webhook_token] = now
        return recorder
    return None

def process_signal_batch(signals):
    """Process a batch of signals efficiently with single DB connection"""
    if not signals:
        return
    
    conn = get_db_connection()
    conn.execute('PRAGMA journal_mode=WAL')
    conn.execute('PRAGMA synchronous=NORMAL')
    cursor = conn.cursor()
    
    try:
        # Batch insert signals
        for sig in signals:
            cursor.execute('''
                INSERT INTO recorded_signals 
                (recorder_id, action, ticker, price, position_size, market_position, signal_type, raw_data, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                sig['recorder_id'],
                sig['action'],
                sig['ticker'],
                sig['price'],
                sig.get('position_size'),
                sig.get('market_position'),
                sig['signal_type'],
                json.dumps(sig['raw_data']),
                sig['timestamp']
            ))
        conn.commit()
    except Exception as e:
        logger.error(f"Batch signal insert error: {e}")
        conn.rollback()
    finally:
        conn.close()

def signal_processor_worker():
    """Background worker that processes signals from queue in batches"""
    logger.info("üöÄ Signal processor worker started")
    batch = []
    last_process_time = time.time()
    
    while True:
        try:
            # Try to get signal with timeout
            signal = signal_queue.get(timeout=BATCH_TIMEOUT)
            batch.append(signal)
            
            # Process batch when full or timeout reached
            if len(batch) >= BATCH_SIZE:
                process_signal_batch(batch)
                batch = []
                last_process_time = time.time()
                
        except Empty:
            # Timeout - process partial batch if any
            if batch and time.time() - last_process_time >= BATCH_TIMEOUT:
                process_signal_batch(batch)
                batch = []
                last_process_time = time.time()
        except Exception as e:
            logger.error(f"Signal processor error: {e}")

# Start background signal processor
signal_processor_thread = threading.Thread(target=signal_processor_worker, daemon=True)
signal_processor_thread.start()

# ============================================================================
# BROKER EXECUTION WORKER - Trade Manager Style (Async, Reliable)
# ============================================================================
def broker_execution_worker(worker_id=0):
    """
    Background worker that processes broker execution queue.
    HIVE MIND: Multiple workers process in parallel for instant execution.

    Trade Manager Style:
    - Webhook records signal instantly
    - Broker execution happens async with retries
    - If broker fails, it retries later (doesn't block webhook)
    - Multiple workers = signals fan out to all accounts INSTANTLY
    """
    logger.info(f"üöÄ Broker execution worker #{worker_id} started (HIVE MIND - instant parallel execution)")
    logger.info(f"   Queue maxsize: {broker_execution_queue.maxsize}")

    while True:
        try:
            # Get next broker execution task (blocking, but that's OK - we're in background)
            task = broker_execution_queue.get(timeout=1)

            recorder_id = task.get('recorder_id')
            action = task.get('action')
            ticker = task.get('ticker')
            quantity = task.get('quantity')
            tp_ticks = task.get('tp_ticks', 10)
            sl_ticks = task.get('sl_ticks', 0)
            break_even_enabled = task.get('break_even_enabled', False)
            break_even_ticks = task.get('break_even_ticks', 10)
            entry_price = task.get('entry_price', 0)
            is_long = task.get('is_long', True)
            risk_config = task.get('risk_config', {})  # NEW: Get risk_config for trailing stop/break-even
            sl_type = task.get('sl_type', 'Fixed')  # NEW: Get sl_type
            queued_at = task.get('queued_at', 0)  # When signal was queued
            signal_price = task.get('signal_price', 0)  # Original signal price
            signal_id = task.get('signal_id', f'sig_broker_{uuid.uuid4().hex[:8]}')  # Pipeline tracking

            # STEP 7: Broker worker picked up task
            track_signal_step(signal_id, 'STEP7_BROKER_WORKER_PICKED', {
                'worker_id': worker_id,
                'action': action,
                'ticker': ticker,
                'queue_remaining': broker_execution_queue.qsize()
            })

            logger.info(f"üîî Worker #{worker_id} received task: {action} {quantity} {ticker} signal={signal_id} (recorder_id={recorder_id})")

            # STALENESS CHECK - Reject signals that are too old (price has moved)
            # This prevents executing stale signals after a processing delay
            SIGNAL_MAX_AGE_SECONDS = 30  # Signals older than this are rejected
            if queued_at > 0:
                signal_age = time.time() - queued_at
                if signal_age > SIGNAL_MAX_AGE_SECONDS:
                    logger.warning(f"‚è∞ STALE SIGNAL REJECTED: {action} {ticker} was {signal_age:.1f}s old (max {SIGNAL_MAX_AGE_SECONDS}s)")
                    logger.warning(f"   Original price: {signal_price}, Signal queued at: {queued_at}")
                    track_signal_step(signal_id, 'STEP7_STALE_REJECTED', {'age_seconds': signal_age})
                    complete_signal(signal_id, 'failed', f'Stale signal ({signal_age:.1f}s old)')
                    _broker_execution_stats['total_failed'] += 1
                    _broker_execution_stats['last_error'] = f'Stale signal rejected ({signal_age:.1f}s old)'
                    broker_execution_queue.task_done()
                    continue
                else:
                    logger.info(f"‚è±Ô∏è Signal age: {signal_age:.2f}s (within {SIGNAL_MAX_AGE_SECONDS}s limit)")

            # NO RETRIES - Try once, if fail log and move on
            # Retries can cause duplicate trades which is dangerous

            try:
                from recorder_service import execute_trade_simple

                # STEP 8: Calling Tradovate API
                track_signal_step(signal_id, 'STEP8_CALLING_BROKER', {
                    'action': action,
                    'quantity': quantity,
                    'ticker': ticker,
                    'tp_ticks': tp_ticks,
                    'sl_ticks': sl_ticks,
                    'sl_type': sl_type,
                    'risk_config': risk_config
                })

                logger.info(f"üì§ Broker execution: {action} {quantity} {ticker} signal={signal_id}")
                logger.info(f"üîß Calling execute_trade_simple: recorder_id={recorder_id}, action={action}, ticker={ticker}, quantity={quantity}")
                if risk_config:
                    logger.info(f"üìä Risk config: {risk_config}")

                result = execute_trade_simple(
                    recorder_id=recorder_id,
                    action=action,
                    ticker=ticker,
                    quantity=quantity,
                    tp_ticks=tp_ticks,
                    sl_ticks=sl_ticks if sl_ticks > 0 else 0,
                    risk_config=risk_config  # NEW: Pass risk_config for trailing stop/break-even
                )

                logger.info(f"üîß execute_trade_simple returned: success={result.get('success')}, error={result.get('error')}, accounts_traded={result.get('accounts_traded', 0)}")

                if result.get('success'):
                    accounts_traded = result.get('accounts_traded', 0)
                    # STEP 9: Trade executed successfully
                    track_signal_step(signal_id, 'STEP9_TRADE_SUCCESS', {
                        'accounts_traded': accounts_traded,
                        'fill_price': result.get('fill_price'),
                        'tp_price': result.get('tp_price')
                    })
                    complete_signal(signal_id, 'complete')
                    logger.info(f"‚úÖ Broker execution successful: {action} {quantity} {ticker} on {accounts_traded} account(s) signal={signal_id}")
                    _broker_execution_stats['total_executed'] += 1
                    _broker_execution_stats['last_execution_time'] = time.time()

                    # Signal blocking: track position after successful entry
                    if task.get('signal_blocking') and action not in ('close', 'flat', 'exit'):
                        try:
                            set_signal_blocking_position(recorder_id, extract_symbol_root(ticker), 'LONG' if task.get('is_long') else 'SHORT')
                        except Exception as sb_err:
                            logger.warning(f"‚ö†Ô∏è Signal blocking SET failed: {sb_err}")

                    # Discord notification for successful trade
                    try:
                        logger.info(f"üîî Discord notification check: DISCORD_NOTIFICATIONS_ENABLED={DISCORD_NOTIFICATIONS_ENABLED}")
                        if DISCORD_NOTIFICATIONS_ENABLED:
                            # Get user_id from recorder
                            conn = get_db_connection()
                            cursor = conn.cursor()
                            if is_using_postgres():
                                cursor.execute('SELECT user_id, name FROM recorders WHERE id = %s', (recorder_id,))
                            else:
                                cursor.execute('SELECT user_id, name FROM recorders WHERE id = ?', (recorder_id,))
                            rec_row = cursor.fetchone()
                            conn.close()
                            logger.info(f"üîî Recorder lookup: recorder_id={recorder_id}, rec_row={rec_row}")
                            if rec_row:
                                rec_user_id = rec_row[0] if isinstance(rec_row, tuple) else rec_row.get('user_id')
                                rec_name = rec_row[1] if isinstance(rec_row, tuple) else rec_row.get('name')
                                logger.info(f"üîî Recorder data: user_id={rec_user_id}, name={rec_name}")
                                logger.info(f"üîî Sending Discord notification for trade: {action} {quantity} {ticker}")
                                notify_trade_execution(
                                    action=action,
                                    symbol=ticker,
                                    quantity=quantity,
                                    price=entry_price if entry_price else 0,
                                    recorder_name=rec_name,
                                    recorder_id=recorder_id  # Notify ALL users linked to this recorder
                                )
                            else:
                                logger.warning(f"‚ö†Ô∏è Recorder {recorder_id} not found in database")
                        else:
                            logger.info(f"üîî Discord notifications disabled (no DISCORD_BOT_TOKEN)")
                    except Exception as notif_err:
                        logger.warning(f"‚ö†Ô∏è Discord notification failed: {notif_err}")
                    
                    # Register break-even monitor if enabled
                    if break_even_enabled and break_even_ticks > 0 and entry_price > 0:
                        try:
                            # Get account info from result for break-even monitoring
                            # Result can have subaccount_id directly or in executed_accounts list
                            subaccount_id = result.get('subaccount_id')
                            account_spec = result.get('account_spec')
                            broker_avg = result.get('broker_avg') or entry_price
                            
                            # If we have subaccount_id directly, use it
                            if subaccount_id:
                                # Get tick size for break-even calculation
                                from recorder_service import get_tick_size
                                tick_size_val = get_tick_size(ticker) if ticker else 0.25
                                
                                register_break_even_monitor(
                                    account_id=int(subaccount_id),
                                    symbol=ticker.upper(),
                                    entry_price=float(broker_avg),  # Use actual fill price from broker
                                    is_long=is_long,
                                    activation_ticks=break_even_ticks,
                                    tick_size=tick_size_val,
                                    sl_order_id=None,  # Will be found by monitor when it moves SL
                                    quantity=quantity,
                                    account_spec=account_spec or str(subaccount_id)
                                )
                                logger.info(f"üìä Break-even monitor registered: {ticker} @ {broker_avg}, trigger={break_even_ticks} ticks")
                            else:
                                # Fallback: try executed_accounts list
                                executed_accounts = result.get('executed_accounts', [])
                                for acct_info in executed_accounts:
                                    acct_id = acct_info.get('subaccount_id') or acct_info.get('account_id')
                                    if acct_id:
                                        from recorder_service import get_tick_size
                                        tick_size_val = get_tick_size(ticker) if ticker else 0.25
                                        broker_avg_val = acct_info.get('broker_avg') or entry_price
                                        
                                        register_break_even_monitor(
                                            account_id=int(acct_id),
                                            symbol=ticker.upper(),
                                            entry_price=float(broker_avg_val),
                                            is_long=is_long,
                                            activation_ticks=break_even_ticks,
                                            tick_size=tick_size_val,
                                            sl_order_id=None,
                                            quantity=quantity,
                                            account_spec=acct_info.get('account_spec') or str(acct_id)
                                        )
                                        logger.info(f"üìä Break-even monitor registered: {ticker} @ {broker_avg_val}, trigger={break_even_ticks} ticks")
                        except Exception as be_err:
                            logger.warning(f"‚ö†Ô∏è Could not register break-even monitor: {be_err}")
                            import traceback
                            logger.warning(traceback.format_exc())
                else:
                    error = result.get('error') or 'Unknown error'
                    logger.error(f"‚ùå Broker execution FAILED: {error}")
                    logger.error(f"   Recorder ID: {recorder_id}, Action: {action}, Quantity: {quantity}, Ticker: {ticker}")
                    logger.error(f"   Full result: {result}")

                    # Enhanced diagnostics for common failures
                    if 'No accounts to trade on' in error or 'No trader linked' in error:
                        logger.error(f"   üîç DIAGNOSTIC: Checking trader configuration for recorder {recorder_id}...")
                        try:
                            conn = get_db_connection()
                            cursor = conn.cursor()
                            is_postgres = is_using_postgres()
                            placeholder = '%s' if is_postgres else '?'
                            enabled_value = 'true' if is_postgres else '1'
                            
                            # Check traders
                            cursor.execute(f'''
                                SELECT t.id, t.enabled, t.enabled_accounts, t.recorder_id
                                FROM traders t
                                WHERE t.recorder_id = {placeholder}
                            ''', (recorder_id,))
                            traders = cursor.fetchall()
                            logger.error(f"   üîç Found {len(traders)} trader(s) linked to recorder {recorder_id}")
                            for trader_row in traders:
                                trader = dict(trader_row) if hasattr(trader_row, 'keys') else {
                                    'id': trader_row[0],
                                    'enabled': trader_row[1],
                                    'enabled_accounts': trader_row[2],
                                    'recorder_id': trader_row[3]
                                }
                                enabled_accts = trader.get('enabled_accounts')
                                enabled_accts_str = str(enabled_accts)[:200] if enabled_accts else 'None'
                                logger.error(f"   üîç Trader {trader.get('id')}: enabled={trader.get('enabled')}, enabled_accounts={enabled_accts_str}")
                            
                            conn.close()
                        except Exception as diag_err:
                            logger.error(f"   ‚ö†Ô∏è Could not run diagnostics: {diag_err}")
                    
                    logger.error(f"   ‚ö†Ô∏è NO RETRY - task abandoned to prevent duplicate trades")
                    # STEP 9: Trade failed
                    track_signal_step(signal_id, 'STEP9_TRADE_FAILED', {'error': error[:200]})
                    complete_signal(signal_id, 'failed', error)
                    _broker_execution_stats['total_failed'] += 1
                    _broker_execution_stats['last_error'] = error

                    # Log failure details for debugging endpoint
                    failed_accts = result.get('failed_accounts', []) if result else []
                    log_broker_failure(recorder_id, action, ticker, error, failed_accts)

                    # Discord notification for failed trade
                    try:
                        if DISCORD_NOTIFICATIONS_ENABLED:
                            conn = get_db_connection()
                            cursor = conn.cursor()
                            if is_using_postgres():
                                cursor.execute('SELECT user_id, name FROM recorders WHERE id = %s', (recorder_id,))
                            else:
                                cursor.execute('SELECT user_id, name FROM recorders WHERE id = ?', (recorder_id,))
                            rec_row = cursor.fetchone()
                            conn.close()
                            if rec_row:
                                rec_user_id = rec_row[0] if isinstance(rec_row, tuple) else rec_row.get('user_id')
                                rec_name = rec_row[1] if isinstance(rec_row, tuple) else rec_row.get('name')
                                if rec_user_id:
                                    notify_error(
                                        user_id=rec_user_id,
                                        error_type="Trade Execution Failed",
                                        error_message=f"{action} {quantity} {ticker} failed",
                                        details=f"Strategy: {rec_name}. Error: {error[:100]}"
                                    )
                    except Exception as notif_err:
                        logger.warning(f"‚ö†Ô∏è Discord error notification failed: {notif_err}")
                        
            except Exception as e:
                logger.error(f"‚ùå Broker execution exception: {e}")
                import traceback
                traceback.print_exc()
                logger.error(f"   ‚ö†Ô∏è NO RETRY - task abandoned to prevent duplicate trades")
                _broker_execution_stats['total_failed'] += 1
                _broker_execution_stats['last_error'] = str(e)
            
            # Mark task as done only on success
            broker_execution_queue.task_done()
            
        except Empty:
            # Timeout - no tasks, continue loop
            # Log health status every 60 seconds
            if int(time.time()) % 60 == 0:
                queue_size = broker_execution_queue.qsize()
                logger.debug(f"üíì Broker execution worker alive: queue_size={queue_size}, stats={_broker_execution_stats}")
            continue
        except Exception as e:
            logger.error(f"Broker execution worker error: {e}")
            import traceback
            traceback.print_exc()
            time.sleep(1)  # Brief pause before retrying

# HIVE MIND: Start multiple broker execution workers for parallel execution
_broker_execution_threads = []

def start_broker_execution_workers():
    """Start multiple broker execution workers for HIVE MIND parallel execution."""
    global _broker_execution_threads
    logger.info(f"üêù HIVE MIND: Starting {_broker_execution_worker_count} parallel broker execution workers...")

    for i in range(_broker_execution_worker_count):
        t = threading.Thread(target=broker_execution_worker, args=(i,), daemon=True, name=f"Broker-Execution-Worker-{i}")
        t.start()
        _broker_execution_threads.append(t)

    # Verify workers started
    time.sleep(0.5)
    alive_count = sum(1 for t in _broker_execution_threads if t.is_alive())
    if alive_count == 0:
        logger.error("‚ùå CRITICAL: No broker execution workers started!")
        logger.error("   This means broker orders will be queued but NEVER executed!")
    else:
        logger.info(f"‚úÖ HIVE MIND ACTIVE: {alive_count}/{_broker_execution_worker_count} broker execution workers running")
        logger.info(f"   Queue maxsize: {broker_execution_queue.maxsize}")
        logger.info(f"   Signals will fan out to ALL accounts INSTANTLY in parallel")

# Start the hive mind workers (skip if external trading engine handles them)
if not _EXTERNAL_ENGINE:
    start_broker_execution_workers()
else:
    logger.info("‚è≠Ô∏è Skipping broker execution workers (external trading engine mode)")

# Track broker execution stats
_broker_execution_stats = {
    'total_queued': 0,
    'total_executed': 0,
    'total_failed': 0,
    'last_execution_time': None,
    'last_error': None
}

# Track recent failed executions for debugging
_broker_failed_executions = []  # List of {timestamp, recorder, action, ticker, error}
_broker_failed_max_size = 50  # Keep last 50 failures

def log_broker_failure(recorder_id, action, ticker, error, failed_accounts=None):
    """Log a broker execution failure for debugging"""
    global _broker_failed_executions
    from datetime import datetime
    entry = {
        'timestamp': datetime.now().isoformat(),
        'recorder_id': recorder_id,
        'action': action,
        'ticker': ticker,
        'error': str(error)[:200],
        'failed_accounts': failed_accounts or []
    }
    _broker_failed_executions.append(entry)
    if len(_broker_failed_executions) > _broker_failed_max_size:
        _broker_failed_executions = _broker_failed_executions[-_broker_failed_max_size:]

def get_broker_failures(limit=20):
    """Get recent broker failures"""
    return list(reversed(_broker_failed_executions[-limit:]))

# Register for thread health monitoring
def restart_broker_execution_worker():
    """Restart all broker execution worker threads (HIVE MIND)"""
    global _broker_execution_threads
    try:
        # Clear old threads and restart all workers
        _broker_execution_threads = []
        for i in range(_broker_execution_worker_count):
            t = threading.Thread(target=broker_execution_worker, args=(i,), daemon=True, name=f"Broker-Execution-Worker-{i}")
            t.start()
            _broker_execution_threads.append(t)
        logger.info(f"üêù HIVE MIND: Restarted {len(_broker_execution_threads)} broker execution workers")
        return _broker_execution_threads
    except Exception as e:
        logger.error(f"Failed to restart broker execution workers: {e}")
        return None

# ============================================================
# WEBHOOK HANDLER - Direct Processing (No Proxy)
# ============================================================
# Webhook processing is done directly here using recorder_service logic.
# This works on Railway (single container) without needing separate service.
# ============================================================

@app.route('/webhook/incoming', methods=['POST'])
def receive_webhook_universal():
    """Universal webhook endpoint - routes by recorder name in body.

    This endpoint allows TradingView to send webhooks with any URL as long as
    the body contains a "recorder" field with the recorder name.

    Example body:
    {"recorder": "JADVIX", "action": "buy", "ticker": "MNQH6", ...}

    Benefits:
    - Works even if TradingView has the wrong webhook token/URL
    - Routes based on recorder name in body
    - Maintains backward compatibility with token-based routing
    """
    logger.info(f"üåê UNIVERSAL WEBHOOK ENDPOINT HIT: /webhook/incoming")

    try:
        raw_body = request.get_data(as_text=True) or ''
        logger.info(f"   POST data: {raw_body[:200]}")

        # Parse the body to get recorder name
        data = {}
        if raw_body:
            try:
                import json
                data = json.loads(raw_body)
            except json.JSONDecodeError:
                # Try parsing as key=value format
                for item in raw_body.replace(';', ',').split(','):
                    if '=' in item:
                        k, v = item.strip().split('=', 1)
                        data[k.strip()] = v.strip()

        recorder_name = data.get('recorder') or data.get('strategy') or data.get('name')
        if not recorder_name:
            logger.warning("‚ö†Ô∏è Universal webhook missing recorder name in body")
            return jsonify({
                'success': False,
                'error': 'Missing recorder name in body. Include "recorder": "YourRecorderName" in webhook payload.'
            }), 400

        logger.info(f"üîç Looking up recorder by name: '{recorder_name}'")

        # Look up recorder by name to get its webhook token
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        # PostgreSQL: rollback any stuck transactions
        if is_postgres:
            try:
                conn.rollback()
            except:
                pass

        cursor.execute(f'SELECT webhook_token FROM recorders WHERE name = {placeholder}', (recorder_name,))
        row = cursor.fetchone()
        conn.close()

        if not row:
            logger.warning(f"‚ö†Ô∏è No recorder found with name: '{recorder_name}'")
            return jsonify({
                'success': False,
                'error': f'No recorder found with name: {recorder_name}'
            }), 404

        webhook_token = row[0] if isinstance(row, tuple) else row['webhook_token']
        logger.info(f"‚úÖ Found recorder '{recorder_name}' -> token: {webhook_token[:8]}...")

        # Now process using the existing logic with the looked-up token
        # Use fast queue if enabled, otherwise process directly
        if _fast_webhook_enabled:
            received_at = time.time()
            log_raw_webhook(webhook_token, raw_body)

            _fast_webhook_queue.put_nowait({
                'token': webhook_token,
                'body': raw_body,
                'received_at': received_at
            })

            queue_size = _fast_webhook_queue.qsize()
            workers_alive = sum(1 for t in _fast_webhook_threads if t.is_alive()) if _fast_webhook_threads else 0
            logger.info(f"‚ö° UNIVERSAL FAST QUEUED: recorder={recorder_name} queue={queue_size}, workers={workers_alive}")

            return jsonify({
                'success': True,
                'queued': True,
                'message': f'Signal for {recorder_name} received and queued',
                'queue_size': queue_size
            }), 200
        else:
            # Process synchronously
            return process_webhook_directly(webhook_token, raw_body_override=raw_body)

    except Exception as e:
        logger.error(f"‚ùå Universal webhook error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/webhook/fast/<webhook_token>', methods=['POST'])
def receive_webhook_fast(webhook_token):
    """Fast webhook endpoint - processes directly."""
    logger.info(f"üåê FAST WEBHOOK ENDPOINT HIT: /webhook/fast/{webhook_token[:8]}...")
    # Validate token exists before processing
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f'SELECT id FROM recorders WHERE webhook_token = {ph}', (webhook_token,))
        if not cursor.fetchone():
            conn.close()
            return jsonify({'success': False, 'error': 'Invalid webhook token'}), 404
        conn.close()
    except Exception as e:
        logger.error(f"Token validation error: {e}")
    logger.info(f"   POST data length: {len(request.get_data()) if request.get_data() else 0} bytes")
    logger.info(f"   POST JSON: {request.get_json(silent=True)}")
    return process_webhook_directly(webhook_token)

@app.route('/webhook/<webhook_token>', methods=['GET', 'POST'])
def receive_webhook(webhook_token):
    """Main webhook endpoint - FAST MODE queues instantly, returns <50ms.
    GET: Returns webhook status (for verification)
    POST: Queues for instant parallel processing"""
    logger.info(f"üåê WEBHOOK ENDPOINT HIT: /webhook/{webhook_token[:8]}... method={request.method}")

    # GET: TradingView or browser verification
    if request.method == 'GET':
        conn = get_db_connection()
        cursor = conn.cursor()
        if is_using_postgres():
            cursor.execute('SELECT id, name FROM recorders WHERE webhook_token = %s', (webhook_token,))
        else:
            cursor.execute('SELECT id, name FROM recorders WHERE webhook_token = ?', (webhook_token,))
        recorder = cursor.fetchone()
        conn.close()

        if recorder:
            return jsonify({
                'status': 'active',
                'message': f'Webhook ready for recorder: {recorder[1]}',
                'recorder_id': recorder[0]
            })
        else:
            return jsonify({'status': 'error', 'message': 'Invalid webhook token'}), 404

    # POST request - FAST MODE: Queue immediately, respond instantly
    if request.method == 'POST':
        logger.info(f"   POST data length: {len(request.get_data()) if request.get_data() else 0} bytes")

        # Validate token exists before queuing to prevent queue flooding
        try:
            _val_conn = get_db_connection()
            _val_cursor = _val_conn.cursor()
            _val_ph = '%s' if is_using_postgres() else '?'
            _val_cursor.execute(f'SELECT id FROM recorders WHERE webhook_token = {_val_ph}', (webhook_token,))
            if not _val_cursor.fetchone():
                _val_conn.close()
                return jsonify({'success': False, 'error': 'Invalid webhook token'}), 404
            _val_conn.close()
        except Exception as _val_err:
            logger.error(f"Token validation error: {_val_err}")

        if _fast_webhook_enabled:
            try:
                raw_body = request.get_data(as_text=True) or ''
                received_at = time.time()

                # Generate unique signal ID for pipeline tracking
                signal_id = f"sig_{uuid.uuid4().hex[:12]}"
                track_signal_step(signal_id, 'STEP1_RECEIVED', {
                    'token': webhook_token[:8],
                    'body_length': len(raw_body),
                    'source': 'endpoint'
                })

                # Log raw webhook immediately for tracking
                log_raw_webhook(webhook_token, raw_body)
                track_signal_step(signal_id, 'STEP2_LOGGED_RAW', {'logged': True})

                # Queue for background processing by 10 parallel workers
                _fast_webhook_queue.put_nowait({
                    'token': webhook_token,
                    'body': raw_body,
                    'received_at': received_at,
                    'signal_id': signal_id  # Track through queue
                })
                track_signal_step(signal_id, 'STEP3_QUEUED', {'queue_size': _fast_webhook_queue.qsize()})

                # Return IMMEDIATELY (<50ms) - TradingView won't timeout
                queue_size = _fast_webhook_queue.qsize()
                workers_alive = sum(1 for t in _fast_webhook_threads if t.is_alive()) if _fast_webhook_threads else 0
                logger.info(f"‚ö° FAST QUEUED: token={webhook_token[:8]}... queue={queue_size}, workers={workers_alive}")

                return jsonify({
                    'success': True,
                    'queued': True,
                    'message': 'Signal received and queued for instant processing',
                    'queue_size': queue_size,
                    'workers_alive': workers_alive
                }), 200
            except Full:
                # Queue full - fall back to synchronous processing
                logger.warning("‚ö†Ô∏è Fast webhook queue full (10000) - processing synchronously")
                return process_webhook_directly(webhook_token)
        else:
            # Synchronous mode (disabled)
            logger.info(f"   POST JSON: {request.get_json(silent=True)}")
            return process_webhook_directly(webhook_token)

    return jsonify({'error': 'Method not allowed'}), 405


def process_webhook_with_data(webhook_token, raw_body_str, signal_id=None):
    """
    Process webhook with raw body data directly (no Flask request needed).
    Called by fast webhook workers. Returns a dict with success/error.
    """
    # Call the main processor with the body override
    try:
        result = process_webhook_directly(webhook_token, raw_body_override=raw_body_str, signal_id=signal_id)
        # result is a Flask Response tuple or Response object
        if hasattr(result, 'get_json'):
            return result.get_json()
        elif isinstance(result, tuple):
            response, status_code = result
            if hasattr(response, 'get_json'):
                return response.get_json()
            return {'success': status_code < 400}
        return {'success': True}
    except Exception as e:
        return {'success': False, 'error': str(e)}


def process_webhook_directly(webhook_token, raw_body_override=None, signal_id=None):
    """
    Process webhook directly using recorder_service DCA logic.
    This is the REAL webhook handler with proper DCA, TP calculation, etc.

    Args:
        webhook_token: The webhook token from URL
        raw_body_override: If provided, use this instead of request.get_data()
                          (used by fast webhook workers that don't have Flask context)

    FULL RISK MANAGEMENT:
    - Direction Filter (long only, short only)
    - Time Filters (trading windows)
    - Signal Cooldown
    - Max Signals Per Session
    - Max Daily Loss
    - Max Contracts Per Trade
    - Signal Delay (Nth signal)
    - Stop Loss
    - Take Profit
    """
    from datetime import datetime, timedelta, timezone
    import logging

    # CRITICAL: Set up logger FIRST, before anything else
    # Use logging module directly - this can NEVER fail
    _logger = logging.getLogger('ultra_simple_server.webhook')
    if not _logger.handlers:
        _logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        _logger.addHandler(handler)

    # Track webhook received time for health monitoring
    global _webhook_last_received, _webhook_last_processed, _webhook_processing_count, _webhook_error_count
    global _webhook_dedup_cache, _webhook_dedup_window, _webhook_dedup_max_size
    webhook_start_time = time.time()
    _webhook_last_received = webhook_start_time

    # Generate signal_id if not provided (for direct calls without fast queue)
    if not signal_id:
        signal_id = f"sig_direct_{uuid.uuid4().hex[:8]}"

    # STEP 5: Processing started
    track_signal_step(signal_id, 'STEP5_PROCESSING_START', {'token': webhook_token[:8]})

    # EARLY LOGGING - Capture ALL incoming webhooks for debugging
    import hashlib
    # Use override if provided (from fast webhook worker), otherwise use Flask request
    if raw_body_override is not None:
        raw_body = raw_body_override
    else:
        raw_body = request.get_data(as_text=True) or ''
    _logger.info(f"üîî RAW WEBHOOK RECEIVED: token={webhook_token[:8]}... signal={signal_id} body={raw_body[:200]}")

    # Log to raw webhook log for API access
    log_raw_webhook(webhook_token, raw_body)

    # DEDUPLICATION CHECK - Prevent processing same webhook twice
    body_hash = hashlib.md5(raw_body.encode()).hexdigest()[:8]
    dedup_key = f"{webhook_token}:{body_hash}"

    # Check if we've seen this exact webhook recently
    if dedup_key in _webhook_dedup_cache:
        last_seen = _webhook_dedup_cache[dedup_key]
        age = webhook_start_time - last_seen
        if age < _webhook_dedup_window:
            _logger.warning(f"‚ö†Ô∏è DUPLICATE WEBHOOK BLOCKED: Same signal received {age:.2f}s ago (token: {webhook_token[:8]}...)")
            # Log blocked webhook for monitoring
            log_webhook_activity(
                recorder_name=f"Token:{webhook_token[:8]}",
                action='Unknown',
                symbol='Unknown',
                status='blocked',
                error=f'Duplicate signal ({age:.2f}s ago)'
            )
            return jsonify({
                'success': False,
                'blocked': True,
                'reason': 'duplicate',
                'message': f'Duplicate signal blocked (same webhook {age:.2f}s ago)'
            }), 200

    # Store this webhook in dedup cache
    _webhook_dedup_cache[dedup_key] = webhook_start_time

    # Cleanup old entries if cache is too large
    if len(_webhook_dedup_cache) > _webhook_dedup_max_size:
        cutoff = webhook_start_time - 60  # Keep last 60 seconds
        _webhook_dedup_cache = {k: v for k, v in _webhook_dedup_cache.items() if v > cutoff}

    # Log that we're starting to process (helps track where signals get lost)
    track_signal_step(signal_id, 'STEP5_DEDUP_PASSED', {'dedup_key': dedup_key})
    _logger.info(f"üöÄ PROCESSING WEBHOOK: token={webhook_token[:8]}... signal={signal_id} (passed dedup check)")

    try:
        # Import helper functions (broker execution is queued, not called here)
        from recorder_service import (
            get_price_from_tradingview_api,
            convert_ticker_to_tradovate,
            get_tick_size,
            get_tick_value
        )

        # Find recorder by webhook token
        _logger.info(f"üì° Getting DB connection for {webhook_token[:8]}...")
        conn = get_db_connection()
        _logger.info(f"‚úÖ DB connection acquired for {webhook_token[:8]}")
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # PostgreSQL: rollback any stuck transactions from previous errors
        if is_postgres:
            try:
                conn.rollback()
            except:
                pass
        
        cursor.execute(f'SELECT * FROM recorders WHERE webhook_token = {placeholder}', (webhook_token,))
        recorder_row = cursor.fetchone()
        
        if not recorder_row:
            _logger.warning(f"Webhook received for unknown token: {webhook_token[:8]}...")
            # Log to activity for monitoring
            log_webhook_activity(
                recorder_name=f"Token:{webhook_token[:8]}",
                action='Unknown',
                symbol='Unknown',
                status='failed',
                error='Invalid webhook token'
            )
            conn.close()
            return jsonify({'success': False, 'error': 'Invalid webhook token'}), 404

        recorder = dict(recorder_row)
        recorder_id = recorder['id']
        recorder_name = recorder['name']
        track_signal_step(signal_id, 'STEP5_RECORDER_FOUND', {'recorder_id': recorder_id, 'name': recorder_name})
        _logger.info(f"‚úÖ Found recorder '{recorder_name}' (ID: {recorder_id}) for {webhook_token[:8]}")

        # Check if recorder is enabled - if disabled, reject the signal
        if not recorder.get('recording_enabled', 1):
            _logger.info(f"‚ö†Ô∏è Webhook BLOCKED for '{recorder_name}' - recorder is DISABLED")
            # Log to activity for monitoring
            log_webhook_activity(
                recorder_name=recorder_name,
                action='Unknown',
                symbol='Unknown',
                status='blocked',
                error='Recorder disabled'
            )
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder is disabled', 'blocked': True}), 200

        # Parse incoming data (use raw_body which was already captured above)
        data = {}
        if raw_body:
            try:
                data = json.loads(raw_body)
            except json.JSONDecodeError:
                pass
        # Fallback to Flask request methods only if raw_body parsing failed and we have Flask context
        if not data and raw_body_override is None:
            try:
                data = request.get_json(force=True, silent=True) or {}
                if not data:
                    data = request.form.to_dict() or {}
            except:
                pass

        if not data:
            # Log to activity for monitoring
            log_webhook_activity(
                recorder_name=recorder_name,
                action='Unknown',
                symbol='Unknown',
                status='failed',
                error='No data received'
            )
            conn.close()
            return jsonify({'success': False, 'error': 'No data received'}), 400
        
        _logger.info(f"üì® Webhook received for recorder '{recorder_name}': {data}")
        track_signal_step(signal_id, 'STEP5_DATA_PARSED', {'data_keys': list(data.keys())[:10]})

        # ============================================================
        # UNIVERSAL MESSAGE PARSING - Handle ANY webhook format
        # Supports: TradingView indicators, strategies, custom formats
        # ============================================================

        # Extract ticker (multiple field names supported)
        ticker = data.get('ticker', data.get('symbol', data.get('instrument', '')))
        
        # Extract price (multiple field names supported)
        price = data.get('price', data.get('close', data.get('last', 0)))
        
        # Extract quantity (multiple field names supported)
        raw_qty = data.get('quantity', data.get('qty', data.get('contracts', data.get('size', 1))))
        quantity = int(raw_qty) if raw_qty else 1
        
        # Extract action from ANY of these fields (priority order)
        action = ''
        for field in ['action', 'order_action', 'side', 'signal', 'direction', 'order_type', 'type']:
            val = data.get(field)
            if val:
                action = str(val).lower().strip()
                break
        
        # Strategy alert fields
        position_size = data.get('position_size', data.get('contracts'))
        market_position = str(data.get('market_position', data.get('position', ''))).lower().strip()
        prev_position_size = data.get('prev_position_size', data.get('prev_market_position_size'))
        is_strategy_alert = position_size is not None or market_position

        # ============================================================
        # NEW WEBHOOK FIELDS - PickMyTrade/TradersPost compatible
        # These override recorder/trader settings when provided
        # ============================================================

        # Dollar-based SL/TP (takes priority over ticks/percent from settings)
        webhook_dollar_sl = data.get('dollar_sl', data.get('sl_dollar', None))
        webhook_dollar_tp = data.get('dollar_tp', data.get('tp_dollar', None))

        # Percentage-based SL/TP
        webhook_percentage_sl = data.get('percentage_sl', data.get('sl_percent', data.get('sl_percentage', None)))
        webhook_percentage_tp = data.get('percentage_tp', data.get('tp_percent', data.get('tp_percentage', None)))

        # Tick-based SL/TP directly from webhook
        webhook_sl_ticks = data.get('sl', data.get('stop_loss', data.get('sl_ticks', None)))
        webhook_tp_ticks = data.get('tp', data.get('take_profit', data.get('tp_ticks', None)))

        # Risk-based position sizing: quantity = risk_amount / (sl_ticks * tick_value)
        webhook_risk_percentage = data.get('risk_percentage', data.get('risk_percent', data.get('risk', None)))
        webhook_risk_dollars = data.get('risk_dollars', data.get('risk_amount', data.get('risk_dollar', None)))

        # Same direction ignore - prevent duplicate signals in same direction
        webhook_same_direction_ignore = data.get('same_direction_ignore', data.get('ignore_same_direction', data.get('no_duplicate', False)))

        # Trailing stop fields
        webhook_trail = data.get('trail', data.get('trailing_stop', None))
        webhook_trail_trigger = data.get('trail_trigger', data.get('trail_start', None))
        webhook_trail_freq = data.get('trail_freq', data.get('trail_frequency', data.get('trail_interval', None)))

        # Breakeven fields
        webhook_breakeven = data.get('breakeven', data.get('break_even', None))
        webhook_breakeven_offset = data.get('breakeven_offset', data.get('be_offset', data.get('break_even_offset', None)))

        # Log webhook overrides if any provided
        webhook_overrides = []
        if webhook_dollar_sl: webhook_overrides.append(f"dollar_sl={webhook_dollar_sl}")
        if webhook_dollar_tp: webhook_overrides.append(f"dollar_tp={webhook_dollar_tp}")
        if webhook_percentage_sl: webhook_overrides.append(f"percentage_sl={webhook_percentage_sl}")
        if webhook_percentage_tp: webhook_overrides.append(f"percentage_tp={webhook_percentage_tp}")
        if webhook_sl_ticks: webhook_overrides.append(f"sl_ticks={webhook_sl_ticks}")
        if webhook_tp_ticks: webhook_overrides.append(f"tp_ticks={webhook_tp_ticks}")
        if webhook_risk_percentage: webhook_overrides.append(f"risk_percentage={webhook_risk_percentage}")
        if webhook_risk_dollars: webhook_overrides.append(f"risk_dollars={webhook_risk_dollars}")
        if webhook_same_direction_ignore: webhook_overrides.append(f"same_direction_ignore=True")
        if webhook_trail: webhook_overrides.append(f"trail={webhook_trail}")
        if webhook_breakeven: webhook_overrides.append(f"breakeven={webhook_breakeven}")

        if webhook_overrides:
            _logger.info(f"üìã Webhook overrides detected: {', '.join(webhook_overrides)}")

        # Convert position_size to float for comparison
        try:
            pos_size = abs(float(position_size)) if position_size is not None else None
        except (ValueError, TypeError):
            pos_size = None
        
        try:
            prev_pos_size = abs(float(prev_position_size)) if prev_position_size is not None else None
        except (ValueError, TypeError):
            prev_pos_size = None
        
        original_action = action
        
        # ============================================================
        # SMART ACTION DETECTION - Priority order
        # ============================================================
        
        # Priority 1: market_position tells us the FINAL state (MOST RELIABLE)
        if market_position == 'flat':
            action = 'close'
            _logger.info(f"üß† PARSE: market_position=flat ‚Üí CLOSE")
        elif market_position == 'long':
            action = 'buy'
            _logger.info(f"üß† PARSE: market_position=long ‚Üí BUY")
        elif market_position == 'short':
            action = 'short'
            _logger.info(f"üß† PARSE: market_position=short ‚Üí SHORT")
        
        # Priority 2: position_size change detection (strategy mode)
        elif pos_size is not None and prev_pos_size is not None:
            if pos_size == 0 and prev_pos_size > 0:
                action = 'close'
                _logger.info(f"üß† PARSE: position {prev_pos_size}‚Üí0 ‚Üí CLOSE")
            elif pos_size > prev_pos_size:
                # Position increased - use action field or default to buy
                if not action or action not in ['buy', 'sell', 'long', 'short']:
                    action = 'buy'
                _logger.info(f"üß† PARSE: position {prev_pos_size}‚Üí{pos_size} ‚Üí {action.upper()}")
            elif pos_size < prev_pos_size and pos_size > 0:
                # Position decreased but not flat - partial close
                action = 'close'
                _logger.info(f"üß† PARSE: position {prev_pos_size}‚Üí{pos_size} (partial) ‚Üí CLOSE")
        
        # Priority 3: position_size alone with no action
        elif pos_size is not None and pos_size == 0 and not action:
            action = 'close'
            _logger.info(f"üß† PARSE: position_size=0 ‚Üí CLOSE")
        
        # Priority 4: Normalize action synonyms
        if action in ['long']:
            action = 'buy'
        elif action in ['sell']:
            # Keep as 'sell' - will be converted based on context later
            pass
        elif action in ['flat', 'exit', 'flatten']:
            action = 'close'
        
        if original_action and original_action != action:
            _logger.info(f"üß† PARSE: Normalized '{original_action}' ‚Üí '{action}'")
        
        # Validate action - allow empty if we couldn't determine from context
        valid_actions = ['buy', 'sell', 'long', 'short', 'close', 'flat', 'exit']
        if not action:
            # If we still have no action, try to use the raw action from TradingView
            # TradingView {{strategy.order.action}} returns "buy" or "sell"
            _logger.warning(f"‚ö†Ô∏è No action determined for {recorder_name}, data: {data}")
            # Log to activity for monitoring
            log_webhook_activity(
                recorder_name=recorder_name,
                action='Unknown',
                symbol=ticker,
                status='failed',
                error='Could not determine action'
            )
            conn.close()
            return jsonify({'success': False, 'error': 'Could not determine action from message'}), 400

        if action not in valid_actions:
            _logger.warning(f"Invalid action '{action}' for recorder {recorder_name}")
            # Log to activity for monitoring
            log_webhook_activity(
                recorder_name=recorder_name,
                action=action,
                symbol=ticker,
                status='failed',
                error=f'Invalid action: {action}'
            )
            conn.close()
            return jsonify({'success': False, 'error': f'Invalid action: {action}'}), 400

        # ============================================================
        # SIGNAL BLOCKING - Instant reject if position tracked in-memory
        # ============================================================
        if recorder.get('signal_blocking') and ticker and action not in ('close', 'flat', 'exit'):
            sb_entry = check_signal_blocking(recorder_id, extract_symbol_root(ticker))
            if sb_entry:
                sb_age = time.time() - sb_entry['set_at']
                _logger.info(f"üõë SIGNAL BLOCKED: {recorder_name} {action} {ticker} ‚Äî position {sb_entry['side']} active for {sb_age:.0f}s")
                log_webhook_activity(
                    recorder_name=recorder_name,
                    action=action,
                    symbol=ticker,
                    status='blocked',
                    error=f"Signal blocking: {sb_entry['side']} position active ({sb_age:.0f}s)"
                )
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f"Signal blocking: {sb_entry['side']} position active"}), 200

        # ============================================================
        # INVERSE SIGNALS - Flip BUY‚ÜîSELL if inverse_signals is enabled
        # ============================================================
        inverse_enabled = recorder.get('inverse_signals', False)
        _logger.info(f"üìä Recorder '{recorder_name}' inverse_signals={inverse_enabled} (type={type(inverse_enabled).__name__})")
        
        # Ensure we handle both boolean True and integer 1
        if inverse_enabled and inverse_enabled not in [0, False, '0', 'false', 'False']:
            original_action_before_inverse = action
            if action in ['buy', 'long']:
                action = 'sell'
                _logger.info(f"üîÑ INVERSE ACTIVE: {original_action_before_inverse.upper()} ‚Üí SELL for '{recorder_name}'")
            elif action in ['sell', 'short']:
                action = 'buy'
                _logger.info(f"üîÑ INVERSE ACTIVE: {original_action_before_inverse.upper()} ‚Üí BUY for '{recorder_name}'")
            # Note: 'close' stays as 'close' - we still want to close positions
        else:
            _logger.info(f"üìä INVERSE NOT ACTIVE for '{recorder_name}' - proceeding with original action: {action}")
        
        # STRATEGY MODE: market_position: flat = CLOSE POSITION
        # SMART CLOSE: Validates close makes sense before executing
        # Handles multiple strategies on same ticker correctly
        if market_position and market_position.lower() == 'flat':
            # Trade Manager Style: Trust the signal, close position in tracker
            # No broker checks - signal is the source of truth
            _logger.info(f"üîÑ FLAT signal for {recorder_name} - closing position based on signal")
            
            # TradingView tells us what to do - trust it
            close_action = action.upper()  # 'BUY' or 'SELL' from TradingView
            close_qty = quantity           # How many contracts TradingView wants to close
            
            # Get current price for closing
            current_price = float(price) if price else 0
            if not current_price:
                root = extract_symbol_root(ticker)
                if root in _market_data_cache:
                    current_price = _market_data_cache[root].get('last')
                if not current_price:
                    current_price = get_price_from_tradingview_api(ticker) or 0
            
            # Close position in tracker (signal-based, instant)
            try:
                trade_conn = get_db_connection()
                trade_cursor = trade_conn.cursor()
                is_pg = is_using_postgres()
                ph = '%s' if is_pg else '?'
                
                # Close recorder_positions
                trade_cursor.execute(f'''
                    SELECT id, side, total_quantity, avg_entry_price FROM recorder_positions
                    WHERE recorder_id = {ph} AND ticker = {ph} AND status = 'open'
                ''', (recorder_id, ticker))
                existing_pos = trade_cursor.fetchone()
                
                if existing_pos:
                    pos_id, pos_side, pos_qty, pos_avg = existing_pos
                    tick_size = get_tick_size(ticker)
                    tick_value = get_tick_value(ticker)
                    
                    # Calculate PnL
                    if pos_side == 'LONG':
                        pos_pnl_ticks = (current_price - pos_avg) / tick_size if tick_size else 0
                    else:
                        pos_pnl_ticks = (pos_avg - current_price) / tick_size if tick_size else 0
                    pos_pnl = pos_pnl_ticks * tick_value * pos_qty
                    
                    # Close position
                    trade_cursor.execute(f'''
                        UPDATE recorder_positions
                        SET status = 'closed', exit_price = {ph}, realized_pnl = {ph}, 
                            exit_time = CURRENT_TIMESTAMP, closed_at = CURRENT_TIMESTAMP
                        WHERE id = {ph}
                    ''', (current_price, pos_pnl, pos_id))
                    
                    _logger.info(f"‚úÖ Position closed in tracker: {pos_side} {pos_qty} @ {pos_avg} ‚Üí {current_price} | PnL: ${pos_pnl:.2f}")
                
                # Close all open recorded_trades
                trade_cursor.execute(f'''
                    UPDATE recorded_trades
                    SET status = 'closed', exit_price = {ph}, exit_time = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                    WHERE recorder_id = {ph} AND ticker = {ph} AND status = 'open'
                ''', (current_price, recorder_id, ticker))
                
                trade_conn.commit()
                trade_conn.close()
                
            except Exception as e:
                _logger.error(f"‚ùå Error closing position in tracker: {e}")
            
            # Queue broker execution (async, non-blocking)
            try:
                broker_task = {
                    'recorder_id': recorder_id,
                    'action': close_action,
                    'ticker': ticker,
                    'quantity': close_qty,
                    'tp_ticks': 0,  # No TP - this is a close
                    'sl_ticks': 0,
                    'retry_count': 0
                }
                broker_execution_queue.put_nowait(broker_task)
                _logger.info(f"üì§ Broker close queued: {close_action} {close_qty} {ticker}")
            except:
                _logger.warning(f"‚ö†Ô∏è Broker execution queue full - signal still tracked")
            
            # Track successful webhook processing
            _webhook_last_processed = time.time()
            _webhook_processing_count += 1
            processing_time = _webhook_last_processed - webhook_start_time
            _logger.info(f"‚úÖ FLAT signal processed in {processing_time:.2f}s")

            # Track paper trade close for FLAT signal
            try:
                track_paper_trade(recorder_id, ticker, 'CLOSE', close_qty, current_price)
            except Exception as paper_err:
                _logger.debug(f"Paper trade tracking error: {paper_err}")

            # Signal blocking: clear position on close/flat signal
            if recorder.get('signal_blocking') and ticker:
                clear_signal_blocking_position(recorder_id, extract_symbol_root(ticker))

            conn.close()
            return jsonify({
                'success': True,
                'action': 'close',
                'side': 'FLAT',
                'quantity': close_qty,
                'exit_price': current_price,
                'broker_queued': True,
                'tracking': 'signal-based',
                'processing_time_ms': int(processing_time * 1000)
            })
        
        # Determine side early (needed for filters)
        if action in ['buy', 'long']:
            side = 'LONG'
            trade_action = 'BUY'
            track_signal_step(signal_id, 'STEP5_ACTION_DETERMINED', {'action': action, 'side': side, 'trade_action': trade_action})
        elif action in ['sell', 'short']:
            side = 'SHORT'
            trade_action = 'SELL'
            track_signal_step(signal_id, 'STEP5_ACTION_DETERMINED', {'action': action, 'side': side, 'trade_action': trade_action})
        elif action in ['close', 'flat', 'exit']:
            # Close signals bypass filters - update open trades to closed
            _logger.info(f"üîÑ CLOSE signal received for {recorder_name}")
            
            # Get current price
            close_price = float(price) if price else 0
            if not close_price:
                close_price = get_price_from_tradingview_api(ticker) or 0
            
            # Close any open trade records in recorded_trades
            is_pg = is_using_postgres()
            ph = '%s' if is_pg else '?'
            
            try:
                cursor.execute(f'''
                    SELECT id, side, entry_price, quantity FROM recorded_trades 
                    WHERE recorder_id = {ph} AND status = 'open'
                ''', (recorder_id,))
                open_trade = cursor.fetchone()
                
                if open_trade:
                    trade_id, trade_side, entry_price, qty = open_trade
                    
                    # Calculate PnL
                    tick_size = get_tick_size(ticker) if ticker else 0.25
                    tick_value = get_tick_value(ticker) if ticker else 0.50
                    
                    if trade_side == 'LONG':
                        pnl_ticks = (close_price - entry_price) / tick_size if tick_size else 0
                    else:
                        pnl_ticks = (entry_price - close_price) / tick_size if tick_size else 0
                    
                    pnl = pnl_ticks * tick_value * qty
                    
                    # Update trade record
                    cursor.execute(f'''
                        UPDATE recorded_trades 
                        SET status = 'closed', exit_price = {ph}, pnl = {ph}, pnl_ticks = {ph},
                            exit_reason = 'signal', exit_time = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                        WHERE id = {ph}
                    ''', (close_price, pnl, pnl_ticks, trade_id))
                    
                    conn.commit()
                    _logger.info(f"üìä Trade #{trade_id} closed: {trade_side} {ticker} @ {entry_price} ‚Üí {close_price} | PnL: ${pnl:.2f}")

                    # Track paper trade close
                    try:
                        track_paper_trade(recorder_id, ticker, 'CLOSE', qty, close_price)
                    except Exception as paper_err:
                        _logger.debug(f"Paper trade tracking error: {paper_err}")
            except Exception as e:
                _logger.warning(f"‚ö†Ô∏è Could not update trade record: {e}")

            # Signal blocking: clear position on close signal
            if recorder.get('signal_blocking') and ticker:
                clear_signal_blocking_position(recorder_id, extract_symbol_root(ticker))

            conn.close()
            return jsonify({'success': True, 'action': 'close', 'message': 'Close signal processed'})
        else:
            _logger.error(f"‚ùå UNKNOWN ACTION: '{action}' - not in buy/long/sell/short/close/flat/exit")
            track_signal_step(signal_id, 'STEP5_UNKNOWN_ACTION', {'action': action})
            conn.close()
            return jsonify({'success': False, 'error': f'Unknown action: {action}'}), 400
        
        # ============================================================
        # üõ°Ô∏è RISK MANAGEMENT FILTERS - Check ALL before executing
        # ============================================================
        track_signal_step(signal_id, 'STEP5_FILTERS_START', {'about_to_check': 'direction,time,cooldown,etc'})

        # Get current time in USER'S timezone (falls back to Chicago if not set)
        _user_tz = CHICAGO_TZ  # default
        _user_tz_name = 'America/Chicago'
        try:
            _rec_user_id = recorder.get('user_id')
            if _rec_user_id:
                cursor.execute(f'SELECT settings_json FROM users WHERE id = {placeholder}', (_rec_user_id,))
                _tz_row = cursor.fetchone()
                if _tz_row:
                    _sj_raw = _tz_row[0] if isinstance(_tz_row, (tuple, list)) else (_tz_row.get('settings_json') if hasattr(_tz_row, 'get') else _tz_row[0])
                    if _sj_raw:
                        _sj = json.loads(_sj_raw) if isinstance(_sj_raw, str) else _sj_raw
                        _user_tz_name = _sj.get('timezone', 'America/Chicago')
                        try:
                            _user_tz = ZoneInfo(_user_tz_name)
                        except Exception:
                            _user_tz = CHICAGO_TZ
                            _user_tz_name = 'America/Chicago'
        except Exception as _tz_err:
            _logger.debug(f"Could not load user timezone: {_tz_err}")
        now = datetime.now(_user_tz)
        _logger.info(f"üïê Time filter using timezone: {_user_tz_name} ‚Üí {now.strftime('%I:%M %p')}")
        
        # --- FILTER 1: Direction Filter ---
        direction_filter = recorder.get('direction_filter', '')
        if direction_filter:
            if direction_filter.lower() == 'long only' and side != 'LONG':
                _logger.warning(f"üö´ [{recorder_name}] Direction filter BLOCKED: {side} signal (filter: Long Only)")
                track_signal_step(signal_id, 'STEP5_BLOCKED_DIRECTION', {'filter': 'Long Only', 'signal_side': side})
                complete_signal(signal_id, 'blocked', f'Direction filter: Long Only (received {side})')
                # Log blocked signal for monitoring
                log_webhook_activity(
                    recorder_name=recorder_name,
                    action=action,
                    symbol=ticker,
                    status='blocked',
                    error=f'Direction filter: Long Only (received {side})'
                )
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Direction filter: Long Only (received {side})'}), 200
            elif direction_filter.lower() == 'short only' and side != 'SHORT':
                _logger.warning(f"üö´ [{recorder_name}] Direction filter BLOCKED: {side} signal (filter: Short Only)")
                track_signal_step(signal_id, 'STEP5_BLOCKED_DIRECTION', {'filter': 'Short Only', 'signal_side': side})
                complete_signal(signal_id, 'blocked', f'Direction filter: Short Only (received {side})')
                # Log blocked signal for monitoring
                log_webhook_activity(
                    recorder_name=recorder_name,
                    action=action,
                    symbol=ticker,
                    status='blocked',
                    error=f'Direction filter: Short Only (received {side})'
                )
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Direction filter: Short Only (received {side})'}), 200
            _logger.info(f"‚úÖ Direction filter passed: {direction_filter}")
        
        # --- FILTER 2: Time Filters (Trading Windows) ---
        def parse_time(time_str):
            """Parse time string like '8:45 AM' or '13:45' to datetime.time"""
            if not time_str:
                return None
            time_str = time_str.strip()
            try:
                # Try 12-hour format first
                if 'AM' in time_str.upper() or 'PM' in time_str.upper():
                    return datetime.strptime(time_str.upper(), '%I:%M %p').time()
                # Try 24-hour format
                return datetime.strptime(time_str, '%H:%M').time()
            except:
                return None
        
        def is_time_in_window(current_time, start_str, stop_str):
            """Check if current time is within the window"""
            start = parse_time(start_str)
            stop = parse_time(stop_str)
            if not start or not stop:
                return True  # No filter if times not set
            current = current_time.time()
            if start <= stop:
                return start <= current <= stop
            else:
                # Overnight window (e.g., 10 PM to 6 AM)
                return current >= start or current <= stop
        
        # Check Time Filter 1 and 2 (only if enabled)
        time_filter_1_enabled = recorder.get('time_filter_1_enabled', False)
        time_filter_1_start = recorder.get('time_filter_1_start', '')
        time_filter_1_stop = recorder.get('time_filter_1_stop', '')
        time_filter_2_enabled = recorder.get('time_filter_2_enabled', False)
        time_filter_2_start = recorder.get('time_filter_2_start', '')
        time_filter_2_stop = recorder.get('time_filter_2_stop', '')
        
        # Time filter is active only if ENABLED and has valid times
        has_time_filter_1 = time_filter_1_enabled and time_filter_1_start and time_filter_1_stop
        has_time_filter_2 = time_filter_2_enabled and time_filter_2_start and time_filter_2_stop
        
        if has_time_filter_1 or has_time_filter_2:
            in_window_1 = is_time_in_window(now, time_filter_1_start, time_filter_1_stop) if has_time_filter_1 else False
            in_window_2 = is_time_in_window(now, time_filter_2_start, time_filter_2_stop) if has_time_filter_2 else False
            
            if not in_window_1 and not in_window_2:
                _logger.warning(f"üö´ [{recorder_name}] Time filter BLOCKED: {now.strftime('%I:%M %p')} not in trading window")
                if has_time_filter_1:
                    _logger.warning(f"   Window 1 (enabled): {time_filter_1_start} - {time_filter_1_stop}")
                if has_time_filter_2:
                    _logger.warning(f"   Window 2 (enabled): {time_filter_2_start} - {time_filter_2_stop}")
                track_signal_step(signal_id, 'STEP5_BLOCKED_TIME', {
                    'current_time': now.strftime('%I:%M %p'),
                    'window1': f"{time_filter_1_start}-{time_filter_1_stop}" if has_time_filter_1 else 'disabled',
                    'window2': f"{time_filter_2_start}-{time_filter_2_stop}" if has_time_filter_2 else 'disabled'
                })
                complete_signal(signal_id, 'blocked', f'Time filter ({now.strftime("%I:%M %p")})')
                # Log to activity so time-blocked signals appear in monitoring
                log_webhook_activity(
                    recorder_name=recorder_name,
                    action=action,
                    symbol=ticker,
                    status='blocked',
                    error=f'Time filter ({now.strftime("%I:%M %p")})'
                )
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Outside trading hours ({now.strftime("%I:%M %p")})'}), 200
            _logger.info(f"‚úÖ Time filter passed: {now.strftime('%I:%M %p')} in window")
        
        # --- FILTER 3: Signal Cooldown ---
        signal_cooldown = int(recorder.get('signal_cooldown', 0) or 0)
        if signal_cooldown > 0:
            if is_postgres:
                cursor.execute(f'''
                    SELECT MAX(created_at) FROM recorded_signals 
                    WHERE recorder_id = %s AND created_at > NOW() - INTERVAL '{signal_cooldown} seconds'
                ''', (recorder_id,))
            else:
                cursor.execute('''
                    SELECT MAX(created_at) FROM recorded_signals 
                    WHERE recorder_id = ? AND created_at > datetime('now', ?)
                ''', (recorder_id, f'-{signal_cooldown} seconds'))
            last_signal = cursor.fetchone()
            if last_signal and last_signal[0]:
                _logger.warning(f"üö´ [{recorder_name}] Cooldown BLOCKED: Last signal was within {signal_cooldown}s")
                track_signal_step(signal_id, 'STEP5_BLOCKED_COOLDOWN', {'cooldown_seconds': signal_cooldown, 'last_signal': str(last_signal[0])})
                complete_signal(signal_id, 'blocked', f'Signal cooldown ({signal_cooldown}s)')
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Signal cooldown ({signal_cooldown}s)'}), 200
            _logger.info(f"‚úÖ Signal cooldown passed: {signal_cooldown}s")
        
        # --- FILTER 4: Max Signals Per Session ---
        max_signals = int(recorder.get('max_signals_per_session', 0) or 0)
        if max_signals > 0:
            # Count signals today
            if is_postgres:
                cursor.execute('''
                    SELECT COUNT(*) FROM recorded_signals 
                    WHERE recorder_id = %s AND DATE(created_at) = CURRENT_DATE
                ''', (recorder_id,))
            else:
                cursor.execute('''
                    SELECT COUNT(*) FROM recorded_signals 
                    WHERE recorder_id = ? AND DATE(created_at) = DATE('now')
                ''', (recorder_id,))
            signal_count = cursor.fetchone()[0] or 0
            if signal_count >= max_signals:
                _logger.warning(f"üö´ [{recorder_name}] Max signals BLOCKED: {signal_count}/{max_signals} signals today")
                track_signal_step(signal_id, 'STEP5_BLOCKED_MAX_SIGNALS', {'count': signal_count, 'limit': max_signals})
                complete_signal(signal_id, 'blocked', f'Max signals reached ({signal_count}/{max_signals})')
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Max signals reached ({signal_count}/{max_signals})'}), 200
            _logger.info(f"‚úÖ Max signals passed: {signal_count}/{max_signals}")
        
        # --- FILTER 5: Max Daily Loss ---
        max_daily_loss = float(recorder.get('max_daily_loss', 0) or 0)
        if max_daily_loss > 0:
            # Calculate today's P&L from closed trades
            if is_postgres:
                cursor.execute('''
                    SELECT COALESCE(SUM(pnl), 0) FROM recorded_trades 
                    WHERE recorder_id = %s AND DATE(exit_time) = CURRENT_DATE AND status = 'closed'
                ''', (recorder_id,))
            else:
                cursor.execute('''
                    SELECT COALESCE(SUM(pnl), 0) FROM recorded_trades 
                    WHERE recorder_id = ? AND DATE(exit_time) = DATE('now') AND status = 'closed'
                ''', (recorder_id,))
            daily_pnl = cursor.fetchone()[0] or 0

            # ENHANCED: Add unrealized P&L from open positions (Feb 23, 2026)
            # Uses recorder_positions (WS-synced) + _market_data_cache (TradingView prices)
            unrealized_total = 0
            try:
                if is_postgres:
                    cursor.execute('''
                        SELECT rp.side, rp.total_quantity, rp.avg_entry_price, rp.ticker
                        FROM recorder_positions rp
                        WHERE rp.recorder_id = %s AND rp.status = 'open' AND rp.total_quantity > 0
                    ''', (recorder_id,))
                else:
                    cursor.execute('''
                        SELECT rp.side, rp.total_quantity, rp.avg_entry_price, rp.ticker
                        FROM recorder_positions rp
                        WHERE rp.recorder_id = ? AND rp.status = 'open' AND rp.total_quantity > 0
                    ''', (recorder_id,))
                open_positions = cursor.fetchall()
                for pos_row in open_positions:
                    pos_side, pos_qty, pos_entry, pos_symbol = pos_row
                    if pos_entry and pos_qty:
                        live_price = _get_live_price_for_symbol(pos_symbol or ticker)
                        if live_price:
                            pos_upnl = _calculate_unrealized_pnl(
                                pos_symbol or ticker, pos_side or 'LONG',
                                float(pos_qty), float(pos_entry), float(live_price)
                            )
                            unrealized_total += pos_upnl
            except Exception as upnl_err:
                _logger.warning(f"‚ö†Ô∏è [{recorder_name}] Unrealized P&L check failed (proceeding with realized only): {upnl_err}")

            combined_pnl = daily_pnl + unrealized_total
            if unrealized_total != 0:
                _logger.info(f"üìä [{recorder_name}] Daily P&L: realized=${daily_pnl:.2f} + unrealized=${unrealized_total:.2f} = ${combined_pnl:.2f}")

            if combined_pnl <= -max_daily_loss:
                _logger.warning(f"üö´ [{recorder_name}] Max daily loss BLOCKED: ${combined_pnl:.2f} (realized=${daily_pnl:.2f} + unrealized=${unrealized_total:.2f}, limit: -${max_daily_loss})")
                track_signal_step(signal_id, 'STEP5_BLOCKED_MAX_DAILY_LOSS', {'daily_pnl': daily_pnl, 'unrealized': unrealized_total, 'combined': combined_pnl, 'limit': max_daily_loss})
                complete_signal(signal_id, 'blocked', f'Max daily loss hit (${combined_pnl:.2f})')
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Max daily loss hit (${combined_pnl:.2f})'}), 200
            _logger.info(f"‚úÖ Max daily loss passed: ${combined_pnl:.2f} / -${max_daily_loss}")
        
        # --- FILTER 6: Max Contracts Per Trade ---
        max_contracts = int(recorder.get('max_contracts_per_trade', 0) or 0)
        if max_contracts > 0 and quantity > max_contracts:
            _logger.info(f"üìä [{recorder_name}] Quantity capped: {quantity} ‚Üí {max_contracts} (max_contracts_per_trade)")
            quantity = max_contracts
        
        # --- FILTER 7: Option Premium Filter (for options strategies) ---
        option_premium_filter = float(recorder.get('option_premium_filter', 0) or 0)
        if option_premium_filter > 0:
            # Check if signal includes premium data
            signal_premium = data.get('premium', 0) or data.get('option_premium', 0) or 0
            try:
                signal_premium = float(signal_premium)
            except:
                signal_premium = 0
            
            if signal_premium > 0 and signal_premium < option_premium_filter:
                _logger.warning(f"üö´ [{recorder_name}] Option premium filter BLOCKED: ${signal_premium} < ${option_premium_filter}")
                track_signal_step(signal_id, 'STEP5_BLOCKED_OPTION_PREMIUM', {'premium': signal_premium, 'minimum': option_premium_filter})
                complete_signal(signal_id, 'blocked', f'Premium ${signal_premium} below minimum ${option_premium_filter}')
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Premium ${signal_premium} below minimum ${option_premium_filter}'}), 200
            elif signal_premium >= option_premium_filter:
                _logger.info(f"‚úÖ Option premium filter passed: ${signal_premium} >= ${option_premium_filter}")
        
        # --- FILTER 8: Auto Flat After Cutoff ---
        auto_flat_after_cutoff = recorder.get('auto_flat_after_cutoff', False)
        if auto_flat_after_cutoff:
            # Check if we're past the last time filter stop time and should flatten
            cutoff_time = None
            if has_time_filter_2 and time_filter_2_stop:
                cutoff_time = parse_time(time_filter_2_stop)
            elif has_time_filter_1 and time_filter_1_stop:
                cutoff_time = parse_time(time_filter_1_stop)
            
            if cutoff_time:
                current_time_obj = now.time()
                # If current time is past cutoff, check for open positions
                if current_time_obj > cutoff_time:
                    # Check for any open positions for this recorder
                    cursor.execute(f'''
                        SELECT id, side, quantity, ticker FROM recorded_trades 
                        WHERE recorder_id = {placeholder} AND status = 'open'
                    ''', (recorder_id,))
                    open_positions = cursor.fetchall()
                    
                    if open_positions:
                        _logger.info(f"‚è∞ [{recorder_name}] Auto-flat triggered: Past cutoff time {time_filter_1_stop or time_filter_2_stop}")
                        # Don't block the signal if it's a close/flatten - allow it through
                        if action.upper() not in ['CLOSE', 'FLATTEN', 'EXIT', 'TP_HIT', 'SL_HIT']:
                            _logger.warning(f"üö´ [{recorder_name}] Auto-flat BLOCKING new entries after cutoff - only exits allowed")
                            track_signal_step(signal_id, 'STEP5_BLOCKED_AUTO_FLAT', {'cutoff': str(cutoff_time), 'action': action})
                            complete_signal(signal_id, 'blocked', 'Past cutoff time - only exits allowed')
                            conn.close()
                            return jsonify({'success': False, 'blocked': True, 'reason': f'Past cutoff time - only exits allowed'}), 200
        
        # --- FILTER 9: Signal Delay (Nth Signal) ---
        add_delay = int(recorder.get('add_delay', 1) or 1)
        if add_delay > 1:
            # Count total signals for this recorder
            cursor.execute(f'SELECT COUNT(*) FROM recorded_signals WHERE recorder_id = {placeholder}', (recorder_id,))
            total_signals = cursor.fetchone()[0] or 0
            signal_number = total_signals + 1  # This will be the Nth signal
            
            if signal_number % add_delay != 0:
                _logger.warning(f"üö´ [{recorder_name}] Signal delay BLOCKED: Signal #{signal_number} (executing every {add_delay})")
                track_signal_step(signal_id, 'STEP5_BLOCKED_SIGNAL_DELAY', {'signal_number': signal_number, 'every_nth': add_delay})
                complete_signal(signal_id, 'blocked', f'Signal delay ({signal_number} mod {add_delay} != 0)')
                # Still record the signal but don't execute
                # For PostgreSQL: created_at has DEFAULT, store quantity in raw_signal as JSON
                if is_postgres:
                    import json as _json
                    raw_signal = _json.dumps({'quantity': quantity})
                    cursor.execute(f'''
                        INSERT INTO recorded_signals (recorder_id, action, ticker, price, raw_signal, processed)
                        VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, false)
                    ''', (recorder_id, action, ticker, price, raw_signal))
                else:
                    timestamp_fn = "datetime('now')"
                    cursor.execute(f'''
                        INSERT INTO recorded_signals (recorder_id, action, ticker, price, created_at, processed)
                        VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {timestamp_fn}, 0)
                    ''', (recorder_id, action, ticker, price))
                conn.commit()
                conn.close()
                return jsonify({'success': False, 'blocked': True, 'reason': f'Signal delay ({signal_number} mod {add_delay} != 0)'}), 200
            _logger.info(f"‚úÖ Signal delay passed: #{signal_number} (every {add_delay})")

        # All filters passed!
        track_signal_step(signal_id, 'STEP5_FILTERS_PASSED', {'action': trade_action, 'ticker': ticker})

        # ============================================================
        # üìä RECORD THE SIGNAL (after filters pass)
        # ============================================================
        track_signal_step(signal_id, 'STEP5A_RECORDING_SIGNAL', {'recorder_id': recorder_id})
        try:
            # For PostgreSQL: created_at has DEFAULT, store quantity in raw_signal as JSON
            if is_postgres:
                import json as _json
                raw_signal = _json.dumps({'quantity': quantity, 'executed': True})
                cursor.execute(f'''
                    INSERT INTO recorded_signals (recorder_id, action, ticker, price, raw_signal, processed)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, true)
                ''', (recorder_id, action, ticker, price, raw_signal))
            else:
                timestamp_fn = "datetime('now')"
                cursor.execute(f'''
                    INSERT INTO recorded_signals (recorder_id, action, ticker, price, created_at, processed)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {timestamp_fn}, 1)
                ''', (recorder_id, action, ticker, price))
            conn.commit()
        except Exception as e:
            _logger.warning(f"Could not record signal: {e}")

        # ============================================================
        # üìÑ PAPER TRADING: Record trade for analytics (BACKGROUND - never blocks broker execution)
        # ============================================================
        paper_action = action.upper()
        if paper_action in ['BUY', 'SELL', 'LONG', 'SHORT', 'CLOSE', 'FLAT', 'EXIT']:
            # Apply paper-specific filters (mirrors broker-side filters)
            _paper_ok, _paper_reason = _paper_should_execute_signal(recorder_id, paper_action, recorder)
            if not _paper_ok:
                _logger.info(f"üìù Paper trade FILTERED: {paper_action} {ticker} ‚Äî {_paper_reason}")
            else:
                # Use recorder position sizes (matches broker behavior) instead of raw webhook qty
                if paper_action in ['CLOSE', 'FLAT', 'EXIT']:
                    paper_qty = int(quantity) if quantity else 1
                else:
                    # Determine if this is a DCA add by checking for open paper position
                    _paper_is_dca = False
                    try:
                        _paper_side = 'LONG' if paper_action in ['BUY', 'LONG'] else 'SHORT'
                        import os as _pp_os
                        _pp_db_url = _pp_os.environ.get('DATABASE_URL')
                        if _pp_db_url:
                            import psycopg2 as _pp_pg
                            _pp_conn = _pp_pg.connect(_pp_db_url)
                            _pp_ph = '%s'
                        else:
                            _pp_conn = sqlite3.connect('paper_trades.db')
                            _pp_ph = '?'
                        try:
                            _pp_cursor = _pp_conn.cursor()
                            _pp_cursor.execute(f'''
                                SELECT id FROM paper_trades
                                WHERE recorder_id = {_pp_ph} AND symbol = {_pp_ph} AND side = {_pp_ph} AND status = 'open'
                                LIMIT 1
                            ''', (recorder_id, extract_symbol_root(ticker) if ticker else ticker, _paper_side))
                            if _pp_cursor.fetchone():
                                _paper_is_dca = True
                        finally:
                            _pp_conn.close()
                    except Exception as _pq_err:
                        _logger.debug(f"Paper DCA check error: {_pq_err}")

                    if _paper_is_dca:
                        paper_qty = int(recorder.get('add_position_size') or 1)
                    else:
                        paper_qty = int(recorder.get('initial_position_size') or 1)
                    # Apply trader multiplier to paper qty (matches real execution at recorder_service.py:2032)
                    _paper_multiplier = 1.0
                    try:
                        _pp_m_db_url = os.environ.get('DATABASE_URL')
                        if _pp_m_db_url:
                            import psycopg2 as _pp_m_pg
                            _pp_m_conn = _pp_m_pg.connect(_pp_m_db_url)
                            _pp_m_ph = '%s'
                        else:
                            _pp_m_conn = sqlite3.connect('just_trades.db')
                            _pp_m_ph = '?'
                        try:
                            _pp_m_cursor = _pp_m_conn.cursor()
                            _pp_m_cursor.execute(f'''
                                SELECT multiplier FROM traders
                                WHERE recorder_id = {_pp_m_ph} AND enabled = {'TRUE' if is_postgres else '1'}
                                ORDER BY id LIMIT 1
                            ''', (recorder_id,))
                            _pp_m_row = _pp_m_cursor.fetchone()
                            if _pp_m_row and _pp_m_row[0]:
                                _paper_multiplier = float(_pp_m_row[0])
                        finally:
                            _pp_m_conn.close()
                    except Exception as _pp_m_err:
                        _logger.debug(f"Paper multiplier lookup error: {_pp_m_err}")
                    if _paper_multiplier != 1.0:
                        paper_qty = max(1, int(paper_qty * _paper_multiplier))
                    _logger.info(f"üìù Paper qty: {paper_qty} ({'DCA add' if _paper_is_dca else 'initial'}, multiplier={_paper_multiplier}x)")

                paper_price = float(price) if price else None
                import threading
                def _bg_paper_trade(rec_id, sym, act, qty, px, sig_t):
                    try:
                        result = record_paper_trade_from_webhook(
                            recorder_id=rec_id, symbol=sym, action=act, quantity=qty, price=px,
                            signal_time=sig_t
                        )
                        print(f"üß™ PAPER TRADE (bg): {act} {qty} {sym} ‚Üí {result}", flush=True)
                        if result:
                            _paper_update_filter_tracking(rec_id)
                    except Exception as e:
                        print(f"üß™ PAPER TRADE ERROR (bg): {e}", flush=True)
                threading.Thread(
                    target=_bg_paper_trade,
                    args=(recorder_id, ticker, paper_action, paper_qty, paper_price, time.time()),
                    daemon=True
                ).start()

        # ============================================================
        # üìà GET RISK SETTINGS
        # ============================================================

        # Trade Manager Style: NO broker sync - position tracking is signal-based only
        # Signals are the source of truth. Broker execution happens async.
        # This makes webhook processing instant and never fails.
        
        # Use LIVE TradingView price for entry (so P&L starts at ~$0)
        # Priority: 1) Live WebSocket price, 2) Cached price, 3) Signal price (fallback)
        signal_price = float(price) if price else 0
        
        # Try to get live price from TradingView WebSocket cache
        live_price = None
        ticker_root = extract_symbol_root(ticker) if ticker else None
        if ticker_root and ticker_root in _market_data_cache:
            live_price = _market_data_cache[ticker_root].get('last')
        
        # Fallback to cached price function
        if not live_price:
            live_price = get_cached_price(ticker) if ticker else None
        
        # Use LIVE price if available (P&L starts at $0), otherwise fallback to signal price
        current_price = live_price if live_price else signal_price
        _logger.info(f"üìä Entry price: {current_price} (live={live_price}, signal={signal_price})")
        
        # Get tick size for this symbol
        tick_size = get_tick_size(ticker) if ticker else 0.25
        
        # Get TP/SL settings - TRADER settings override RECORDER settings
        # We'll get trader settings after loading the trader object below
        # For now, get recorder defaults (will be overridden if trader has settings)
        tp_targets_raw = recorder.get('tp_targets', '[]')
        tp_units = recorder.get('tp_units', 'Ticks')
        trim_units = recorder.get('trim_units', 'Percent')
        sl_enabled = recorder.get('sl_enabled', 0)
        sl_amount = float(recorder.get('sl_amount', 0) or 0)
        sl_units = recorder.get('sl_units', 'Ticks')
        sl_type = recorder.get('sl_type', 'Fixed')
        break_even_enabled = recorder.get('break_even_enabled', 0)
        break_even_ticks = int(recorder.get('break_even_ticks', 10) or 10)
        break_even_offset = int(recorder.get('break_even_offset', 0) or 0)
        trail_trigger = int(recorder.get('trail_trigger', 0) or 0)
        trail_freq = int(recorder.get('trail_freq', 0) or 0)
        same_direction_ignore = recorder.get('same_direction_ignore', 0)

        # Get linked trader for live execution with ALL risk settings
        # Also get trader's sl_type if available (trader settings override recorder)
        trader_sl_type = None
        # PostgreSQL uses TRUE (boolean), SQLite uses 1 (integer)
        # CRITICAL: Include all trader risk settings (initial_position_size, add_position_size, tp_targets, sl_*, etc.)
        track_signal_step(signal_id, 'STEP5B_LOOKING_FOR_TRADER', {'recorder_id': recorder_id, 'recorder_name': recorder_name})
        _logger.info(f"üîç Looking for trader linked to recorder_id={recorder_id} (recorder_name='{recorder_name}')")
        
        if is_postgres:
            cursor.execute(f'''
                SELECT t.*,
                       t.initial_position_size, t.add_position_size,
                       t.tp_targets,
                       t.sl_enabled, t.sl_amount, t.sl_units,
                       a.tradovate_token, a.md_access_token, a.username, a.password, a.id as account_id
                FROM traders t
                JOIN accounts a ON t.account_id = a.id
                WHERE t.recorder_id = {placeholder} AND t.enabled = TRUE
                ORDER BY t.id
                LIMIT 1
            ''', (recorder_id,))
        else:
            cursor.execute(f'''
                SELECT t.*,
                       t.initial_position_size, t.add_position_size,
                       t.tp_targets,
                       t.sl_enabled, t.sl_amount, t.sl_units,
                       a.tradovate_token, a.md_access_token, a.username, a.password, a.id as account_id
                FROM traders t
                JOIN accounts a ON t.account_id = a.id
                WHERE t.recorder_id = {placeholder} AND t.enabled = 1
                ORDER BY t.id
                LIMIT 1
            ''', (recorder_id,))
        trader_row = cursor.fetchone()
        trader = dict(trader_row) if trader_row else None
        
        if not trader:
            # Check if ANY traders exist for this recorder (even disabled ones)
            cursor.execute(f'SELECT COUNT(*) as count FROM traders WHERE recorder_id = {placeholder}', (recorder_id,))
            trader_count = cursor.fetchone()[0] if cursor.rowcount > 0 else 0
            cursor.execute(f'SELECT COUNT(*) as count FROM traders WHERE recorder_id = {placeholder} AND enabled = {1 if not is_postgres else "TRUE"}', (recorder_id,))
            enabled_count = cursor.fetchone()[0] if cursor.rowcount > 0 else 0

            track_signal_step(signal_id, 'STEP5C_NO_TRADER_FOUND', {'total': trader_count, 'enabled': enabled_count})
            complete_signal(signal_id, 'failed', f'No trader linked (total: {trader_count}, enabled: {enabled_count})')
            _logger.error(f"‚ùå No active trader linked to recorder '{recorder_name}' (recorder_id={recorder_id})")
            _logger.error(f"   Total traders for this recorder: {trader_count}")
            _logger.error(f"   Enabled traders for this recorder: {enabled_count}")
            _logger.error(f"   ACTION REQUIRED: Go to /traders and create/link a trader to recorder '{recorder_name}'")
            # Log to activity so signal appears in monitoring (CRITICAL for debugging)
            log_webhook_activity(
                recorder_name=recorder_name,
                action=action if 'action' in dir() else 'Unknown',
                symbol=ticker if 'ticker' in dir() else 'Unknown',
                status='blocked',
                error=f'No trader linked (total: {trader_count}, enabled: {enabled_count})'
            )
            conn.close()
            return jsonify({
                'success': False,
                'error': 'No trader linked',
                'recorder_id': recorder_id,
                'recorder_name': recorder_name,
                'total_traders': trader_count,
                'enabled_traders': enabled_count
            }), 400
        
        track_signal_step(signal_id, 'STEP5D_TRADER_FOUND', {'trader_id': trader.get('id'), 'account_id': trader.get('account_id')})
        _logger.info(f"‚úÖ Found trader: id={trader.get('id')}, account_id={trader.get('account_id')}, enabled_accounts={bool(trader.get('enabled_accounts'))}")
        
        # CRITICAL: Use TRADER's risk settings to override recorder defaults
        # A trader setting should ONLY override the recorder when the trader
        # has a REAL value ‚Äî not a DB default like '[]', 0, or NULL.
        # Helper: check if a trader field has a meaningful (non-default) value
        def _has_real_value(val, empty_vals=(None, '', '[]', '{}', 0, 0.0, False)):
            """Return True only if val is explicitly set and not a DB default."""
            return val is not None and val not in empty_vals

        def _parse_tp_targets(raw):
            """Parse tp_targets to a list, returns [] if empty/invalid."""
            if not raw:
                return []
            try:
                parsed = json.loads(raw) if isinstance(raw, str) else raw
                return parsed if isinstance(parsed, list) else []
            except (json.JSONDecodeError, TypeError):
                return []

        # --- TP Targets: only override if trader has real, non-empty targets ---
        trader_tp_targets = trader.get('tp_targets')
        parsed_trader_tp = _parse_tp_targets(trader_tp_targets)
        if parsed_trader_tp and len(parsed_trader_tp) > 0:
            tp_targets_raw = trader_tp_targets
            _logger.info(f"üìä Using TRADER's TP targets: {trader_tp_targets}")
        else:
            _logger.info(f"üìä Using RECORDER TP targets (trader has none): {tp_targets_raw}")

        # --- TP/Trim units: only override if trader has a non-empty string ---
        trader_trim_units = trader.get('trim_units')
        if trader_trim_units and trader_trim_units.strip():
            trim_units = trader_trim_units
        trader_tp_units = trader.get('tp_units')
        if trader_tp_units and trader_tp_units.strip():
            tp_units = trader_tp_units

        # --- SL settings: override if trader has explicitly different values ---
        # sl_enabled is a boolean toggle ‚Äî None means "use recorder", 0/False means "explicitly off"
        trader_sl_enabled = trader.get('sl_enabled')
        if trader_sl_enabled is not None:
            sl_enabled = trader_sl_enabled
        trader_sl_amount = trader.get('sl_amount')
        if trader_sl_amount is not None and float(trader_sl_amount or 0) > 0:
            sl_amount = float(trader_sl_amount)
            _logger.info(f"üìä Using TRADER's SL amount: {sl_amount}")
        trader_sl_units = trader.get('sl_units')
        if trader_sl_units and trader_sl_units.strip():
            sl_units = trader_sl_units

        # --- SL Type: only override if trader has an explicit NON-DEFAULT type ---
        # 'Fixed' is the DB default on traders ‚Äî it doesn't mean the user chose Fixed.
        # Only override if the trader explicitly set a non-default value (e.g. 'Trail').
        trader_sl_type = trader.get('sl_type') if trader else None
        if trader_sl_type and trader_sl_type.strip() and trader_sl_type != 'Fixed':
            sl_type = trader_sl_type
            _logger.info(f"üìä Using TRADER's sl_type: {sl_type}")

        # --- Break-even: only override if trader has real values ---
        trader_break_even_enabled = trader.get('break_even_enabled') if trader else None
        if trader_break_even_enabled is not None:
            break_even_enabled = trader_break_even_enabled
        trader_break_even_ticks = trader.get('break_even_ticks') if trader else None
        if trader_break_even_ticks is not None and int(trader_break_even_ticks or 0) > 0:
            break_even_ticks = int(trader_break_even_ticks)
        trader_break_even_offset = trader.get('break_even_offset') if trader else None
        if trader_break_even_offset is not None and int(trader_break_even_offset or 0) > 0:
            break_even_offset = int(trader_break_even_offset)

        # --- Trail settings: only override if trader has real (>0) values ---
        trader_trail_trigger = trader.get('trail_trigger') if trader else None
        if trader_trail_trigger is not None and int(trader_trail_trigger or 0) > 0:
            trail_trigger = int(trader_trail_trigger)
        trader_trail_freq = trader.get('trail_freq') if trader else None
        if trader_trail_freq is not None and int(trader_trail_freq or 0) > 0:
            trail_freq = int(trader_trail_freq)

        # --- Position sizes: extract for use in quantity logic below ---
        trader_initial_size = trader.get('initial_position_size') if trader else None
        trader_add_size = trader.get('add_position_size') if trader else None

        # Log final resolved settings
        signal_type = "STRATEGY" if is_strategy_alert else "INDICATOR"
        _logger.info(f"üìä {signal_type} SIGNAL resolved: tp={tp_targets_raw}, sl_enabled={sl_enabled}, sl_amount={sl_amount}, sl_type={sl_type}, be={break_even_enabled}/{break_even_ticks}, trail={trail_trigger}/{trail_freq}")

        # ============================================================
        # TP/SL CALCULATION - Priority: Webhook > Trader > Recorder
        # ============================================================
        tick_value = get_tick_value(ticker) if ticker else 0.50

        # Parse TP targets (from trader/recorder settings as fallback)
        try:
            tp_targets = json.loads(tp_targets_raw) if isinstance(tp_targets_raw, str) else tp_targets_raw or []
        except:
            tp_targets = []

        # Get TP value from settings (fallback)
        # tp_targets can use 'ticks' or 'value' key depending on how it was saved
        if tp_targets and len(tp_targets) > 0:
            tp_val = tp_targets[0].get('ticks')  # Try 'ticks' first (newer format)
            if tp_val is None:
                tp_val = tp_targets[0].get('value')  # Fallback to 'value' (older format)
            tp_value = float(tp_val) if tp_val is not None else 0
        else:
            tp_value = 0

        # ============================================================
        # WEBHOOK TP OVERRIDE - Highest priority
        # Priority: webhook_tp_ticks > webhook_dollar_tp > webhook_percentage_tp > settings
        # ============================================================
        tp_ticks = 0
        tp_source = "NONE"

        if webhook_tp_ticks is not None:
            # Direct tick value from webhook - use as-is
            tp_ticks = int(float(webhook_tp_ticks))
            tp_source = "WEBHOOK (ticks)"
            _logger.info(f"üìä TP from WEBHOOK ticks: {tp_ticks} ticks")

        elif webhook_dollar_tp is not None:
            # Dollar amount per contract - convert to ticks
            dollar_tp = float(webhook_dollar_tp)
            if tick_value > 0:
                tp_ticks = int(dollar_tp / tick_value)
                tp_source = "WEBHOOK (dollar)"
                _logger.info(f"üìä TP from WEBHOOK dollar: ${dollar_tp} ‚Üí {tp_ticks} ticks (tick_value={tick_value})")

        elif webhook_percentage_tp is not None:
            # Percentage of entry price - convert to ticks
            pct_tp = float(webhook_percentage_tp)
            if current_price and tick_size:
                tp_ticks = int((current_price * (pct_tp / 100)) / tick_size)
                tp_source = "WEBHOOK (percent)"
                _logger.info(f"üìä TP from WEBHOOK percent: {pct_tp}% ‚Üí {tp_ticks} ticks (price={current_price})")

        elif tp_value > 0:
            # Fallback to trader/recorder settings
            if tp_units == 'Points':
                # Points = dollar value per contract. Convert to ticks.
                tp_ticks = int(tp_value / tick_value) if tick_value else int(tp_value / tick_size)
                tp_source = "SETTINGS (points)"
            elif tp_units == 'Percent':
                # Percent of entry price
                tp_ticks = int((current_price * (tp_value / 100)) / tick_size) if current_price and tick_size else 0
                tp_source = "SETTINGS (percent)"
            else:
                # Ticks (default)
                tp_ticks = int(tp_value)
                tp_source = "SETTINGS (ticks)"

        # ============================================================
        # WEBHOOK SL OVERRIDE - Highest priority
        # Priority: webhook_sl_ticks > webhook_dollar_sl > webhook_percentage_sl > settings
        # ============================================================
        sl_ticks = 0
        sl_source = "NONE"

        if webhook_sl_ticks is not None:
            # Direct tick value from webhook - use as-is
            sl_ticks = int(float(webhook_sl_ticks))
            sl_source = "WEBHOOK (ticks)"
            _logger.info(f"üìä SL from WEBHOOK ticks: {sl_ticks} ticks")

        elif webhook_dollar_sl is not None:
            # Dollar amount per contract - convert to ticks
            dollar_sl = float(webhook_dollar_sl)
            if tick_value > 0:
                sl_ticks = int(dollar_sl / tick_value)
                sl_source = "WEBHOOK (dollar)"
                _logger.info(f"üìä SL from WEBHOOK dollar: ${dollar_sl} ‚Üí {sl_ticks} ticks (tick_value={tick_value})")

        elif webhook_percentage_sl is not None:
            # Percentage of entry price - convert to ticks
            pct_sl = float(webhook_percentage_sl)
            if current_price and tick_size:
                sl_ticks = int((current_price * (pct_sl / 100)) / tick_size)
                sl_source = "WEBHOOK (percent)"
                _logger.info(f"üìä SL from WEBHOOK percent: {pct_sl}% ‚Üí {sl_ticks} ticks (price={current_price})")

        elif sl_enabled and sl_amount > 0:
            # Fallback to trader/recorder settings
            if sl_units == 'Points':
                # Points = dollar value per contract. Convert to ticks.
                sl_ticks = int(sl_amount / tick_value) if tick_value else int(sl_amount / tick_size)
                sl_source = "SETTINGS (points)"
            elif sl_units == 'Percent':
                # Percent of entry price
                sl_ticks = int((current_price * (sl_amount / 100)) / tick_size) if current_price and tick_size else 0
                sl_source = "SETTINGS (percent)"
            else:
                # Ticks (default)
                sl_ticks = int(sl_amount)
                sl_source = "SETTINGS (ticks)"

        _logger.info(f"üìä Final TP: {tp_ticks} ticks ({tp_source}) | SL: {sl_ticks} ticks ({sl_source})")
        
        # ============================================================
        # POSITION SIZING - Priority: Webhook risk > Webhook qty > Trader > Recorder
        # ============================================================

        # Check for existing open position first (needed for same_direction_ignore and DCA)
        cursor.execute(f'''
            SELECT * FROM recorded_trades
            WHERE recorder_id = {placeholder} AND status = 'open'
            ORDER BY entry_time DESC LIMIT 1
        ''', (recorder_id,))
        existing_position_row = cursor.fetchone()
        existing_position = None
        existing_side = None
        if existing_position_row:
            existing_position = dict(zip([desc[0] for desc in cursor.description], existing_position_row))
            existing_side = existing_position.get('side', '')

        # ============================================================
        # SAME DIRECTION IGNORE - Block duplicate signals
        # Priority: Webhook setting > Recorder setting
        # ============================================================
        # Use webhook setting if provided, otherwise use recorder setting
        effective_same_direction_ignore = webhook_same_direction_ignore if webhook_same_direction_ignore else same_direction_ignore
        if effective_same_direction_ignore and existing_position:
            is_same_direction = (
                (existing_side == 'LONG' and trade_action.upper() in ['BUY', 'LONG']) or
                (existing_side == 'SHORT' and trade_action.upper() in ['SELL', 'SHORT'])
            )
            if is_same_direction:
                _logger.info(f"‚è≠Ô∏è BLOCKED: same_direction_ignore=True and already have {existing_side} position (trade #{existing_position['id']})")
                # Log to activity so blocked signals appear in monitoring
                log_webhook_activity(
                    recorder_name=recorder_name,
                    action=trade_action,
                    symbol=ticker,
                    status='blocked',
                    error=f'same_direction_ignore (already {existing_side})'
                )
                conn.close()
                return jsonify({
                    'success': True,
                    'blocked': True,
                    'reason': 'same_direction_ignore',
                    'existing_side': existing_side,
                    'existing_trade_id': existing_position['id'],
                    'message': f'Signal ignored - already have {existing_side} position'
                }), 200

        # ============================================================
        # RISK-BASED POSITION SIZING
        # Formula: quantity = risk_amount / (sl_ticks * tick_value)
        # ============================================================
        webhook_provided_qty = any(k in data for k in ('quantity', 'qty', 'contracts', 'size'))
        quantity_source = "WEBHOOK" if webhook_provided_qty else "DEFAULT"

        if webhook_risk_dollars is not None and sl_ticks > 0:
            # Direct risk dollar amount
            risk_amount = float(webhook_risk_dollars)
            calculated_qty = int(risk_amount / (sl_ticks * tick_value)) if (sl_ticks * tick_value) > 0 else 1
            quantity = max(1, calculated_qty)
            quantity_source = f"RISK_DOLLARS (${risk_amount} / ({sl_ticks} ticks * ${tick_value}) = {quantity})"
            _logger.info(f"üìä Quantity from WEBHOOK risk_dollars: {quantity} ({quantity_source})")

        elif webhook_risk_percentage is not None and sl_ticks > 0:
            # Risk percentage of account - need to query account balance
            # For now, we'll need to get the trader's account balance
            try:
                account_id = trader.get('account_id')
                cursor.execute(f'SELECT balance FROM accounts WHERE id = {placeholder}', (account_id,))
                balance_row = cursor.fetchone()
                account_balance = float(balance_row[0]) if balance_row and balance_row[0] else 10000.0  # Default $10k

                risk_pct = float(webhook_risk_percentage)
                risk_amount = account_balance * (risk_pct / 100)
                calculated_qty = int(risk_amount / (sl_ticks * tick_value)) if (sl_ticks * tick_value) > 0 else 1
                quantity = max(1, calculated_qty)
                quantity_source = f"RISK_PCT ({risk_pct}% of ${account_balance:,.0f} = ${risk_amount:,.0f} ‚Üí {quantity} contracts)"
                _logger.info(f"üìä Quantity from WEBHOOK risk_percentage: {quantity} ({quantity_source})")
            except Exception as e:
                _logger.warning(f"‚ö†Ô∏è Could not calculate risk-based quantity: {e}. Using webhook/settings quantity.")

        elif not webhook_provided_qty and not position_size:
            # No webhook quantity specified - use trader/recorder settings
            if trader_initial_size is not None and int(trader_initial_size) > 0:
                quantity = int(trader_initial_size)
                quantity_source = "TRADER (initial_position_size)"
                _logger.info(f"üìä Using TRADER's initial_position_size: {quantity}")
            else:
                # Fallback to recorder setting
                recorder_initial_size = recorder.get('initial_position_size', 0)
                if recorder_initial_size is not None and int(recorder_initial_size) > 0:
                    quantity = int(recorder_initial_size)
                    quantity_source = "RECORDER (initial_position_size)"
                    _logger.info(f"üìä Using RECORDER's initial_position_size: {quantity}")
                else:
                    quantity_source = "WEBHOOK (default ‚Äî initial_position_size=0)"
                    _logger.info(f"üìä initial_position_size=0, keeping webhook quantity: {quantity}")

        # ============================================================
        # DCA DETECTION - Only adjust quantity if DCA is actually enabled
        # ============================================================
        is_dca = False
        if existing_position:
            if (existing_side == 'LONG' and trade_action.upper() == 'BUY') or \
               (existing_side == 'SHORT' and trade_action.upper() == 'SELL'):
                # Check if DCA is enabled: trader.dca_enabled takes precedence, then recorder.avg_down_enabled
                trader_dca_raw = trader.get('dca_enabled') if trader else None
                recorder_dca = recorder.get('avg_down_enabled', False)
                effective_dca = bool(trader_dca_raw) if trader_dca_raw is not None else bool(recorder_dca)
                if effective_dca:
                    is_dca = True
                    # Use add_position_size for DCA (unless risk-based sizing was used)
                    if 'RISK' not in quantity_source:
                        if trader_add_size is not None and int(trader_add_size) > 0:
                            quantity = int(trader_add_size)
                            quantity_source = "TRADER (add_position_size - DCA)"
                            _logger.info(f"üìà DCA detected - Using TRADER's add_position_size: {quantity}")
                        else:
                            recorder_add_size = recorder.get('add_position_size', 0)
                            if recorder_add_size is not None and int(recorder_add_size) > 0:
                                quantity = int(recorder_add_size)
                                quantity_source = "RECORDER (add_position_size - DCA)"
                                _logger.info(f"üìà DCA detected - Using RECORDER's add_position_size: {quantity}")
                            else:
                                quantity_source = "WEBHOOK (default ‚Äî add_position_size=0)"
                                _logger.info(f"üìà DCA detected - add_position_size=0, keeping webhook quantity: {quantity}")
                    else:
                        _logger.info(f"üìà DCA detected - Keeping risk-based quantity: {quantity}")
                else:
                    _logger.info(f"üìä Same-direction signal but DCA OFF - treating as fresh entry with initial_position_size")
        
        # Execute the trade with FULL risk settings (trader settings override recorder)
        _logger.info(f"üöÄ Executing {trade_action} {quantity} {ticker} for '{recorder_name}' | Price: {current_price} | TP: {tp_ticks} ticks ({tp_source}) | SL: {sl_ticks} ticks ({sl_source}) | Qty: {quantity_source}")
        
        # ============================================================
        # STRATEGY MODE: Check for open position BEFORE broker execution
        # For TradingView Strategy: SELL closes LONG, BUY closes SHORT
        # ============================================================
        cursor.execute(f'''
            SELECT * FROM recorded_trades 
            WHERE recorder_id = {placeholder} AND status = 'open' 
            ORDER BY entry_time DESC LIMIT 1
        ''', (recorder_id,))
        strategy_open_trade_row = cursor.fetchone()
        
        if strategy_open_trade_row:
            columns = [desc[0] for desc in cursor.description]
            strategy_open_trade = dict(zip(columns, strategy_open_trade_row))
            open_side = strategy_open_trade.get('side', '')
            
            _logger.info(f"üîç STRATEGY CHECK: Found open {open_side} trade #{strategy_open_trade['id']}, trade_action={trade_action}")
            
            # =============================================================================
            # AUTO-CLOSE DISABLED (Jan 9 2026)
            # This was causing "instant open/close" because:
            # 1. Stale DB records would trigger false closes when opposite signal arrives
            # 2. Even with staleness check, timing issues caused problems
            # 
            # NOW: System will NOT auto-close on opposite signals
            # Positions only close via: TP/SL, explicit CLOSE action, or manual UI
            # =============================================================================
            _logger.info(f"‚è≠Ô∏è SKIP: Auto-close disabled - not closing {open_side} trade #{strategy_open_trade['id']} on {trade_action} signal")
            _logger.info(f"‚è≠Ô∏è Continuing to process {trade_action} as DCA/size-in instead")
            # Don't return - let the trade continue processing as a normal signal
            # This allows DCA/sizing into positions
            
            # OLD CODE - DISABLED:
            # SELL signal when in LONG = just exit (don't open SHORT)
            if False and trade_action.upper() == 'SELL' and open_side == 'LONG':
                _logger.info(f"üìä SELL closes LONG - exiting position only")
                try:
                    tick_size_close = get_tick_size(ticker) if ticker else 0.25
                    tick_value_close = get_tick_value(ticker) if ticker else 0.50
                    entry_price_close = float(strategy_open_trade.get('entry_price', 0) or 0)
                    
                    pnl_ticks_close = (current_price - entry_price_close) / tick_size_close
                    pnl_dollars = pnl_ticks_close * tick_value_close * quantity
                    
                    # Close the trade in DB
                    cursor.execute(f'''
                        UPDATE recorded_trades 
                        SET status = 'closed', exit_price = {placeholder}, pnl = {placeholder}, 
                            pnl_ticks = {placeholder}, exit_reason = 'signal', 
                            exit_time = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                        WHERE id = {placeholder}
                    ''', (current_price, pnl_dollars, pnl_ticks_close, strategy_open_trade['id']))
                    conn.commit()
                    
                    _logger.info(f"‚úÖ LONG closed by SELL: #{strategy_open_trade['id']} | Exit: {current_price} | PnL: ${pnl_dollars:.2f}")
                    
                    # CRITICAL: Only queue broker close if trade is NOT stale
                    # Stale trades are just DB cleanup - broker may not have the position
                    if not is_stale_trade:
                        try:
                            close_task = {
                                'recorder_id': recorder_id,
                                'action': 'SELL',  # SELL to close LONG
                                'ticker': ticker,
                                'quantity': quantity,
                                'tp_ticks': 0,  # No TP for close
                                'sl_ticks': 0,
                                'retry_count': 0
                            }
                            broker_execution_queue.put_nowait(close_task)
                            _logger.info(f"üì§ Broker CLOSE queued: SELL {quantity} {ticker} (closing LONG)")
                            _broker_execution_stats['total_queued'] += 1
                        except Exception as queue_err:
                            _logger.warning(f"‚ö†Ô∏è Could not queue broker close: {queue_err}")
                    else:
                        _logger.info(f"üßπ STALE: Cleaned up DB record only - NO broker order sent")
                    
                    conn.close()
                    return jsonify({
                        'success': True,
                        'action': 'closed',
                        'trade_id': strategy_open_trade['id'],
                        'side': 'LONG',
                        'entry_price': entry_price_close,
                        'exit_price': current_price,
                        'pnl': pnl_dollars,
                        'pnl_ticks': pnl_ticks_close,
                        'exit_reason': 'signal',
                        'broker_queued': True
                    })
                except Exception as close_err:
                    _logger.warning(f"‚ö†Ô∏è Could not close LONG: {close_err}")
            
            # BUY signal when in SHORT = just exit (don't open LONG)
            # DISABLED - see above
            elif False and trade_action.upper() == 'BUY' and open_side == 'SHORT':
                _logger.info(f"üìä BUY closes SHORT - exiting position only")
                try:
                    tick_size_close = get_tick_size(ticker) if ticker else 0.25
                    tick_value_close = get_tick_value(ticker) if ticker else 0.50
                    entry_price_close = float(strategy_open_trade.get('entry_price', 0) or 0)
                    
                    pnl_ticks_close = (entry_price_close - current_price) / tick_size_close
                    pnl_dollars = pnl_ticks_close * tick_value_close * quantity
                    
                    # Close the trade in DB
                    cursor.execute(f'''
                        UPDATE recorded_trades 
                        SET status = 'closed', exit_price = {placeholder}, pnl = {placeholder}, 
                            pnl_ticks = {placeholder}, exit_reason = 'signal', 
                            exit_time = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                        WHERE id = {placeholder}
                    ''', (current_price, pnl_dollars, pnl_ticks_close, strategy_open_trade['id']))
                    conn.commit()
                    
                    _logger.info(f"‚úÖ SHORT closed by BUY: #{strategy_open_trade['id']} | Exit: {current_price} | PnL: ${pnl_dollars:.2f}")
                    
                    # CRITICAL: Only queue broker close if trade is NOT stale
                    # Stale trades are just DB cleanup - broker may not have the position
                    if not is_stale_trade:
                        try:
                            close_task = {
                                'recorder_id': recorder_id,
                                'action': 'BUY',  # BUY to close SHORT
                                'ticker': ticker,
                                'quantity': quantity,
                                'tp_ticks': 0,  # No TP for close
                                'sl_ticks': 0,
                                'retry_count': 0
                            }
                            broker_execution_queue.put_nowait(close_task)
                            _logger.info(f"üì§ Broker CLOSE queued: BUY {quantity} {ticker} (closing SHORT)")
                            _broker_execution_stats['total_queued'] += 1
                        except Exception as queue_err:
                            _logger.warning(f"‚ö†Ô∏è Could not queue broker close: {queue_err}")
                    else:
                        _logger.info(f"üßπ STALE: Cleaned up DB record only - NO broker order sent")
                    
                    conn.close()
                    return jsonify({
                        'success': True,
                        'action': 'closed',
                        'trade_id': strategy_open_trade['id'],
                        'side': 'SHORT',
                        'entry_price': entry_price_close,
                        'exit_price': current_price,
                        'pnl': pnl_dollars,
                        'pnl_ticks': pnl_ticks_close,
                        'exit_reason': 'signal',
                        'broker_queued': True
                    })
                except Exception as close_err:
                    _logger.warning(f"‚ö†Ô∏è Could not close SHORT: {close_err}")
        
        # ============================================================
        # SIGNAL-BASED TRACKING (Like Trade Manager) - BACKGROUND THREAD
        # Record position based on signal for P&L tracking
        # Runs in background so it NEVER blocks broker execution
        # ============================================================

        trade_side = 'LONG' if trade_action.upper() in ['BUY', 'LONG'] else 'SHORT'
        tick_size = get_tick_size(ticker) if ticker else 0.25
        tick_value = get_tick_value(ticker) if ticker else 0.50

        # Calculate TP/SL prices based on signal price
        tp_price = None
        sl_price = None
        if tp_ticks and tp_ticks > 0:
            if trade_side == 'LONG':
                tp_price = current_price + (tp_ticks * tick_size)
            else:
                tp_price = current_price - (tp_ticks * tick_size)
        if sl_ticks and sl_ticks > 0:
            if trade_side == 'LONG':
                sl_price = current_price - (sl_ticks * tick_size)
            else:
                sl_price = current_price + (sl_ticks * tick_size)

        # BACKGROUND: Record trade based on SIGNAL (not broker) - Like Trade Manager!
        import threading
        def _bg_signal_tracking(rec_id, tkr, t_action, t_side, c_price, qty, t_tp_price, t_sl_price, t_tick_size, t_tick_value, has_trader, rec_name, t_is_dca=False):
            try:
                trade_conn = get_db_connection()
                trade_cursor = trade_conn.cursor()
                is_pg = is_using_postgres()
                ph = '%s' if is_pg else '?'

                # Check for existing open position to handle reversals
                trade_cursor.execute(f'''
                    SELECT id, side, entry_price, quantity FROM recorded_trades
                    WHERE recorder_id = {ph} AND ticker = {ph} AND status = 'open'
                    ORDER BY id DESC LIMIT 1
                ''', (rec_id, tkr))
                existing_trade = trade_cursor.fetchone()

                recorded_trade_id = None

                if existing_trade:
                    existing_id, existing_side, existing_entry, existing_qty = existing_trade

                    if existing_side != t_side:
                        # REVERSAL: Close existing position and calculate P&L
                        if existing_side == 'LONG':
                            pnl_ticks = (c_price - existing_entry) / t_tick_size
                        else:
                            pnl_ticks = (existing_entry - c_price) / t_tick_size
                        pnl_dollars = pnl_ticks * t_tick_value * existing_qty

                        trade_cursor.execute(f'''
                            UPDATE recorded_trades
                            SET status = 'closed', exit_price = {ph}, pnl = {ph}, pnl_ticks = {ph},
                                exit_reason = 'signal', exit_time = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                            WHERE id = {ph}
                        ''', (c_price, pnl_dollars, pnl_ticks, existing_id))

                        _logger.info(f"üìä {existing_side} CLOSED by {t_side} signal | Entry: {existing_entry} | Exit: {c_price} | P&L: ${pnl_dollars:.2f} ({pnl_ticks:.1f} ticks)")

                        # Now open new position in opposite direction
                        broker_managed = True if has_trader else False
                        trade_cursor.execute(f'''
                            INSERT INTO recorded_trades
                            (recorder_id, ticker, action, side, entry_price, entry_time, quantity, status, tp_price, sl_price, broker_managed_tp_sl, created_at, updated_at)
                            VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, {ph}, 'open', {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                        ''', (rec_id, tkr, t_action, t_side, c_price, qty, t_tp_price, t_sl_price, broker_managed))
                        _logger.info(f"üìà NEW {t_side} opened @ {c_price} x{qty} | TP: {t_tp_price} | SL: {t_sl_price}")

                        try:
                            notify_trade_execution(action=t_side, symbol=tkr, quantity=qty, price=c_price, recorder_name=rec_name, tp_price=t_tp_price, sl_price=t_sl_price, recorder_id=rec_id)
                        except Exception:
                            pass
                    else:
                        # SAME SIDE
                        broker_managed = True if has_trader else False
                        if t_is_dca:
                            # DCA MODE ON: Add new record (stack positions)
                            trade_cursor.execute(f'''
                                INSERT INTO recorded_trades
                                (recorder_id, ticker, action, side, entry_price, entry_time, quantity, status, tp_price, sl_price, broker_managed_tp_sl, created_at, updated_at)
                                VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, {ph}, 'open', {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                            ''', (rec_id, tkr, t_action, t_side, c_price, qty, t_tp_price, t_sl_price, broker_managed))
                            _logger.info(f"üìà DCA {t_side} +{qty} @ {c_price}")

                            try:
                                notify_trade_execution(action=f"DCA {t_side}", symbol=tkr, quantity=qty, price=c_price, recorder_name=rec_name, tp_price=t_tp_price, sl_price=t_sl_price, recorder_id=rec_id)
                            except Exception:
                                pass
                        else:
                            # DCA MODE OFF: Close old record, open fresh one (no stacking)
                            trade_cursor.execute(f'''
                                UPDATE recorded_trades
                                SET status = 'closed', exit_price = {ph}, exit_reason = 'new_entry',
                                    exit_time = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                                WHERE id = {ph}
                            ''', (c_price, existing_id))
                            _logger.info(f"üìä [{rec_name}] DCA OFF - Closed stale {existing_side} #{existing_id}, opening fresh entry")

                            trade_cursor.execute(f'''
                                INSERT INTO recorded_trades
                                (recorder_id, ticker, action, side, entry_price, entry_time, quantity, status, tp_price, sl_price, broker_managed_tp_sl, created_at, updated_at)
                                VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, {ph}, 'open', {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                            ''', (rec_id, tkr, t_action, t_side, c_price, qty, t_tp_price, t_sl_price, broker_managed))
                            _logger.info(f"üìà NEW {t_side} opened @ {c_price} x{qty} | TP: {t_tp_price} | SL: {t_sl_price}")

                            try:
                                notify_trade_execution(action=t_side, symbol=tkr, quantity=qty, price=c_price, recorder_name=rec_name, tp_price=t_tp_price, sl_price=t_sl_price, recorder_id=rec_id)
                            except Exception:
                                pass
                else:
                    # NO EXISTING POSITION: Open new trade
                    broker_managed = True if has_trader else False
                    trade_cursor.execute(f'''
                        INSERT INTO recorded_trades
                        (recorder_id, ticker, action, side, entry_price, entry_time, quantity, status, tp_price, sl_price, broker_managed_tp_sl, created_at, updated_at)
                        VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, {ph}, 'open', {ph}, {ph}, {ph}, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                    ''', (rec_id, tkr, t_action, t_side, c_price, qty, t_tp_price, t_sl_price, broker_managed))
                    _logger.info(f"üìà NEW {t_side} opened @ {c_price} x{qty} | TP: {t_tp_price} | SL: {t_sl_price}")

                    try:
                        notify_trade_execution(action=t_side, symbol=tkr, quantity=qty, price=c_price, recorder_name=rec_name, tp_price=t_tp_price, sl_price=t_sl_price, recorder_id=rec_id)
                    except Exception:
                        pass

                # Also update recorder_positions for aggregate tracking
                trade_cursor.execute(f'''
                    SELECT id, side, total_quantity, avg_entry_price FROM recorder_positions
                    WHERE recorder_id = {ph} AND ticker = {ph} AND status = 'open'
                ''', (rec_id, tkr))
                existing_pos = trade_cursor.fetchone()

                if existing_pos:
                    pos_id, pos_side, pos_qty, pos_avg = existing_pos
                    if pos_side != t_side:
                        if pos_side == 'LONG':
                            pos_pnl_ticks = (c_price - pos_avg) / t_tick_size
                        else:
                            pos_pnl_ticks = (pos_avg - c_price) / t_tick_size
                        pos_pnl = pos_pnl_ticks * t_tick_value * pos_qty
                        trade_cursor.execute(f'''
                            UPDATE recorder_positions
                            SET status = 'closed', exit_price = {ph}, realized_pnl = {ph},
                                exit_time = CURRENT_TIMESTAMP, closed_at = CURRENT_TIMESTAMP
                            WHERE id = {ph}
                        ''', (c_price, pos_pnl, pos_id))
                        trade_cursor.execute(f'''
                            INSERT INTO recorder_positions (recorder_id, ticker, side, total_quantity, avg_entry_price, status)
                            VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, 'open')
                        ''', (rec_id, tkr, t_side, qty, c_price))
                    else:
                        new_qty = pos_qty + qty
                        new_avg = ((pos_avg * pos_qty) + (c_price * qty)) / new_qty
                        trade_cursor.execute(f'''
                            UPDATE recorder_positions
                            SET total_quantity = {ph}, avg_entry_price = {ph}, updated_at = CURRENT_TIMESTAMP
                            WHERE id = {ph}
                        ''', (new_qty, new_avg, pos_id))
                else:
                    trade_cursor.execute(f'''
                        INSERT INTO recorder_positions (recorder_id, ticker, side, total_quantity, avg_entry_price, status)
                        VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, 'open')
                    ''', (rec_id, tkr, t_side, qty, c_price))

                trade_conn.commit()
                trade_conn.close()
                _logger.info(f"‚úÖ SIGNAL TRACKED (bg): {t_side} {tkr} @ {c_price}")
            except Exception as track_err:
                _logger.error(f"‚ùå Signal tracking error (bg): {track_err}")

        # These were previously set inside signal tracking (now background) - set defaults for response
        recorded_trade_id = None
        pnl_result = None

        threading.Thread(
            target=_bg_signal_tracking,
            args=(recorder_id, ticker, trade_action, trade_side, current_price, quantity, tp_price, sl_price, tick_size, tick_value, bool(trader), recorder_name, is_dca),
            daemon=True
        ).start()

        # Build risk_config for apply_risk_orders (includes trailing stop and break-even)
        risk_config = {}
        
        # Take profit
        if tp_ticks and tp_ticks > 0:
            risk_config['take_profit'] = [{
                'gain_ticks': tp_ticks,
                'trim_percent': 100
            }]
            risk_config['trim_units'] = trim_units  # 'Percent' or 'Contracts'

            # Multi-target TP: if tp_targets has multiple levels, use all of them
            if tp_targets and len(tp_targets) > 1:
                _logger.info(f"üìä Raw tp_targets from DB: {tp_targets}")
                _logger.info(f"üìä trim_units={trim_units}, quantity will be {quantity}")
                multi_tp = []
                for target in tp_targets:
                    t_ticks = target.get('ticks') or target.get('value', 0)
                    t_trim = target.get('trim', 0)
                    _logger.info(f"   TP level: ticks={t_ticks}, trim={t_trim}")
                    if t_ticks and float(t_ticks) > 0:
                        # Apply tp_units conversion (same as single-TP at line 16764)
                        if tp_units == 'Points':
                            converted_ticks = int(float(t_ticks) / tick_value) if tick_value else int(float(t_ticks) / tick_size)
                        else:
                            converted_ticks = int(float(t_ticks))
                        multi_tp.append({
                            'gain_ticks': converted_ticks,
                            'trim_percent': float(t_trim)
                        })
                if len(multi_tp) > 1:
                    risk_config['take_profit'] = multi_tp
                    _logger.info(f"üìä Multi-target TP: {len(multi_tp)} levels configured: {multi_tp}")

        # Stop loss (fixed or trailing)
        # Priority: Webhook trail settings > SL type from settings
        if webhook_trail is not None and sl_ticks > 0:
            # Webhook provides trailing stop directly
            trail_offset = int(float(webhook_trail))
            trail_activation = int(float(webhook_trail_trigger)) if webhook_trail_trigger else 0
            risk_config['trail'] = {
                'activation_ticks': trail_activation,  # When to start trailing (0 = immediate)
                'offset_ticks': trail_offset  # Trail distance
            }
            # Add trail frequency if provided
            if webhook_trail_freq:
                risk_config['trail']['frequency_ticks'] = int(float(webhook_trail_freq))
                _logger.info(f"üìä Trailing stop from WEBHOOK: offset={trail_offset}, trigger={trail_activation}, freq={webhook_trail_freq}")
            else:
                _logger.info(f"üìä Trailing stop from WEBHOOK: offset={trail_offset}, trigger={trail_activation}")
        elif sl_enabled and sl_ticks > 0:
            if sl_type == 'Trailing' or sl_type == 'Trail':
                # Trailing stop from settings: use recorder's trail settings
                risk_config['trail'] = {
                    'activation_ticks': trail_trigger if trail_trigger > 0 else 0,  # From recorder settings (0 = immediate)
                    'offset_ticks': sl_ticks  # Trail offset = SL distance
                }
                # Add trail frequency from recorder settings
                if trail_freq > 0:
                    risk_config['trail']['frequency_ticks'] = trail_freq
                    _logger.info(f"üìä Trailing stop from SETTINGS: offset={sl_ticks}, trigger={trail_trigger}, freq={trail_freq}")
                else:
                    _logger.info(f"üìä Trailing stop from SETTINGS: offset={sl_ticks}, trigger={trail_trigger}")
            else:
                # Fixed stop loss
                risk_config['stop_loss'] = {
                    'type': 'fixed',
                    'loss_ticks': sl_ticks
                }

        # Break-even
        # Priority: Webhook breakeven > Settings breakeven
        if webhook_breakeven is not None:
            # Webhook provides breakeven directly
            be_activation = int(float(webhook_breakeven))
            be_offset = int(float(webhook_breakeven_offset)) if webhook_breakeven_offset else 0
            risk_config['break_even'] = {
                'activation_ticks': be_activation,
                'offset_ticks': be_offset  # How many ticks of profit to lock in (0 = true breakeven)
            }
            _logger.info(f"üìä Break-even from WEBHOOK: activation={be_activation} ticks, offset={be_offset} ticks")
        elif break_even_enabled and break_even_ticks > 0:
            # Use recorder's break_even_offset setting
            risk_config['break_even'] = {
                'activation_ticks': break_even_ticks,
                'offset_ticks': break_even_offset  # From recorder settings (0 = true breakeven)
            }
            if break_even_offset > 0:
                _logger.info(f"üìä Break-even from SETTINGS: activation={break_even_ticks} ticks, offset={break_even_offset} ticks")
            else:
                _logger.info(f"üìä Break-even from SETTINGS: activation={break_even_ticks} ticks (true BE)")
        
        # STEP 3: Queue broker execution (Trade Manager style - async, non-blocking)
        # Webhook returns immediately, broker execution happens in background
        try:
            broker_task = {
                'recorder_id': recorder_id,
                'action': trade_action,
                'ticker': ticker,
                'quantity': quantity,
                'tp_ticks': tp_ticks,
                'sl_ticks': sl_ticks if sl_ticks > 0 else 0,
                'break_even_enabled': break_even_enabled,
                'break_even_ticks': break_even_ticks,
                'entry_price': current_price,
                'is_long': trade_side == 'LONG',
                'risk_config': risk_config,  # NEW: Pass full risk_config for trailing stop/break-even
                'sl_type': sl_type,  # NEW: Pass sl_type for trailing stop detection
                'retry_count': 0,
                'queued_at': time.time(),  # For staleness check - reject if too old
                'signal_price': current_price,  # Original signal price for staleness comparison
                'signal_id': signal_id,  # Pipeline tracking
                'signal_blocking': recorder.get('signal_blocking', False)  # Pass through for SET after execution
            }

            broker_was_queued = False  # Track if we successfully queued
            try:
                # STEP 6: Queue to broker execution
                track_signal_step(signal_id, 'STEP6_BROKER_QUEUING', {
                    'action': trade_action,
                    'quantity': quantity,
                    'ticker': ticker
                })
                broker_execution_queue.put_nowait(broker_task)
                broker_was_queued = True  # Successfully queued!
                track_signal_step(signal_id, 'STEP6_BROKER_QUEUED', {'queue_size': broker_execution_queue.qsize()})
                workers_alive = sum(1 for t in _broker_execution_threads if t.is_alive()) if _broker_execution_threads else 0
                _logger.info(f"üì§ Broker execution queued: {trade_action} {quantity} {ticker} (will execute async)")
                _logger.info(f"   ‚úÖ Queue size: {broker_execution_queue.qsize()}/{broker_execution_queue.maxsize}")
                _logger.info(f"   ‚úÖ HIVE MIND workers alive: {workers_alive}/{_broker_execution_worker_count}")
                _logger.info(f"   ‚úÖ Task details: recorder_id={recorder_id}, action={trade_action}, quantity={quantity}, ticker={ticker}")
                _broker_execution_stats['total_queued'] += 1
            except Exception as queue_err:
                # Queue full - log but don't fail webhook
                _logger.warning(f"‚ö†Ô∏è Broker execution queue full - signal still tracked, broker execution skipped")
                _logger.warning(f"   Queue error: {queue_err}")
                _logger.warning(f"   Queue size: {broker_execution_queue.qsize()}/{broker_execution_queue.maxsize}")
                _broker_execution_stats['total_failed'] += 1

        except Exception as e:
            # Queue error - log but don't fail webhook
            _logger.warning(f"‚ö†Ô∏è Broker execution queue error (tracking still works): {e}")
            broker_was_queued = False
        
        # Return success IMMEDIATELY - position is tracked, broker execution is queued
        # Track successful webhook processing for health monitoring
        _webhook_last_processed = time.time()
        _webhook_processing_count += 1
        processing_time = _webhook_last_processed - webhook_start_time

        # Log to webhook activity for monitoring
        log_webhook_activity(
            recorder_name=recorder_name,
            action=trade_action,
            symbol=ticker,
            status='success',
            broker_result='queued' if broker_was_queued else 'no_broker',
            trade_id=recorded_trade_id
        )

        # Log final status
        queue_status = broker_execution_queue.qsize()
        workers_alive = sum(1 for t in _broker_execution_threads if t.is_alive()) if _broker_execution_threads else 0
        _logger.info(f"‚úÖ Webhook processed in {processing_time:.2f}s (total: {_webhook_processing_count})")
        _logger.info(f"   Final status: Queue size={queue_status}, HIVE MIND workers={workers_alive}/{_broker_execution_worker_count}, Broker task queued={'YES' if queue_status > 0 or 'broker_task' in locals() else 'NO'}")
        
        return jsonify({
            'success': True,
            'action': trade_action,
            'side': trade_side,
            'entry_price': current_price,
            'quantity': quantity,
            'tp_price': tp_price,
            'sl_price': sl_price,
            'trade_id': recorded_trade_id,
            'pnl': pnl_result,
            'broker_queued': True,  # Broker execution queued (async)
            'tracking': 'signal-based',
            'processing_time_ms': int(processing_time * 1000)
        })
            
    except NameError as name_err:
        # CRITICAL: Close DB connection to prevent pool poisoning
        try:
            if 'conn' in locals() and conn:
                conn.close()
        except:
            pass
        # Track webhook error
        _webhook_error_count += 1
        # Log to webhook activity for monitoring
        try:
            log_webhook_activity(
                recorder_name=recorder_name if 'recorder_name' in locals() else 'Unknown',
                action=trade_action if 'trade_action' in locals() else 'Unknown',
                symbol=ticker if 'ticker' in locals() else 'Unknown',
                status='failed',
                error=f"NameError: {str(name_err)}"
            )
        except:
            pass
        # Special handling for NameError (like 'logger' not defined)
        import logging
        safe_logger = logging.getLogger('webhook_handler')
        safe_logger.error(f"‚ùå NameError in webhook handler: {name_err}")
        import traceback
        traceback.print_exc()
        # Return a more helpful error message
        error_msg = str(name_err)
        if 'logger' in error_msg:
            error_msg = "Internal error: logger configuration issue. Please check server logs."
        return jsonify({'success': False, 'error': error_msg}), 500
    except Exception as e:
        # CRITICAL: Close DB connection to prevent pool poisoning
        try:
            if 'conn' in locals() and conn:
                conn.close()
        except:
            pass
        # Track webhook error
        _webhook_error_count += 1
        # Log to webhook activity for monitoring
        try:
            log_webhook_activity(
                recorder_name=recorder_name if 'recorder_name' in locals() else 'Unknown',
                action=trade_action if 'trade_action' in locals() else 'Unknown',
                symbol=ticker if 'ticker' in locals() else 'Unknown',
                status='failed',
                error=str(e)
            )
        except:
            pass
        # Safe error handling - ensure we have a logger
        import logging
        safe_logger = logging.getLogger('webhook_handler')
        try:
            if '_logger' in locals() and _logger:
                _logger.error(f"‚ùå Webhook processing error: {e}")
            else:
                safe_logger.error(f"‚ùå Webhook processing error: {e}")
        except:
            safe_logger.error(f"‚ùå Webhook processing error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================
# LEGACY WEBHOOK CODE - DISABLED (handled by Trading Engine)
# ============================================================
# The following code has been moved to recorder_service.py
# It's kept here commented out for reference only.
# ============================================================

def _DISABLED_receive_webhook_legacy(webhook_token):
    """
    DISABLED - This code has been moved to Trading Engine (recorder_service.py)
    Kept here for reference only. DO NOT RE-ENABLE.
    """
    try:
        # Find recorder by webhook token
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                SELECT * FROM recorders WHERE webhook_token = %s AND recording_enabled = true
            ''', (webhook_token,))
        else:
            cursor.execute('''
                SELECT * FROM recorders WHERE webhook_token = ? AND recording_enabled = 1
            ''', (webhook_token,))
        recorder = cursor.fetchone()
        
        if not recorder:
            logger.warning(f"Webhook received for unknown/disabled token: {webhook_token[:8]}...")
            return jsonify({'success': False, 'error': 'Invalid or disabled webhook'}), 404
        
        recorder = dict(recorder)
        recorder_id = recorder['id']
        recorder_name = recorder['name']
        
        # Parse incoming data (support both JSON and form data)
        if request.is_json:
            data = request.get_json()
        else:
            # Try to parse as JSON from text body
            try:
                data = json.loads(request.data.decode('utf-8'))
            except:
                data = request.form.to_dict()
        
        if not data:
            return jsonify({'success': False, 'error': 'No data received'}), 400
        
        logger.info(f"üì® Webhook received for recorder '{recorder_name}': {data}")
        
        # Extract signal data
        action = str(data.get('action', '')).lower().strip()
        ticker = data.get('ticker', data.get('symbol', ''))
        # Fallback to recorder's default symbol if ticker not in webhook
        if not ticker:
            ticker = recorder.get('symbol') or ''
        price = data.get('price', data.get('close', 0))
        
        # Strategy-specific fields (when TradingView handles sizing)
        position_size = data.get('position_size', data.get('contracts'))
        market_position = data.get('market_position', '')  # long, short, flat
        prev_position_size = data.get('prev_position_size', data.get('prev_market_position_size'))
        
        # Validate action - including TP/SL price alerts
        valid_actions = ['buy', 'sell', 'long', 'short', 'close', 'flat', 'exit', 
                         'tp_hit', 'sl_hit', 'take_profit', 'stop_loss', 'price_alert']
        if action not in valid_actions:
            logger.warning(f"Invalid action '{action}' for recorder {recorder_name}")
            return jsonify({'success': False, 'error': f'Invalid action: {action}'}), 400
        
        # Handle TP/SL price alerts - these close open positions at TP/SL price
        if action in ['tp_hit', 'take_profit']:
            normalized_action = 'TP_HIT'
            direction = 'flat'
        elif action in ['sl_hit', 'stop_loss']:
            normalized_action = 'SL_HIT'
            direction = 'flat'
        elif action in ['price_alert', 'price_update', 'price', 'tick', 'update']:
            # Price update for unrealized P&L tracking (Trade Manager style)
            normalized_action = 'PRICE_UPDATE'
            direction = None
        # Standard actions
        elif action in ['long', 'buy']:
            normalized_action = 'BUY'
            direction = 'long'
        elif action in ['short', 'sell']:
            normalized_action = 'SELL'
            direction = 'short'
        else:  # close, flat, exit
            normalized_action = 'CLOSE'
            direction = 'flat'
        
        # Check signal delay (skip signals based on recorder settings)
        signal_delay = recorder.get('add_delay', 1) or 1

        # Get/update signal counter for this recorder
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'

        cursor.execute(f'''
            SELECT signal_count FROM recorders WHERE id = {ph}
        ''', (recorder_id,))
        result = cursor.fetchone()
        signal_count = (result['signal_count'] if result and result['signal_count'] else 0) + 1

        cursor.execute(f'''
            UPDATE recorders SET signal_count = {ph}, updated_at = CURRENT_TIMESTAMP WHERE id = {ph}
        ''', (signal_count, recorder_id))
        conn.commit()
        
        # Check if we should skip this signal
        if signal_delay > 1 and signal_count % signal_delay != 0:
            logger.info(f"‚è≠Ô∏è Skipping signal {signal_count} for {recorder_name} (delay={signal_delay})")
            conn.close()
            return jsonify({
                'success': True,
                'skipped': True,
                'message': f'Signal {signal_count} skipped (executing every {signal_delay} signals)',
                'next_execute': signal_count + (signal_delay - (signal_count % signal_delay))
            })
        
        # Determine if this is a simple alert or strategy alert
        is_strategy_alert = position_size is not None or market_position

        # ============================================================
        # PAPER TRADING: Record trade using live price feed
        # ============================================================
        print(f"üß™üß™üß™ WEBHOOK PAPER CHECK: action={normalized_action}, ticker={ticker}", flush=True)
        if normalized_action in ['BUY', 'SELL', 'LONG', 'SHORT', 'CLOSE', 'FLAT', 'EXIT'] and ticker:
            try:
                # Extract quantity for paper trading (from webhook data or position_size)
                paper_qty_raw = data.get('quantity', data.get('contracts', position_size))
                paper_qty = int(paper_qty_raw) if paper_qty_raw else 1
                paper_price = float(price) if price else None
                print(f"üß™üß™üß™ CALLING record_paper_trade: rec={recorder_id}, sym={ticker}, act={normalized_action}, qty={paper_qty}, price={paper_price}", flush=True)
                result = record_paper_trade_from_webhook(
                    recorder_id=recorder_id,
                    symbol=ticker,
                    action=normalized_action,
                    quantity=paper_qty,
                    price=paper_price
                )
                print(f"üß™üß™üß™ PAPER TRADE RESULT: {result}", flush=True)
            except Exception as paper_err:
                import traceback
                print(f"üß™üß™üß™ PAPER TRADE ERROR: {paper_err}", flush=True)
                print(traceback.format_exc(), flush=True)
        else:
            print(f"üß™üß™üß™ PAPER SKIPPED: action={normalized_action} not in list OR ticker={ticker} empty", flush=True)

        # Record the signal
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS recorded_signals (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                recorder_id INTEGER NOT NULL,
                action TEXT NOT NULL,
                ticker TEXT,
                price REAL,
                quantity INTEGER DEFAULT 1,
                position_size TEXT,
                market_position TEXT,
                signal_type TEXT,
                raw_data TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                executed INTEGER DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (recorder_id) REFERENCES recorders(id)
            )
        ''')
        
        cursor.execute('''
            INSERT INTO recorded_signals 
            (recorder_id, action, ticker, price, position_size, market_position, signal_type, raw_data)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            recorder_id,
            normalized_action,
            ticker,
            float(price) if price else None,
            str(position_size) if position_size else None,
            market_position,
            'strategy' if is_strategy_alert else 'alert',
            json.dumps(data)
        ))
        
        signal_id = cursor.lastrowid
        
        # =====================================================
        # TRADE PROCESSING LOGIC - Convert signals into trades
        # WITH TP/SL SETTINGS FROM RECORDER
        # =====================================================
        trade_result = None
        
        # Use LIVE TradingView price for entry (so P&L starts at ~$0)
        signal_price = float(price) if price else 0
        live_price = None
        ticker_root = extract_symbol_root(ticker) if ticker else None
        if ticker_root and ticker_root in _market_data_cache:
            live_price = _market_data_cache[ticker_root].get('last')
        if not live_price:
            live_price = get_cached_price(ticker) if ticker else None
        current_price = live_price if live_price else signal_price
        logger.info(f"üìä Entry price: {current_price} (live={live_price}, signal={signal_price})")
        
        # Get position size from recorder settings
        # Use add_position_size when adding to existing position (DCA), otherwise use initial_position_size
        initial_position_size = int(recorder.get('initial_position_size', 1) or 1)
        add_position_size = int(recorder.get('add_position_size', 0) or 0)
        
        # Check if there's an existing open position to determine if this is DCA
        is_dca_add = False
        if open_trade:
            # If same direction as existing position, this is a DCA add
            existing_side = open_trade.get('side', '')
            if (existing_side == 'LONG' and side == 'LONG') or (existing_side == 'SHORT' and side == 'SHORT'):
                is_dca_add = True
                logger.info(f"üìà [{recorder_name}] DCA detected: Adding to existing {existing_side} position")
        
        # Determine quantity
        if position_size:
            quantity = int(position_size)
        elif is_dca_add and add_position_size > 0:
            quantity = add_position_size
            logger.info(f"üìà [{recorder_name}] Using add_position_size: {quantity} contracts")
        else:
            quantity = initial_position_size
        
        # Get TP/SL settings from recorder
        sl_enabled = recorder.get('sl_enabled', 0)
        sl_amount = recorder.get('sl_amount', 0) or 0
        sl_units = recorder.get('sl_units', 'Ticks')
        
        # Parse TP targets (JSON array)
        tp_targets_raw = recorder.get('tp_targets', '[]')
        try:
            tp_targets = json.loads(tp_targets_raw) if isinstance(tp_targets_raw, str) else tp_targets_raw or []
        except:
            tp_targets = []
        
        # Get first TP target (primary)
        tp_ticks = tp_targets[0].get('value', 0) if tp_targets else 0
        
        # Determine tick size and tick value for PnL calculation
        tick_size = get_tick_size(ticker) if ticker else 0.25
        tick_value = get_tick_value(ticker) if ticker else 0.50
        
        # Check for existing open trade for this recorder
        cursor.execute(f'''
            SELECT * FROM recorded_trades 
            WHERE recorder_id = {placeholder} AND status = 'open' 
            ORDER BY entry_time DESC LIMIT 1
        ''', (recorder_id,))
        open_trade_row = cursor.fetchone()
        
        open_trade = None
        if open_trade_row:
            # Convert to dict properly
            columns = [desc[0] for desc in cursor.description]
            open_trade = dict(zip(columns, open_trade_row))
        
        # ============================================================
        # üìâ AVERAGING DOWN CHECK
        # ============================================================
        # If enabled, check if price has moved against position and we should add
        avg_down_enabled = recorder.get('avg_down_enabled', False)
        avg_down_amount = int(recorder.get('avg_down_amount', 0) or 0)
        avg_down_point = float(recorder.get('avg_down_point', 0) or 0)
        avg_down_units = recorder.get('avg_down_units', 'Ticks')
        
        avg_down_triggered = False
        if avg_down_enabled and avg_down_amount > 0 and avg_down_point > 0 and open_trade:
            entry_price = float(open_trade.get('entry_price', 0) or 0)
            trade_side = open_trade.get('side', '')
            
            if entry_price > 0 and current_price > 0:
                # Convert avg_down_point to price difference based on units
                if avg_down_units == 'Points':
                    # Points = dollar value, convert using tick value
                    price_threshold = avg_down_point / tick_value * tick_size if tick_value else avg_down_point
                elif avg_down_units == 'Percent':
                    # Percent of entry price
                    price_threshold = entry_price * (avg_down_point / 100)
                else:
                    # Ticks (default)
                    price_threshold = avg_down_point * tick_size
                
                # Check if price moved against position by threshold
                if trade_side == 'LONG':
                    # For LONG: price dropped below entry by threshold
                    price_diff = entry_price - current_price
                    if price_diff >= price_threshold:
                        avg_down_triggered = True
                        logger.info(f"üìâ [{recorder_name}] AVG DOWN triggered: LONG entry ${entry_price:.2f}, current ${current_price:.2f}, diff {price_diff:.2f} >= threshold {price_threshold:.2f}")
                elif trade_side == 'SHORT':
                    # For SHORT: price rose above entry by threshold
                    price_diff = current_price - entry_price
                    if price_diff >= price_threshold:
                        avg_down_triggered = True
                        logger.info(f"üìâ [{recorder_name}] AVG DOWN triggered: SHORT entry ${entry_price:.2f}, current ${current_price:.2f}, diff {price_diff:.2f} >= threshold {price_threshold:.2f}")
                
                if avg_down_triggered:
                    # Use avg_down_amount as the quantity for this trade
                    quantity = avg_down_amount
                    # Force the signal to be in the same direction as existing position
                    if trade_side == 'LONG':
                        side = 'LONG'
                        action = 'BUY'
                    else:
                        side = 'SHORT'
                        action = 'SELL'
                    logger.info(f"üìâ [{recorder_name}] AVG DOWN: Adding {quantity} contracts to {trade_side} position")
        
        def calculate_tp_sl_prices(entry_price, side, tp_ticks, sl_ticks, tick_size):
            """Calculate TP and SL price levels based on entry and tick settings"""
            if side == 'LONG':
                tp_price = entry_price + (tp_ticks * tick_size) if tp_ticks else None
                sl_price = entry_price - (sl_ticks * tick_size) if sl_ticks else None
            else:  # SHORT
                tp_price = entry_price - (tp_ticks * tick_size) if tp_ticks else None
                sl_price = entry_price + (sl_ticks * tick_size) if sl_ticks else None
            return tp_price, sl_price
        
        def check_tp_sl_hit(open_trade, current_price, tick_size):
            """Check if TP or SL was hit and return exit info"""
            if not open_trade:
                return None, None, None
            
            tp_price = open_trade.get('tp_price')
            sl_price = open_trade.get('sl_price')
            side = open_trade['side']
            entry_price = open_trade['entry_price']
            
            if side == 'LONG':
                # For LONG: TP hit if price >= tp_price, SL hit if price <= sl_price
                if tp_price and current_price >= tp_price:
                    return 'tp', tp_price, (tp_price - entry_price) / tick_size
                if sl_price and current_price <= sl_price:
                    return 'sl', sl_price, (sl_price - entry_price) / tick_size
            else:  # SHORT
                # For SHORT: TP hit if price <= tp_price, SL hit if price >= sl_price
                if tp_price and current_price <= tp_price:
                    return 'tp', tp_price, (entry_price - tp_price) / tick_size
                if sl_price and current_price >= sl_price:
                    return 'sl', sl_price, (entry_price - sl_price) / tick_size
            
            return None, None, None
        
        def close_trade(cursor, trade, exit_price, pnl_ticks, tick_value, exit_reason):
            """Close a trade and calculate PnL"""
            pnl = pnl_ticks * tick_value * trade['quantity']
            ph = '%s' if is_postgres else '?'
            
            cursor.execute(f'''
                UPDATE recorded_trades 
                SET exit_price = {ph}, exit_time = CURRENT_TIMESTAMP, 
                    pnl = {ph}, pnl_ticks = {ph}, status = 'closed', 
                    exit_reason = {ph}, updated_at = CURRENT_TIMESTAMP
                WHERE id = {ph}
            ''', (exit_price, pnl, pnl_ticks, exit_reason, trade['id']))
            
            return pnl, pnl_ticks
        
        def close_recorder_position(cursor, position_id, exit_price, ticker):
            """Close a recorder position and calculate final PnL"""
            ph = '%s' if is_postgres else '?'
            cursor.execute(f'SELECT * FROM recorder_positions WHERE id = {ph}', (position_id,))
            row = cursor.fetchone()
            
            if not row:
                return
            
            # Get column names for dict conversion
            columns = [desc[0] for desc in cursor.description]
            pos = dict(zip(columns, row))
            
            avg_entry = pos['avg_entry_price']
            total_qty = pos['total_quantity']
            side = pos['side']
            
            pos_tick_size = get_tick_size(ticker)
            pos_tick_value = get_tick_value(ticker)
            
            if side == 'LONG':
                pnl_ticks = (exit_price - avg_entry) / pos_tick_size
            else:
                pnl_ticks = (avg_entry - exit_price) / pos_tick_size
            
            realized_pnl = pnl_ticks * pos_tick_value * total_qty
            
            cursor.execute(f'''
                UPDATE recorder_positions
                SET status = 'closed',
                    exit_price = {ph},
                    realized_pnl = {ph},
                    closed_at = CURRENT_TIMESTAMP,
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = {ph}
            ''', (exit_price, realized_pnl, position_id))
            
            logger.info(f"üìä Position closed: {side} {ticker} x{total_qty} @ avg {avg_entry} -> {exit_price} | PnL: ${realized_pnl:.2f}")
        
        def update_recorder_position(cursor, recorder_id, ticker, side, price, quantity=1):
            """
            Update or create a recorder position for position-based drawdown tracking.
            This is ADDITIVE to the recorded_trades table - keeps existing trade records.
            
            Returns: position_id, is_new_position, total_quantity
            """
            import json as json_module
            from datetime import datetime as dt
            ph = '%s' if is_postgres else '?'
            
            # Check for existing open position for this recorder+ticker
            cursor.execute(f'''
                SELECT id, total_quantity, avg_entry_price, side
                FROM recorder_positions
                WHERE recorder_id = {ph} AND ticker = {ph} AND status = 'open'
            ''', (recorder_id, ticker))
            
            existing = cursor.fetchone()
            
            if existing:
                pos_id, total_qty, avg_entry, pos_side = existing
                
                if pos_side == side:
                    # SAME SIDE: Add to position (DCA)
                    new_qty = total_qty + quantity
                    new_avg = ((avg_entry * total_qty) + (price * quantity)) / new_qty
                    
                    cursor.execute(f'''
                        UPDATE recorder_positions
                        SET total_quantity = {ph}, avg_entry_price = {ph}, updated_at = CURRENT_TIMESTAMP
                        WHERE id = {ph}
                    ''', (new_qty, new_avg, pos_id))
                    
                    logger.info(f"üìà Position DCA: {side} {ticker} +{quantity} @ {price} | Total: {new_qty} @ avg {new_avg:.2f}")
                    return pos_id, False, new_qty
                else:
                    # OPPOSITE SIDE: Close existing position, create new one
                    close_recorder_position(cursor, pos_id, price, ticker)
                    # Fall through to create new position below
            
            # NO POSITION or just closed opposite: Create new position
            cursor.execute(f'''
                INSERT INTO recorder_positions (recorder_id, ticker, side, total_quantity, avg_entry_price)
                VALUES ({ph}, {ph}, {ph}, {ph}, {ph})
            ''', (recorder_id, ticker, side, quantity, price))
            
            if is_postgres:
                cursor.execute('SELECT lastval()')
                new_id = cursor.fetchone()[0]
            else:
                new_id = cursor.lastrowid
            
            logger.info(f"üìä New position: {side} {ticker} x{quantity} @ {price}")
            return new_id, True, quantity
        
        # First, check if any open trade hit TP/SL based on current price
        if open_trade:
            hit_type, hit_price, hit_ticks = check_tp_sl_hit(open_trade, current_price, tick_size)
            
            if hit_type:
                # TP or SL was hit - close at the TP/SL price, not current price
                pnl, pnl_ticks = close_trade(cursor, open_trade, hit_price, hit_ticks, tick_value, hit_type)
                
                trade_result = {
                    'action': 'closed',
                    'trade_id': open_trade['id'],
                    'side': open_trade['side'],
                    'entry_price': open_trade['entry_price'],
                    'exit_price': hit_price,
                    'pnl': pnl,
                    'pnl_ticks': pnl_ticks,
                    'exit_reason': hit_type.upper()
                }
                logger.info(f"üéØ {hit_type.upper()} HIT for '{recorder_name}': {open_trade['side']} {ticker} | Exit: {hit_price} | PnL: ${pnl:.2f} ({pnl_ticks:.1f} ticks)")
                open_trade = None  # Trade is now closed
        
        # Now process the signal action
        
        # Handle TP/SL price alerts from TradingView
        if normalized_action == 'TP_HIT':
            if open_trade:
                # Close at TP price (use stored TP or current price if not set)
                tp_price = open_trade.get('tp_price') or current_price
                if open_trade['side'] == 'LONG':
                    pnl_ticks = (tp_price - open_trade['entry_price']) / tick_size
                else:
                    pnl_ticks = (open_trade['entry_price'] - tp_price) / tick_size
                
                pnl, _ = close_trade(cursor, open_trade, tp_price, pnl_ticks, tick_value, 'tp')
                
                # Also close any open position in recorder_positions
                cursor.execute(f'''
                    SELECT id FROM recorder_positions 
                    WHERE recorder_id = {placeholder} AND ticker = {placeholder} AND status = 'open'
                ''', (recorder_id, ticker))
                open_pos = cursor.fetchone()
                if open_pos:
                    close_recorder_position(cursor, open_pos[0], tp_price, ticker)
                
                trade_result = {
                    'action': 'closed',
                    'trade_id': open_trade['id'],
                    'side': open_trade['side'],
                    'entry_price': open_trade['entry_price'],
                    'exit_price': tp_price,
                    'pnl': pnl,
                    'pnl_ticks': pnl_ticks,
                    'exit_reason': 'TP'
                }
                logger.info(f"üéØ TP HIT (alert) for '{recorder_name}': {open_trade['side']} {ticker} | Exit: {tp_price} | PnL: ${pnl:.2f}")
        
        elif normalized_action == 'SL_HIT':
            if open_trade:
                # Close at SL price (use stored SL or current price if not set)
                sl_price = open_trade.get('sl_price') or current_price
                if open_trade['side'] == 'LONG':
                    pnl_ticks = (sl_price - open_trade['entry_price']) / tick_size
                else:
                    pnl_ticks = (open_trade['entry_price'] - sl_price) / tick_size
                
                pnl, _ = close_trade(cursor, open_trade, sl_price, pnl_ticks, tick_value, 'sl')
                
                # Also close any open position in recorder_positions
                cursor.execute(f'''
                    SELECT id FROM recorder_positions 
                    WHERE recorder_id = {placeholder} AND ticker = {placeholder} AND status = 'open'
                ''', (recorder_id, ticker))
                open_pos = cursor.fetchone()
                if open_pos:
                    close_recorder_position(cursor, open_pos[0], sl_price, ticker)
                
                trade_result = {
                    'action': 'closed',
                    'trade_id': open_trade['id'],
                    'side': open_trade['side'],
                    'entry_price': open_trade['entry_price'],
                    'exit_price': sl_price,
                    'pnl': pnl,
                    'pnl_ticks': pnl_ticks,
                    'exit_reason': 'SL'
                }
                logger.info(f"üõë SL HIT (alert) for '{recorder_name}': {open_trade['side']} {ticker} | Exit: {sl_price} | PnL: ${pnl:.2f}")
        
        elif normalized_action == 'PRICE_UPDATE':
            # Just a price update - check if it hits TP/SL
            if open_trade:
                hit_type, hit_price, hit_ticks = check_tp_sl_hit(open_trade, current_price, tick_size)
                if hit_type:
                    pnl, pnl_ticks = close_trade(cursor, open_trade, hit_price, hit_ticks, tick_value, hit_type)
                    trade_result = {
                        'action': 'closed',
                        'trade_id': open_trade['id'],
                        'side': open_trade['side'],
                        'entry_price': open_trade['entry_price'],
                        'exit_price': hit_price,
                        'pnl': pnl,
                        'pnl_ticks': pnl_ticks,
                        'exit_reason': hit_type.upper()
                    }
                    logger.info(f"üéØ {hit_type.upper()} from price update for '{recorder_name}': PnL ${pnl:.2f}")
                    open_trade = None
        
        elif normalized_action == 'CLOSE' or (market_position and market_position.lower() == 'flat'):
            # Close any open trade at current price
            if open_trade:
                if open_trade['side'] == 'LONG':
                    pnl_ticks = (current_price - open_trade['entry_price']) / tick_size
                else:
                    pnl_ticks = (open_trade['entry_price'] - current_price) / tick_size
                
                pnl, _ = close_trade(cursor, open_trade, current_price, pnl_ticks, tick_value, 'signal')
                
                trade_result = {
                    'action': 'closed',
                    'trade_id': open_trade['id'],
                    'side': open_trade['side'],
                    'entry_price': open_trade['entry_price'],
                    'exit_price': current_price,
                    'pnl': pnl,
                    'pnl_ticks': pnl_ticks,
                    'exit_reason': 'SIGNAL'
                }
                logger.info(f"üìä Trade CLOSED by signal for '{recorder_name}': {open_trade['side']} {ticker} | PnL: ${pnl:.2f} ({pnl_ticks:.1f} ticks)")
            
            # Always close any open position in recorder_positions (even if no open trade)
            cursor.execute(f'''
                SELECT id FROM recorder_positions 
                WHERE recorder_id = {placeholder} AND ticker = {placeholder} AND status = 'open'
            ''', (recorder_id, ticker))
            open_pos = cursor.fetchone()
            if open_pos:
                close_recorder_position(cursor, open_pos[0], current_price, ticker)
        
        elif normalized_action == 'BUY':
            # If we have an open SHORT, close it first
            if open_trade and open_trade['side'] == 'SHORT':
                pnl_ticks = (open_trade['entry_price'] - current_price) / tick_size
                pnl, _ = close_trade(cursor, open_trade, current_price, pnl_ticks, tick_value, 'reversal')
                
                logger.info(f"üìä SHORT closed by BUY reversal: ${pnl:.2f}")
                open_trade = None
            
            # Open new LONG trade if no open trade
            if not open_trade:
                # For STRATEGY alerts: NO TP/SL - TradingView strategy manages exits via signals
                # For INDICATOR alerts: Use recorder TP/SL settings
                if is_strategy_alert:
                    tp_price, sl_price = None, None
                    logger.info(f"üìä STRATEGY MODE: No TP/SL - TradingView controls exits")
                else:
                    # Calculate TP/SL prices based on recorder settings
                    tp_price, sl_price = calculate_tp_sl_prices(
                        current_price, 'LONG', tp_ticks, sl_amount if sl_enabled else 0, tick_size
                    )
                
                # Use correct placeholder for database type
                ph = '%s' if is_postgres else '?'
                cursor.execute(f'''
                    INSERT INTO recorded_trades 
                    (recorder_id, signal_id, ticker, action, side, entry_price, entry_time, 
                     quantity, status, tp_price, sl_price, tp_ticks, sl_ticks)
                    VALUES ({ph}, {ph}, {ph}, {ph}, 'LONG', {ph}, CURRENT_TIMESTAMP, {ph}, 'open', {ph}, {ph}, {ph}, {ph})
                ''', (recorder_id, signal_id, ticker, 'BUY', current_price, quantity,
                      tp_price, sl_price, tp_ticks, sl_amount if sl_enabled else None))
                
                if is_postgres:
                    cursor.execute('SELECT lastval()')
                    new_trade_id = cursor.fetchone()[0]
                else:
                    new_trade_id = cursor.lastrowid
                
                # Also update position tracking for combined drawdown
                pos_id, is_new_pos, total_qty = update_recorder_position(
                    cursor, recorder_id, ticker, 'LONG', current_price, quantity
                )
                
                trade_result = {
                    'action': 'opened',
                    'trade_id': new_trade_id,
                    'side': 'LONG',
                    'entry_price': current_price,
                    'quantity': quantity,
                    'tp_price': tp_price,
                    'sl_price': sl_price,
                    'position_id': pos_id,
                    'position_qty': total_qty
                }
                logger.info(f"üìà LONG opened for '{recorder_name}': {ticker} @ {current_price} x{quantity} | TP: {tp_price} | SL: {sl_price} | Position: {total_qty} total")
        
        elif normalized_action == 'SELL':
            # If we have an open LONG, close it first
            if open_trade and open_trade['side'] == 'LONG':
                pnl_ticks = (current_price - open_trade['entry_price']) / tick_size
                pnl, _ = close_trade(cursor, open_trade, current_price, pnl_ticks, tick_value, 'reversal')
                
                logger.info(f"üìä LONG closed by SELL reversal: ${pnl:.2f}")
                open_trade = None
            
            # Open new SHORT trade if no open trade
            if not open_trade:
                # For STRATEGY alerts: NO TP/SL - TradingView strategy manages exits via signals
                # For INDICATOR alerts: Use recorder TP/SL settings
                if is_strategy_alert:
                    tp_price, sl_price = None, None
                    logger.info(f"üìä STRATEGY MODE: No TP/SL - TradingView controls exits")
                else:
                    # Calculate TP/SL prices based on recorder settings
                    tp_price, sl_price = calculate_tp_sl_prices(
                        current_price, 'SHORT', tp_ticks, sl_amount if sl_enabled else 0, tick_size
                    )
                
                # Use correct placeholder for database type
                ph = '%s' if is_postgres else '?'
                cursor.execute(f'''
                    INSERT INTO recorded_trades 
                    (recorder_id, signal_id, ticker, action, side, entry_price, entry_time, 
                     quantity, status, tp_price, sl_price, tp_ticks, sl_ticks)
                    VALUES ({ph}, {ph}, {ph}, {ph}, 'SHORT', {ph}, CURRENT_TIMESTAMP, {ph}, 'open', {ph}, {ph}, {ph}, {ph})
                ''', (recorder_id, signal_id, ticker, 'SELL', current_price, quantity,
                      tp_price, sl_price, tp_ticks, sl_amount if sl_enabled else None))
                
                if is_postgres:
                    cursor.execute('SELECT lastval()')
                    new_trade_id = cursor.fetchone()[0]
                else:
                    new_trade_id = cursor.lastrowid
                
                # Also update position tracking for combined drawdown
                pos_id, is_new_pos, total_qty = update_recorder_position(
                    cursor, recorder_id, ticker, 'SHORT', current_price, quantity
                )
                
                trade_result = {
                    'action': 'opened',
                    'trade_id': new_trade_id,
                    'side': 'SHORT',
                    'entry_price': current_price,
                    'quantity': quantity,
                    'tp_price': tp_price,
                    'sl_price': sl_price,
                    'position_id': pos_id,
                    'position_qty': total_qty
                }
                logger.info(f"üìâ SHORT opened for '{recorder_name}': {ticker} @ {current_price} x{quantity} | TP: {tp_price} | SL: {sl_price} | Position: {total_qty} total")
        
        conn.commit()
        conn.close()
        
        # Emit real-time update via WebSocket
        try:
            socketio.emit('signal_received', {
                'recorder_id': recorder_id,
                'recorder_name': recorder_name,
                'signal_id': signal_id,
                'action': normalized_action,
                'ticker': ticker,
                'price': price,
                'position_size': position_size,
                'signal_type': 'strategy' if is_strategy_alert else 'alert',
                'timestamp': datetime.now().isoformat(),
                'trade': trade_result
            })
            
            # If a trade was closed, also emit trade_executed event for dashboard
            if trade_result and trade_result.get('action') == 'closed':
                socketio.emit('trade_executed', {
                    'recorder_id': recorder_id,
                    'recorder_name': recorder_name,
                    'trade_id': trade_result['trade_id'],
                    'side': trade_result['side'],
                    'pnl': trade_result['pnl'],
                    'ticker': ticker,
                    'timestamp': datetime.now().isoformat()
                })
        except Exception as e:
            logger.warning(f"Could not emit WebSocket update: {e}")
        
        logger.info(f"‚úÖ Signal recorded for '{recorder_name}': {normalized_action} {ticker} @ {price}")
        
        # Build response based on alert type
        response = {
            'success': True,
            'message': f'Signal received and recorded',
            'signal_id': signal_id,
            'recorder': recorder_name,
            'action': normalized_action,
            'ticker': ticker,
            'price': price,
            'signal_number': signal_count
        }
        
        # Add trade information to response
        if trade_result:
            response['trade'] = trade_result
            if trade_result.get('action') == 'closed':
                response['message'] = f"Trade closed with PnL: ${trade_result['pnl']:.2f}"
            elif trade_result.get('action') == 'opened':
                response['message'] = f"{trade_result['side']} position opened @ {trade_result['entry_price']}"
        
        if is_strategy_alert:
            response['signal_type'] = 'strategy'
            response['position_size'] = position_size
            response['market_position'] = market_position
            response['note'] = 'Strategy alert - TradingView manages position sizing'
        else:
            response['signal_type'] = 'alert'
            response['note'] = 'Simple alert - Recorder settings control sizing/risk'
            response['recorder_settings'] = {
                'initial_position_size': recorder.get('initial_position_size'),
                'tp_enabled': bool(recorder.get('tp_targets')),
                'sl_enabled': bool(recorder.get('sl_enabled'))
            }
        
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"Error processing webhook: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500

# ============================================================
# END OF DISABLED LEGACY WEBHOOK CODE
# ============================================================

@app.route('/api/recorders/<int:recorder_id>/signals', methods=['GET'])
def api_get_recorder_signals(recorder_id):
    """Get recorded signals for a recorder"""
    try:
        limit = int(request.args.get('limit', 50))
        
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM recorded_signals 
            WHERE recorder_id = ? 
            ORDER BY created_at DESC 
            LIMIT ?
        ''', (recorder_id, limit))
        
        signals = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        return jsonify({
            'success': True,
            'signals': signals,
            'count': len(signals)
        })
    except Exception as e:
        logger.error(f"Error getting signals: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/recorders/<int:recorder_id>/trades', methods=['GET'])
def api_get_recorder_trades(recorder_id):
    """Get recorded trades for a recorder with pagination and filters"""
    try:
        page = int(request.args.get('page', 1))
        per_page = int(request.args.get('per_page', 20))
        status = request.args.get('status')  # open, closed, or all
        
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Build query with filters
        where_clauses = ['recorder_id = ?']
        params = [recorder_id]
        
        if status and status != 'all':
            where_clauses.append('status = ?')
            params.append(status)
        
        where_sql = ' AND '.join(where_clauses)
        
        # Get total count
        cursor.execute(f'SELECT COUNT(*) FROM recorded_trades WHERE {where_sql}', params)
        total = cursor.fetchone()[0]
        
        # Get paginated trades
        offset = (page - 1) * per_page
        cursor.execute(f'''
            SELECT * FROM recorded_trades 
            WHERE {where_sql}
            ORDER BY entry_time DESC 
            LIMIT ? OFFSET ?
        ''', params + [per_page, offset])
        
        trades = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        return jsonify({
            'success': True,
            'trades': trades,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total,
                'total_pages': (total + per_page - 1) // per_page,
                'has_prev': page > 1,
                'has_next': page * per_page < total
            }
        })
    except Exception as e:
        logger.error(f"Error getting trades: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/recorders/<int:recorder_id>/pnl', methods=['GET'])
def api_get_recorder_pnl(recorder_id):
    """Get PnL summary and history for a recorder"""
    try:
        timeframe = request.args.get('timeframe', 'all')  # today, week, month, 3months, 6months, year, all
        
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        placeholder = '%s' if is_postgres else '?'
        
        # Build date filter (PostgreSQL vs SQLite syntax)
        date_filter = ''
        if is_postgres:
            if timeframe == 'today':
                date_filter = "AND DATE(exit_time) = CURRENT_DATE"
            elif timeframe == 'week':
                date_filter = "AND exit_time >= NOW() - INTERVAL '7 days'"
            elif timeframe == 'month':
                date_filter = "AND exit_time >= NOW() - INTERVAL '30 days'"
            elif timeframe == '3months':
                date_filter = "AND exit_time >= NOW() - INTERVAL '90 days'"
            elif timeframe == '6months':
                date_filter = "AND exit_time >= NOW() - INTERVAL '180 days'"
            elif timeframe == 'year':
                date_filter = "AND exit_time >= NOW() - INTERVAL '365 days'"
        else:
            if timeframe == 'today':
                date_filter = "AND DATE(exit_time) = DATE('now')"
            elif timeframe == 'week':
                date_filter = "AND exit_time >= DATE('now', '-7 days')"
            elif timeframe == 'month':
                date_filter = "AND exit_time >= DATE('now', '-30 days')"
            elif timeframe == '3months':
                date_filter = "AND exit_time >= DATE('now', '-90 days')"
            elif timeframe == '6months':
                date_filter = "AND exit_time >= DATE('now', '-180 days')"
            elif timeframe == 'year':
                date_filter = "AND exit_time >= DATE('now', '-365 days')"
        
        # Get summary stats for closed trades
        cursor.execute(f'''
            SELECT 
                COUNT(*) as total_trades,
                SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as wins,
                SUM(CASE WHEN pnl < 0 THEN 1 ELSE 0 END) as losses,
                SUM(pnl) as total_pnl,
                SUM(pnl_ticks) as total_ticks,
                AVG(pnl) as avg_pnl,
                MAX(pnl) as max_profit,
                MIN(pnl) as max_loss,
                AVG(CASE WHEN pnl > 0 THEN pnl END) as avg_win,
                AVG(CASE WHEN pnl < 0 THEN pnl END) as avg_loss
            FROM recorded_trades 
            WHERE recorder_id = {placeholder} AND status = 'closed' {date_filter}
        ''', (recorder_id,))
        
        stats = dict(cursor.fetchone())
        
        # Calculate win rate and profit factor
        wins = stats.get('wins') or 0
        losses = stats.get('losses') or 0
        total = wins + losses
        stats['win_rate'] = round((wins / total * 100), 1) if total > 0 else 0
        
        avg_win = abs(stats.get('avg_win') or 0)
        avg_loss = abs(stats.get('avg_loss') or 1)
        stats['profit_factor'] = round(avg_win / avg_loss, 2) if avg_loss > 0 else 0
        
        # Get daily PnL for charting
        cursor.execute(f'''
            SELECT 
                DATE(exit_time) as date,
                SUM(pnl) as daily_pnl,
                COUNT(*) as trade_count,
                SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as daily_wins,
                SUM(CASE WHEN pnl < 0 THEN 1 ELSE 0 END) as daily_losses
            FROM recorded_trades 
            WHERE recorder_id = {placeholder} AND status = 'closed' {date_filter}
            GROUP BY DATE(exit_time)
            ORDER BY DATE(exit_time) ASC
        ''', (recorder_id,))
        
        daily_pnl = [dict(row) for row in cursor.fetchall()]
        
        # Calculate cumulative PnL for chart
        cumulative = 0
        max_cumulative = 0
        max_drawdown = 0
        
        for day in daily_pnl:
            cumulative += day['daily_pnl'] or 0
            day['cumulative_pnl'] = cumulative
            
            if cumulative > max_cumulative:
                max_cumulative = cumulative
            
            drawdown = max_cumulative - cumulative
            if drawdown > max_drawdown:
                max_drawdown = drawdown
            day['drawdown'] = drawdown
        
        stats['max_drawdown'] = max_drawdown
        
        # Get open position if any
        cursor.execute(f'''
            SELECT * FROM recorded_trades
            WHERE recorder_id = {placeholder} AND status = 'open'
            ORDER BY entry_time DESC LIMIT 1
        ''', (recorder_id,))
        
        open_trade = cursor.fetchone()
        if open_trade:
            stats['open_position'] = dict(open_trade)
        
        # Get recorder name
        cursor.execute(f'SELECT name FROM recorders WHERE id = {placeholder}', (recorder_id,))
        recorder = cursor.fetchone()
        stats['recorder_name'] = recorder['name'] if recorder else f'Recorder {recorder_id}'
        
        conn.close()
        
        return jsonify({
            'success': True,
            'summary': stats,
            'daily_pnl': daily_pnl,
            'chart_data': {
                'labels': [d['date'] for d in daily_pnl],
                'profit': [d['cumulative_pnl'] for d in daily_pnl],
                'drawdown': [d['drawdown'] for d in daily_pnl]
            }
        })
    except Exception as e:
        logger.error(f"Error getting PnL: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/recorders/all/pnl', methods=['GET'])
def api_get_all_recorders_pnl():
    """Get aggregated PnL for all recorders (for dashboard)"""
    try:
        timeframe = request.args.get('timeframe', 'month')
        recorder_id = request.args.get('recorder_id')  # Optional filter
        
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        placeholder = '%s' if is_postgres else '?'
        
        # Build date filter (PostgreSQL vs SQLite syntax)
        date_filter = ''
        if is_postgres:
            if timeframe == 'today':
                date_filter = "AND DATE(rt.exit_time) = CURRENT_DATE"
            elif timeframe == 'week':
                date_filter = "AND rt.exit_time >= NOW() - INTERVAL '7 days'"
            elif timeframe == 'month':
                date_filter = "AND rt.exit_time >= NOW() - INTERVAL '30 days'"
            elif timeframe == '3months':
                date_filter = "AND rt.exit_time >= NOW() - INTERVAL '90 days'"
            elif timeframe == '6months':
                date_filter = "AND rt.exit_time >= NOW() - INTERVAL '180 days'"
            elif timeframe == 'year':
                date_filter = "AND rt.exit_time >= NOW() - INTERVAL '365 days'"
        else:
            if timeframe == 'today':
                date_filter = "AND DATE(rt.exit_time) = DATE('now')"
            elif timeframe == 'week':
                date_filter = "AND rt.exit_time >= DATE('now', '-7 days')"
            elif timeframe == 'month':
                date_filter = "AND rt.exit_time >= DATE('now', '-30 days')"
            elif timeframe == '3months':
                date_filter = "AND rt.exit_time >= DATE('now', '-90 days')"
            elif timeframe == '6months':
                date_filter = "AND rt.exit_time >= DATE('now', '-180 days')"
            elif timeframe == 'year':
                date_filter = "AND rt.exit_time >= DATE('now', '-365 days')"
        
        recorder_filter = ''
        params = []
        if recorder_id:
            recorder_filter = f'AND rt.recorder_id = {placeholder}'
            params.append(int(recorder_id))
        
        # Get per-recorder summary
        cursor.execute(f'''
            SELECT 
                r.id as recorder_id,
                r.name as recorder_name,
                r.symbol,
                COUNT(rt.id) as total_trades,
                SUM(CASE WHEN rt.pnl > 0 THEN 1 ELSE 0 END) as wins,
                SUM(CASE WHEN rt.pnl < 0 THEN 1 ELSE 0 END) as losses,
                SUM(rt.pnl) as total_pnl,
                AVG(rt.pnl) as avg_pnl
            FROM recorders r
            LEFT JOIN recorded_trades rt ON r.id = rt.recorder_id AND rt.status = 'closed' {date_filter}
            WHERE 1=1 {recorder_filter}
            GROUP BY r.id
            ORDER BY total_pnl DESC
        ''', params)
        
        recorders = []
        for row in cursor.fetchall():
            rec = dict(row)
            wins = rec['wins'] or 0
            losses = rec['losses'] or 0
            total = wins + losses
            rec['win_rate'] = round((wins / total * 100), 1) if total > 0 else 0
            recorders.append(rec)
        
        # Get daily aggregate PnL for chart
        cursor.execute(f'''
            SELECT 
                DATE(rt.exit_time) as date,
                SUM(rt.pnl) as daily_pnl,
                COUNT(rt.id) as trade_count
            FROM recorded_trades rt
            WHERE rt.status = 'closed' {date_filter} {recorder_filter.replace('rt.recorder_id', 'rt.recorder_id')}
            GROUP BY DATE(rt.exit_time)
            ORDER BY DATE(rt.exit_time) ASC
        ''', params)
        
        daily_data = [dict(row) for row in cursor.fetchall()]
        
        # Calculate cumulative
        cumulative = 0
        max_cumulative = 0
        for day in daily_data:
            cumulative += day['daily_pnl'] or 0
            day['cumulative_pnl'] = cumulative
            if cumulative > max_cumulative:
                max_cumulative = cumulative
            day['drawdown'] = max_cumulative - cumulative
        
        conn.close()
        
        return jsonify({
            'success': True,
            'recorders': recorders,
            'chart_data': {
                'labels': [d['date'] for d in daily_data],
                'profit': [d['cumulative_pnl'] for d in daily_data],
                'drawdown': [d['drawdown'] for d in daily_data]
            },
            'total_pnl': sum(r['total_pnl'] or 0 for r in recorders),
            'total_trades': sum(r['total_trades'] or 0 for r in recorders)
        })
    except Exception as e:
        logger.error(f"Error getting all recorders PnL: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/traders')
@feature_required('auto_trader')
def traders_list():
    """Traders list page - show all recorder-account links"""
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get current user ID for filtering
        user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            user_id = get_current_user_id()
        
        # Filter by user_id if logged in - show only traders that belong to this user
        # Either: trader.user_id matches OR account.user_id matches (for legacy traders)
        if user_id:
            if is_postgres:
                cursor.execute('''
                    SELECT t.id, t.enabled, t.subaccount_name, t.is_demo,
                           t.recorder_id,
                           r.name as recorder_name,
                           a.name as account_name
                    FROM traders t
                    LEFT JOIN recorders r ON t.recorder_id = r.id
                    LEFT JOIN accounts a ON t.account_id = a.id
                    WHERE t.user_id = %s OR a.user_id = %s
                    ORDER BY r.name, t.created_at DESC
                ''', (user_id, user_id))
            else:
                cursor.execute('''
                    SELECT t.id, t.enabled, t.subaccount_name, t.is_demo,
                           t.recorder_id,
                           r.name as recorder_name,
                           a.name as account_name
                    FROM traders t
                    LEFT JOIN recorders r ON t.recorder_id = r.id
                    LEFT JOIN accounts a ON t.account_id = a.id
                    WHERE t.user_id = ? OR a.user_id = ?
                    ORDER BY r.name, t.created_at DESC
                ''', (user_id, user_id))
        else:
            # Fallback: show all (shouldn't happen if login required)
            cursor.execute('''
                SELECT t.id, t.enabled, t.subaccount_name, t.is_demo,
                       t.recorder_id,
                       r.name as recorder_name,
                       a.name as account_name
                FROM traders t
                LEFT JOIN recorders r ON t.recorder_id = r.id
                LEFT JOIN accounts a ON t.account_id = a.id
                ORDER BY r.name, t.created_at DESC
            ''')

        rows = cursor.fetchall()
        columns = ['id', 'enabled', 'subaccount_name', 'is_demo', 'recorder_id', 'recorder_name', 'account_name']

        # Group traders by recorder/strategy
        strategies = {}  # recorder_id -> {name, accounts: []}

        for row in rows:
            if hasattr(row, 'keys'):
                row_dict = dict(row)
            else:
                row_dict = dict(zip(columns[:len(row)], row))

            recorder_id = row_dict.get('recorder_id')
            recorder_name = row_dict.get('recorder_name') or 'Unknown'

            is_demo = bool(row_dict.get('is_demo')) if row_dict.get('is_demo') is not None else None
            env_label = "DEMO" if is_demo else "LIVE" if is_demo is not None else ""
            account_name = row_dict.get('account_name') or 'Unknown'
            subaccount_name = row_dict.get('subaccount_name')

            if subaccount_name:
                display_account = f"{account_name} ({subaccount_name})"
            else:
                display_account = account_name

            # Create strategy entry if doesn't exist
            if recorder_id not in strategies:
                strategies[recorder_id] = {
                    'recorder_id': recorder_id,
                    'recorder_name': recorder_name,
                    'accounts': [],
                    'total_accounts': 0,
                    'live_accounts': 0
                }

            # Add account to strategy
            is_enabled = bool(row_dict.get('enabled'))
            strategies[recorder_id]['accounts'].append({
                'trader_id': row_dict.get('id'),
                'account_name': display_account,
                'env_label': env_label,
                'is_demo': is_demo,
                'enabled': is_enabled
            })
            strategies[recorder_id]['total_accounts'] += 1
            if is_enabled:
                strategies[recorder_id]['live_accounts'] += 1

        # Convert to list sorted by recorder name
        strategies_list = sorted(strategies.values(), key=lambda x: x['recorder_name'].lower())

        conn.close()

        return render_template(
            'traders.html',
            mode='list',
            strategies=strategies_list
        )
    except Exception as e:
        logger.error(f"Error in traders_list: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return f"<h1>Error loading traders</h1><pre>{str(e)}</pre>", 500

@app.route('/my-traders')
def my_traders():
    """My Traders page - link recorders to accounts"""
    return render_template('my_traders_tab.html')

@app.route('/traders/new/debug')
def traders_new_debug():
    """Debug endpoint to test traders/new logic without template"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Get recorders
        cursor.execute('SELECT id, name, strategy_type FROM recorders ORDER BY name')
        recorders = cursor.fetchall()
        
        # Get accounts - use PostgreSQL compatible query
        is_postgres = is_using_postgres()
        if is_postgres:
            cursor.execute('SELECT id, name, tradovate_accounts FROM accounts WHERE enabled = true')
        else:
            cursor.execute('SELECT id, name, tradovate_accounts FROM accounts WHERE enabled = 1')
        accounts = cursor.fetchall()
        
        conn.close()
        
        return jsonify({
            'success': True,
            'is_postgres': is_postgres,
            'recorders_count': len(recorders) if recorders else 0,
            'accounts_count': len(accounts) if accounts else 0,
            'deploy_time': '2025-12-19T01:15:00Z'
        })
    except Exception as e:
        import traceback
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@app.route('/traders/new')
def traders_new():
    """Create new trader page - select recorder and accounts"""
    # CRITICAL SECURITY: Require login - never show accounts without authentication
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        # Get current user for filtering - REQUIRED
        user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            user_id = get_current_user_id()
        
        # SECURITY: If no user_id, show nothing (require login)
        if not user_id:
            logger.warning("traders_new: No user_id - redirecting or showing empty list")
            conn.close()
            if USER_AUTH_AVAILABLE:
                return redirect(url_for('login'))
            # If auth not available, show empty accounts list (legacy mode)
            return render_template(
                'traders.html',
                mode='builder',
                header_title='Create New Trader',
                header_cta='Create Trader',
                recorders=[],
                accounts=[]
            )
        
        # Get recorders for the dropdown:
        # Filter by privacy (is_private) and tier (required_tier)
        # Always show the user's own recorders regardless of privacy/tier
        # Get user's tier for filtering
        user_tier = 'none'
        if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
            try:
                from subscription_models import get_user_plan_tier
                user_tier = get_user_plan_tier(user_id) or 'none'
            except:
                user_tier = 'none'

        # Check if current user is admin ‚Äî admins see everything
        is_admin_user = False
        if USER_AUTH_AVAILABLE and is_logged_in():
            user = get_current_user()
            if user and user.is_admin:
                is_admin_user = True

        # Tier hierarchy: public=0, basic=1, premium=2, elite=3
        tier_rank = {'none': 0, 'public': 0, 'basic': 1, 'premium': 2, 'elite': 3}
        user_tier_rank = tier_rank.get(user_tier, 0)

        cursor.execute('SELECT id, name, strategy_type, user_id, is_private, required_tier FROM recorders ORDER BY name')
        recorders = []
        for row in cursor.fetchall():
            # Handle both dict-style and tuple-style rows
            if hasattr(row, 'get'):
                r_id = row.get('id')
                r_name = row.get('name')
                r_strategy_type = row.get('strategy_type', 'Futures')
                r_user_id = row.get('user_id')
                r_is_private = bool(row.get('is_private'))
                r_required_tier = row.get('required_tier') or 'public'
            else:
                r_id = row[0]
                r_name = row[1]
                r_strategy_type = row[2] if len(row) > 2 else 'Futures'
                r_user_id = row[3] if len(row) > 3 else None
                r_is_private = bool(row[4]) if len(row) > 4 else False
                r_required_tier = (row[5] if len(row) > 5 else None) or 'public'

            # Always show user's own recorders
            if r_user_id is not None and user_id is not None and int(r_user_id) == int(user_id):
                recorders.append({'id': r_id, 'name': r_name, 'strategy_type': r_strategy_type})
                continue

            # Admins see everything
            if is_admin_user:
                recorders.append({'id': r_id, 'name': r_name, 'strategy_type': r_strategy_type})
                continue

            # Skip private recorders from other users
            if r_is_private:
                continue

            # Check tier requirement
            required_rank = tier_rank.get(r_required_tier, 0)
            if user_tier_rank >= required_rank:
                recorders.append({'id': r_id, 'name': r_name, 'strategy_type': r_strategy_type})
        
        # CRITICAL SECURITY: Get accounts with their tradovate subaccounts - ONLY user's own accounts
        # NEVER show accounts without user_id filter
        if is_postgres:
            cursor.execute('SELECT id, name, tradovate_accounts FROM accounts WHERE enabled = true AND user_id = %s', (user_id,))
        else:
            cursor.execute('SELECT id, name, tradovate_accounts FROM accounts WHERE enabled = 1 AND user_id = ?', (user_id,))
        accounts = []
        for row in cursor.fetchall():
            # Handle both dict-style and tuple-style rows
            if hasattr(row, 'get'):
                parent_id = row.get('id')
                parent_name = row.get('name')
                tradovate_json = row.get('tradovate_accounts')
            else:
                parent_id = row['id'] if hasattr(row, '__getitem__') else row[0]
                parent_name = row['name'] if hasattr(row, '__getitem__') else row[1]
                tradovate_json = row['tradovate_accounts'] if hasattr(row, '__getitem__') else (row[2] if len(row) > 2 else None)
            
            # Parse tradovate_accounts JSON if available
            if tradovate_json:
                try:
                    tradovate_accts = json.loads(tradovate_json)
                    for acct in tradovate_accts:
                        if isinstance(acct, dict) and acct.get('name'):
                            env_label = "üü† DEMO" if acct.get('is_demo') else "üü¢ LIVE"
                            accounts.append({
                                'id': parent_id,
                                'name': acct['name'],
                                'display_name': f"{env_label} - {acct['name']} ({parent_name})",
                                'subaccount_id': acct.get('id'),
                                'is_demo': acct.get('is_demo', False)
                            })
                except:
                    pass
            # Fallback to account name if no tradovate_accounts parsed
            if not accounts or (accounts and accounts[-1].get('id') != parent_id):
                accounts.append({
                    'id': parent_id,
                    'name': parent_name,
                    'display_name': parent_name,
                    'subaccount_id': None,
                    'is_demo': False
                })
        
        conn.close()
        
        # Debug: Log what we're passing to template
        logger.info(f"traders_new: {len(recorders)} recorders, {len(accounts)} accounts")
        
        try:
            return render_template(
                'traders.html',
                mode='builder',
                header_title='Create New Trader',
                header_cta='Create Trader',
                recorders=recorders,
                accounts=accounts
            )
        except Exception as template_err:
            logger.error(f"Template rendering error in traders_new: {template_err}")
            import traceback
            logger.error(traceback.format_exc())
            return f"<h1>Template Error</h1><pre>{str(template_err)}</pre>", 500
    except Exception as e:
        logger.error(f"Error in traders_new: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return f"<h1>Error loading page</h1><pre>{str(e)}</pre>", 500

@app.route('/traders/<int:trader_id>')
def traders_edit(trader_id):
    """Edit existing trader - load all saved settings"""
    # Require login
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    
    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        # Only set row_factory for SQLite - PostgreSQL already uses RealDictCursor
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get current user ID for ownership check
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        # Get the trader with its recorder info (CRITICAL: include all recorder settings for risk management)
        placeholder = '%s' if is_postgres else '?'
        cursor.execute(f'''
            SELECT t.*, 
                   r.name as recorder_name, r.strategy_type, r.symbol, r.webhook_token,
                   r.initial_position_size as r_initial_position_size,
                   r.add_position_size as r_add_position_size,
                   r.tp_targets as r_tp_targets,
                   r.tp_units as r_tp_units,
                   r.trim_units as r_trim_units,
                   r.sl_enabled as r_sl_enabled,
                   r.sl_amount as r_sl_amount,
                   r.sl_units as r_sl_units,
                   r.sl_type as r_sl_type,
                   r.break_even_enabled as r_break_even_enabled,
                   r.break_even_ticks as r_break_even_ticks,
                   r.break_even_offset as r_break_even_offset,
                   r.trail_trigger as r_trail_trigger,
                   r.trail_freq as r_trail_freq,
                   r.avg_down_enabled as r_avg_down_enabled,
                   r.avg_down_amount as r_avg_down_amount,
                   r.avg_down_point as r_avg_down_point,
                   r.avg_down_units as r_avg_down_units,
                   r.time_filter_1_enabled as r_time_filter_1_enabled,
                   r.time_filter_1_start as r_time_filter_1_start,
                   r.time_filter_1_stop as r_time_filter_1_stop,
                   r.time_filter_2_enabled as r_time_filter_2_enabled,
                   r.time_filter_2_start as r_time_filter_2_start,
                   r.time_filter_2_stop as r_time_filter_2_stop,
                   r.inverse_signals as r_inverse_signals,
                   r.is_private as recorder_is_private,
                   r.user_id as recorder_user_id,
                   r.required_tier as recorder_required_tier,
                   a.name as account_name, a.id as parent_account_id
            FROM traders t
            JOIN recorders r ON t.recorder_id = r.id
            JOIN accounts a ON t.account_id = a.id
            WHERE t.id = {placeholder}
        ''', (trader_id,))
        trader_row = cursor.fetchone()
        
        if not trader_row:
            conn.close()
            logger.warning(f"Trader {trader_id} not found")
            return redirect('/traders')
        
        # Check ownership - only allow editing own traders
        trader_user_id = trader_row.get('user_id') if hasattr(trader_row, 'get') else (trader_row['user_id'] if 'user_id' in (trader_row.keys() if hasattr(trader_row, 'keys') else []) else None)
        if current_user_id and trader_user_id and trader_user_id != current_user_id:
            conn.close()
            logger.warning(f"User {current_user_id} tried to edit trader {trader_id} owned by user {trader_user_id}")
            return redirect('/traders')
        
        # Parse TP targets from JSON (prefer trader settings over recorder)
        tp_targets = []
        tp_value = 0  # Default to 0 (no TP) if not set
        tp_trim = 100
        try:
            # Prefer trader's own tp_targets over recorder's (user may have customized)
            tp_targets_raw = trader_row['tp_targets'] or trader_row['r_tp_targets']
            if tp_targets_raw:
                tp_targets = json.loads(tp_targets_raw)
                if tp_targets and len(tp_targets) > 0:
                    # Handle both 'ticks' (new format) and 'value' (old format)
                    tp_val = tp_targets[0].get('ticks')
                    if tp_val is None:
                        tp_val = tp_targets[0].get('value')
                    tp_value = tp_val if tp_val is not None else 0
                    tp_trim = tp_targets[0].get('trim', 100)
        except:
            tp_targets = []
        
        # Build trader object: use TRADER values first, fall back to RECORDER defaults
        # Helper: get trader value, fall back to recorder value (r_ prefix), then default
        def _t(field, r_field=None, default=None):
            """Get trader field, fall back to recorder, then default."""
            val = trader_row.get(field)
            if val is not None and val != '' and val != '[]':
                return val
            if r_field:
                rval = trader_row.get(r_field)
                if rval is not None and rval != '' and rval != '[]':
                    return rval
            return default

        trader = {
        'id': trader_row['id'],
        'recorder_id': trader_row['recorder_id'],
        'recorder_name': trader_row['recorder_name'],
        'strategy_type': trader_row['strategy_type'] or 'Futures',
        'account_id': trader_row['account_id'],
        'account_name': trader_row['account_name'],
        'subaccount_id': trader_row['subaccount_id'],
        'subaccount_name': trader_row['subaccount_name'],
        'is_demo': bool(trader_row['is_demo']),
        'enabled': bool(trader_row['enabled']),
        # Position sizes
        'initial_position_size': _t('initial_position_size', 'r_initial_position_size', 1),
        'add_position_size': _t('add_position_size', 'r_add_position_size', 1),
        # TP settings ‚Äî trader first, recorder fallback
        'tp_targets': tp_targets,
        'tp_value': tp_value,
        'tp_trim': tp_trim,
        'tp_units': _t('tp_units', 'r_tp_units', 'Ticks'),
        'trim_units': _t('trim_units', 'r_trim_units', 'Contracts'),
        # SL settings ‚Äî trader first, recorder fallback
        'sl_enabled': bool(_t('sl_enabled', 'r_sl_enabled', 0)),
        'sl_amount': float(_t('sl_amount', 'r_sl_amount', 0) or 0),
        'sl_units': _t('sl_units', 'r_sl_units', 'Ticks'),
        'sl_type': _t('sl_type', 'r_sl_type', 'Fixed'),
        # Break-Even ‚Äî trader first, recorder fallback
        'break_even_enabled': bool(_t('break_even_enabled', 'r_break_even_enabled', 0)),
        'break_even_ticks': int(_t('break_even_ticks', 'r_break_even_ticks', 10) or 10),
        'break_even_offset': int(_t('break_even_offset', 'r_break_even_offset', 0) or 0),
        # Trail settings ‚Äî trader first, recorder fallback
        'trail_trigger': int(_t('trail_trigger', 'r_trail_trigger', 0) or 0),
        'trail_freq': int(_t('trail_freq', 'r_trail_freq', 0) or 0),
        # DCA/Averaging Down ‚Äî trader first, recorder fallback
        'avg_down_enabled': bool(_t('avg_down_enabled', 'r_avg_down_enabled', 0)),
        'avg_down_amount': int(_t('avg_down_amount', 'r_avg_down_amount', 1) or 1),
        'avg_down_point': float(_t('avg_down_point', 'r_avg_down_point', 10) or 10),
        'avg_down_units': _t('avg_down_units', 'r_avg_down_units', 'Ticks'),
        'dca_enabled': bool(_t('dca_enabled', 'r_dca_enabled', 0)),
        # Risk management ‚Äî trader values (these are per-trader settings)
        'max_daily_loss': float(trader_row.get('max_daily_loss') or 0),
        'signal_cooldown': int(trader_row.get('signal_cooldown') or 0),
        'max_signals_per_session': int(trader_row.get('max_signals_per_session') or 0),
        'add_delay': int(trader_row.get('add_delay') or 1),
        'auto_flat_after_cutoff': bool(trader_row.get('auto_flat_after_cutoff') or 0),
        'inverse_signals': bool(trader_row.get('inverse_signals') or 0),
        # Time filters ‚Äî trader first, recorder fallback
        'time_filter_1_enabled': bool(_t('time_filter_1_enabled', 'r_time_filter_1_enabled', 0)),
        'time_filter_1_start': _t('time_filter_1_start', 'r_time_filter_1_start', ''),
        'time_filter_1_stop': _t('time_filter_1_stop', 'r_time_filter_1_stop', ''),
        'time_filter_2_enabled': bool(_t('time_filter_2_enabled', 'r_time_filter_2_enabled', 0)),
        'time_filter_2_start': _t('time_filter_2_start', 'r_time_filter_2_start', ''),
        'time_filter_2_stop': _t('time_filter_2_stop', 'r_time_filter_2_stop', ''),
        }
        
        # Get enabled accounts from routing (if stored)
        enabled_accounts = []
        try:
            # sqlite3.Row uses dict-style access, not .get()
            enabled_accounts_raw = trader_row['enabled_accounts'] if 'enabled_accounts' in trader_row.keys() else None
            logger.info(f"üîç [LOAD] Trader {trader_id} - enabled_accounts_raw type: {type(enabled_accounts_raw)}, value: {str(enabled_accounts_raw)[:200] if enabled_accounts_raw else 'None/Empty'}")
            if enabled_accounts_raw and enabled_accounts_raw != '[]' and str(enabled_accounts_raw).strip():
                enabled_accounts = json.loads(enabled_accounts_raw) if isinstance(enabled_accounts_raw, str) else enabled_accounts_raw
                logger.info(f"‚úÖ Loaded enabled_accounts for trader {trader_id}: {len(enabled_accounts)} accounts")
                for idx, acct in enumerate(enabled_accounts):
                    multiplier = acct.get('multiplier', 1.0)
                    subaccount_id = acct.get('subaccount_id')
                    account_id = acct.get('account_id')
                    account_name = acct.get('account_name', 'Unknown')
                    logger.info(f"  [{idx}] Enabled: {account_name} (account_id={account_id}, subaccount_id={subaccount_id}, multiplier={multiplier}, type={type(multiplier)})")
            else:
                logger.info(f"‚ö†Ô∏è No enabled_accounts found for trader {trader_id} (raw value: '{enabled_accounts_raw}')")
        except Exception as e:
            logger.warning(f"‚ùå Error parsing enabled_accounts: {e}")
            import traceback
            logger.warning(traceback.format_exc())
            enabled_accounts = []
        
        # CRITICAL SECURITY: Get all accounts with subaccounts for the routing table - ONLY user's accounts
        # NEVER show accounts without user_id filter
        if not current_user_id:
            logger.warning(f"traders_edit: No current_user_id for trader {trader_id} - showing empty accounts list")
            accounts = []
        else:
            if is_postgres:
                cursor.execute('SELECT id, name, tradovate_accounts FROM accounts WHERE enabled = true AND user_id = %s', (current_user_id,))
            else:
                cursor.execute('SELECT id, name, tradovate_accounts FROM accounts WHERE enabled = 1 AND user_id = ?', (current_user_id,))
        accounts = []
        for row in cursor.fetchall():
            parent_id = row['id']
            parent_name = row['name']
            if row['tradovate_accounts']:
                try:
                    tradovate_accts = json.loads(row['tradovate_accounts'])
                    for acct in tradovate_accts:
                        if isinstance(acct, dict) and acct.get('name'):
                            env_label = "üü† DEMO" if acct.get('is_demo') else "üü¢ LIVE"
                            # Check if this subaccount is enabled in account routing
                            is_enabled = False
                            multiplier = 1.0  # Default multiplier
                            max_contracts = 0  # Default max contracts
                            custom_ticker = ''  # Default custom ticker
                            acct_subaccount_id = acct.get('id')  # This is the subaccount ID from tradovate_accounts
                            acct_name = acct.get('name', '')
                            
                            # CRITICAL: Only match accounts that are EXPLICITLY in enabled_accounts
                            # NO FALLBACKS - if it doesn't match exactly, it's NOT enabled
                            logger.debug(f"  üîç [MATCH] Checking account '{acct_name}' (subaccount_id={acct_subaccount_id}, parent_id={parent_id}) against {len(enabled_accounts)} enabled accounts")
                            for idx, enabled_acct in enumerate(enabled_accounts):
                                enabled_subaccount_id = enabled_acct.get('subaccount_id')
                                enabled_account_id = enabled_acct.get('account_id')
                                enabled_account_name = enabled_acct.get('account_name', '')
                                
                                matched = False
                                
                                # Strategy 1: Match by subaccount_id (EXACT match only)
                                if acct_subaccount_id and enabled_subaccount_id:
                                    try:
                                        if int(acct_subaccount_id) == int(enabled_subaccount_id):
                                            matched = True
                                            logger.info(f"  ‚úì [{idx}] Matched '{acct_name}' by subaccount_id: {acct_subaccount_id} == {enabled_subaccount_id}")
                                    except (ValueError, TypeError):
                                        pass
                                
                                # Strategy 2: Fallback - match by account_id + account_name (EXACT match only - no case-insensitive, no trimming)
                                # This is only used if subaccount_id matching fails
                                if not matched and enabled_account_id and parent_id and enabled_account_name and acct_name:
                                    try:
                                        if int(enabled_account_id) == int(parent_id) and str(enabled_account_name).strip() == str(acct_name).strip():
                                            matched = True
                                            logger.info(f"  ‚úì [{idx}] Matched '{acct_name}' by account_id+name EXACT (account_id={parent_id}, name='{acct_name}')")
                                    except (ValueError, TypeError):
                                        pass
                                
                                # If matched, extract settings for THIS account only
                                if matched:
                                    is_enabled = True
                                    multiplier_raw = enabled_acct.get('multiplier', 1.0)
                                    multiplier = float(multiplier_raw) if multiplier_raw else 1.0
                                    max_contracts = int(enabled_acct.get('max_contracts', 0) or 0)
                                    custom_ticker = str(enabled_acct.get('custom_ticker', '') or '')
                                    logger.info(f"  ‚úÖ Account '{acct_name}' ENABLED with multiplier={multiplier}, max_contracts={max_contracts}")
                                    break
                            
                            if not is_enabled and enabled_accounts:
                                logger.warning(f"  ‚ùå Account {acct_name} (subaccount_id={acct_subaccount_id}, parent_id={parent_id}) NOT found in enabled_accounts - will default to multiplier=1.0")
                                logger.warning(f"     Available enabled accounts: {[(a.get('account_name'), a.get('subaccount_id'), a.get('account_id'), a.get('multiplier')) for a in enabled_accounts]}")
                            # If no enabled_accounts stored, use legacy: check if it's the primary account
                            if not enabled_accounts:
                                is_enabled = (acct_subaccount_id == trader['subaccount_id'])
                                logger.debug(f"  - Using legacy check: {acct.get('name')} is_selected={is_enabled}")
                            accounts.append({
                                'id': parent_id,
                                'name': acct['name'],
                                'display_name': f"{env_label} - {acct['name']} ({parent_name})",
                                'subaccount_id': acct.get('id'),
                                'is_demo': acct.get('is_demo', False),
                                'is_selected': is_enabled,
                                'multiplier': multiplier,  # Include multiplier for display
                                'max_contracts': max_contracts,  # Include max_contracts for display
                                'custom_ticker': custom_ticker  # Include custom_ticker for display
                            })
                except:
                    pass
        
        conn.close()
        
        # Build recorder object for webhook templates
        recorder = {
            'id': trader_row['recorder_id'],
            'name': trader['recorder_name'],
            'webhook_token': trader_row['webhook_token'] if 'webhook_token' in trader_row.keys() else None,
            'inverse_signals': bool(trader_row.get('r_inverse_signals') if hasattr(trader_row, 'get') else (trader_row['r_inverse_signals'] if 'r_inverse_signals' in trader_row.keys() else False))
        }

        # Privacy and tier data for the recorder toggle + admin pill
        recorder_is_private = bool(trader_row.get('recorder_is_private') if hasattr(trader_row, 'get') else False)
        recorder_user_id = trader_row.get('recorder_user_id') if hasattr(trader_row, 'get') else None
        is_recorder_owner = (current_user_id is not None and recorder_user_id is not None and int(current_user_id) == int(recorder_user_id))
        recorder_required_tier = (trader_row.get('recorder_required_tier') if hasattr(trader_row, 'get') else None) or 'public'

        # Check if current user is admin
        is_admin = False
        if USER_AUTH_AVAILABLE and is_logged_in():
            user = get_current_user()
            if user and user.is_admin:
                is_admin = True

        return render_template(
            'traders.html',
            mode='edit',
            header_title='Edit Trader',
            header_cta='Update Trader',
            trader=trader,
            accounts=accounts,
            recorder=recorder,
            recorder_is_private=recorder_is_private,
            is_recorder_owner=is_recorder_owner,
            is_admin=is_admin,
            recorder_required_tier=recorder_required_tier
        )
    except Exception as e:
        logger.error(f"Error in traders_edit for trader {trader_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return f"<h1>Error loading trader</h1><pre>{str(e)}</pre>", 500

@app.route('/control-center')
@feature_required('control_center')
def control_center():
    """Control Center with live recorder/strategy data and PnL"""
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    try:
        # Get current user for filtering
        current_user = None
        user_id = None
        is_admin = False
        if USER_AUTH_AVAILABLE:
            current_user = get_current_user()
            if current_user:
                user_id = current_user.id
                is_admin = getattr(current_user, 'is_admin', False)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Get recorders with their PnL - FILTERED BY USER (admins see all)
        if user_id and not is_admin:
            cursor.execute(f'''
                SELECT 
                    r.id,
                    r.name,
                    r.symbol,
                    r.recording_enabled,
                    COALESCE(SUM(CASE WHEN rt.status = 'closed' THEN rt.pnl ELSE 0 END), 0) as total_pnl,
                    COUNT(CASE WHEN rt.status = 'open' THEN 1 END) as open_trades,
                    COUNT(CASE WHEN rt.status = 'closed' THEN 1 END) as closed_trades
                FROM recorders r
                LEFT JOIN recorded_trades rt ON r.id = rt.recorder_id
                WHERE r.user_id = {placeholder}
                GROUP BY r.id
                ORDER BY r.name
            ''', (user_id,))
        else:
            # Admin or no user - show all
            cursor.execute('''
                SELECT 
                    r.id,
                    r.name,
                    r.symbol,
                    r.recording_enabled,
                    COALESCE(SUM(CASE WHEN rt.status = 'closed' THEN rt.pnl ELSE 0 END), 0) as total_pnl,
                    COUNT(CASE WHEN rt.status = 'open' THEN 1 END) as open_trades,
                    COUNT(CASE WHEN rt.status = 'closed' THEN 1 END) as closed_trades
                FROM recorders r
                LEFT JOIN recorded_trades rt ON r.id = rt.recorder_id
                GROUP BY r.id
                ORDER BY r.name
            ''')
        
        live_rows = []
        for row in cursor.fetchall():
            row_id = row[0] if isinstance(row, tuple) else row['id']
            row_name = row[1] if isinstance(row, tuple) else row['name']
            row_symbol = row[2] if isinstance(row, tuple) else row['symbol']
            row_enabled = row[3] if isinstance(row, tuple) else row['recording_enabled']
            row_pnl = row[4] if isinstance(row, tuple) else row['total_pnl']
            row_open = row[5] if isinstance(row, tuple) else row['open_trades']
            row_closed = row[6] if isinstance(row, tuple) else row['closed_trades']
            live_rows.append({
                'id': row_id,
                'name': row_name,
                'symbol': row_symbol or '',
                'enabled': bool(row_enabled),
                'pnl': row_pnl or 0,
                'open_trades': row_open or 0,
                'closed_trades': row_closed or 0
            })
        
        # Get recent signals as logs - FILTERED BY USER
        if user_id and not is_admin:
            cursor.execute(f'''
                SELECT 
                    rs.action,
                    rs.ticker,
                    rs.price,
                    rs.created_at,
                    r.name as recorder_name
                FROM recorded_signals rs
                JOIN recorders r ON rs.recorder_id = r.id
                WHERE r.user_id = {placeholder}
                ORDER BY rs.created_at DESC
                LIMIT 20
            ''', (user_id,))
        else:
            cursor.execute('''
                SELECT 
                    rs.action,
                    rs.ticker,
                    rs.price,
                    rs.created_at,
                    r.name as recorder_name
                FROM recorded_signals rs
                JOIN recorders r ON rs.recorder_id = r.id
                ORDER BY rs.created_at DESC
                LIMIT 20
            ''')
        
        logs = []
        for row in cursor.fetchall():
            action = row[0] if isinstance(row, tuple) else row['action']
            ticker = row[1] if isinstance(row, tuple) else row['ticker']
            price = row[2] if isinstance(row, tuple) else row['price']
            created_at = row[3] if isinstance(row, tuple) else row['created_at']
            recorder_name = row[4] if isinstance(row, tuple) else row['recorder_name']
            
            log_type = 'open' if action in ['BUY', 'LONG'] else 'close'
            logs.append({
                'type': log_type,
                'message': f"{recorder_name}: {action} {ticker} @ {price}",
                'time': created_at
            })
        
        conn.close()
        
        # Check platform subscription
        has_platform_subscription = True
        user_tier = 'none'
        if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
            user = get_current_user()
            if user:
                from subscription_models import get_user_subscription, get_user_plan_tier
                platform_sub = get_user_subscription(user.id, plan_type='platform')
                has_platform_subscription = platform_sub is not None
                user_tier = get_user_plan_tier(user.id)
                if user.is_admin:
                    has_platform_subscription = True
        
        current_user_id = user.id if user else None
        return render_template('control_center.html',
                              live_rows=live_rows,
                              logs=logs,
                              has_platform_subscription=has_platform_subscription,
                              user_tier=user_tier,
                              current_user_id=current_user_id)
    except Exception as e:
        logger.error(f"Error loading control center: {e}")
        return render_template('control_center.html',
                              live_rows=[],
                              logs=[],
                              has_platform_subscription=False,
                              user_tier='none',
                              current_user_id=None)

@app.route('/manual-trader')
def manual_trader_page():
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    
    # Check platform subscription
    has_platform_subscription = True
    user_tier = 'none'
    if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
        user = get_current_user()
        if user:
            from subscription_models import get_user_subscription, get_user_plan_tier
            platform_sub = get_user_subscription(user.id, plan_type='platform')
            has_platform_subscription = platform_sub is not None
            user_tier = get_user_plan_tier(user.id)
            if user.is_admin:
                has_platform_subscription = True
    
    # Check advanced copy trader feature access
    has_advanced_copy_trader = False
    if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
        user = get_current_user()
        if user:
            from subscription_models import check_feature_access
            has_advanced_copy_trader = check_feature_access(user.id, 'advanced_copy_trader')
            if user.is_admin:
                has_advanced_copy_trader = True

    # Pass current user ID for filtering live positions
    current_user_id = get_current_user_id() if USER_AUTH_AVAILABLE else None
    return render_template('manual_copy_trader.html',
                          current_user_id=current_user_id,
                          has_platform_subscription=has_platform_subscription,
                          user_tier=user_tier,
                          has_advanced_copy_trader=has_advanced_copy_trader)


# ============================================================================
# QUANT STOCK SCREENER (Restored Dec 8, 2025)
# ============================================================================

@app.route('/quant-screener')
def quant_screener_page():
    """Render the Quant Stock Screener page"""
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    
    # Check subscription access
    has_access = True
    user_tier = 'none'
    if SUBSCRIPTION_SYSTEM_AVAILABLE and USER_AUTH_AVAILABLE:
        user = get_current_user()
        if user:
            from subscription_models import check_feature_access, get_user_plan_tier
            has_access = check_feature_access(user.id, 'quant_screener')
            user_tier = get_user_plan_tier(user.id)
            # Admins always have access
            if user.is_admin:
                has_access = True
    
    return render_template('quant_screener.html',
                          feature_access=has_access,
                          user_tier=user_tier)


@app.route('/api/quant-screener/screen', methods=['POST'])
@feature_required('quant_screener')
def api_quant_screener_screen():
    """Run a stock screen with the given filters"""
    try:
        filters = request.get_json() or {}
        
        # Get filter values
        min_score = filters.get('min_score', 0)
        min_rating = filters.get('min_rating', '')
        value_grade = filters.get('value_grade', '')
        growth_grade = filters.get('growth_grade', '')
        profitability_grade = filters.get('profitability_grade', '')
        momentum_grade = filters.get('momentum_grade', '')
        eps_revisions_grade = filters.get('eps_revisions_grade', '')
        sector = filters.get('sector', '')
        market_cap = filters.get('market_cap', '')
        min_price = filters.get('min_price', 0)
        max_price = filters.get('max_price', None)
        
        # Generate sample stock universe with quant ratings
        results = generate_quant_stock_data(
            min_score=min_score,
            min_rating=min_rating,
            value_grade=value_grade,
            growth_grade=growth_grade,
            profitability_grade=profitability_grade,
            momentum_grade=momentum_grade,
            eps_revisions_grade=eps_revisions_grade,
            sector=sector,
            market_cap=market_cap,
            min_price=min_price,
            max_price=max_price
        )
        
        return jsonify({
            'success': True,
            'results': results,
            'count': len(results)
        })
        
    except Exception as e:
        logger.error(f"Error running quant screen: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/quant-screener/factors/<symbol>', methods=['GET'])
@feature_required('quant_screener')
def api_quant_screener_factors(symbol):
    """Get detailed factor grades for a specific stock"""
    try:
        stock_data = generate_single_stock_factors(symbol.upper())
        
        if stock_data:
            return jsonify({
                'success': True,
                'data': stock_data
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Stock {symbol} not found'
            }), 404
            
    except Exception as e:
        logger.error(f"Error getting factors for {symbol}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/quant-screener/search', methods=['GET'])
@feature_required('quant_screener')
def api_quant_screener_search():
    """Search for stocks by symbol or name"""
    try:
        query = request.args.get('q', '').upper().strip()
        
        if not query:
            return jsonify({'success': True, 'results': []})
        
        all_stocks = get_stock_universe()
        
        results = []
        for stock in all_stocks:
            symbol_match = query in stock['symbol'].upper()
            name_match = query in stock['name'].upper()
            
            if symbol_match or name_match:
                results.append({
                    'symbol': stock['symbol'],
                    'name': stock['name'],
                    'sector': stock.get('sector', 'Unknown').replace('_', ' ').title()
                })
        
        results.sort(key=lambda x: (0 if x['symbol'] == query else 1, x['symbol']))
        results = results[:10]
        
        return jsonify({
            'success': True,
            'results': results
        })
        
    except Exception as e:
        logger.error(f"Error searching stocks: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/quant-screener/ticker/<symbol>', methods=['GET'])
@feature_required('quant_screener')
def api_quant_screener_ticker(symbol):
    """Get full quant report for a specific ticker using REAL financial data"""
    try:
        symbol = symbol.upper().strip()
        
        # Get REAL factor grades from yfinance
        real_factors = get_real_factor_grades(symbol)
        stock_data = real_factors.get('_stock_data', {})
        data_source = real_factors.get('_data_source', 'fallback')
        
        # Extract factor grades (excluding metadata keys)
        factors = {
            'value': real_factors.get('value', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'growth': real_factors.get('growth', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'profitability': real_factors.get('profitability', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'momentum': real_factors.get('momentum', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'eps_revisions': real_factors.get('eps_revisions', {'grade': 'C', 'score': 0.5, 'metrics': {}})
        }
        
        # Use real price from yfinance, fallback to TradingView
        price = stock_data.get('price')
        change_pct = stock_data.get('change_pct', 0)
        
        if not price:
            try:
                prices = fetch_live_stock_prices([symbol])
                if symbol in prices:
                    price = prices[symbol]['price']
                    change_pct = prices[symbol].get('change_pct', 0)
            except Exception as e:
                logger.warning(f"Could not fetch live price for {symbol}: {e}")
        
        # Calculate quant score (1.0-5.0 scale) based on REAL scores
        quant_score = (
            (factors['value']['score'] * 4 + 1) * 0.20 +
            (factors['growth']['score'] * 4 + 1) * 0.20 +
            (factors['profitability']['score'] * 4 + 1) * 0.25 +
            (factors['momentum']['score'] * 4 + 1) * 0.20 +
            (factors['eps_revisions']['score'] * 4 + 1) * 0.15
        )
        
        # Determine rating
        rating = score_to_quant_rating(quant_score)
        
        # Get company info from real data or fallback
        company_name = stock_data.get('name') or symbol
        company_sector = stock_data.get('sector') or 'Unknown'
        company_industry = stock_data.get('industry') or 'Unknown'
        market_cap = stock_data.get('market_cap', 0)
        
        # Determine market cap category
        if market_cap >= 200_000_000_000:
            market_cap_cat = 'Mega'
        elif market_cap >= 10_000_000_000:
            market_cap_cat = 'Large'
        elif market_cap >= 2_000_000_000:
            market_cap_cat = 'Mid'
        elif market_cap >= 300_000_000:
            market_cap_cat = 'Small'
        else:
            market_cap_cat = 'Micro'
        
        stock_report = {
            'symbol': symbol,
            'name': company_name,
            'sector': company_sector,
            'industry': company_industry,
            'market_cap': market_cap_cat,
            'market_cap_value': market_cap,
            'price': round(price, 2) if price else None,
            'change': round(change_pct, 2) if change_pct else 0,
            'quant_score': round(quant_score, 2),
            'rating': rating,
            'factors': factors,
            'data_source': data_source  # 'yfinance' or 'fallback'
        }
        
        return jsonify({
            'success': True,
            'stock': stock_report
        })
        
    except Exception as e:
        logger.error(f"Error getting ticker report for {symbol}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/quant-screener/live-prices', methods=['POST'])
@feature_required('quant_screener')
def api_quant_screener_live_prices():
    """Fetch live stock prices from TradingView"""
    try:
        data = request.get_json() or {}
        symbols = data.get('symbols', [])
        
        if not symbols:
            return jsonify({'success': False, 'error': 'No symbols provided'}), 400
        
        symbols = symbols[:50]
        prices = fetch_live_stock_prices(symbols)
        
        return jsonify({
            'success': True,
            'prices': prices,
            'count': len(prices),
            'requested': len(symbols)
        })
        
    except Exception as e:
        logger.error(f"Error fetching live prices: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/quant-screener/live-price/<symbol>', methods=['GET'])
@feature_required('quant_screener')
def api_quant_screener_live_price(symbol):
    """Get live price for a single stock symbol"""
    try:
        prices = fetch_live_stock_prices([symbol.upper()])
        
        if symbol.upper() in prices:
            return jsonify({
                'success': True,
                'symbol': symbol.upper(),
                'data': prices[symbol.upper()]
            })
        else:
            return jsonify({
                'success': False,
                'error': f'Price not found for {symbol}'
            }), 404
            
    except Exception as e:
        logger.error(f"Error fetching price for {symbol}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


def get_stock_universe():
    """Get the full stock universe for searching"""
    return [
        # Tech Giants
        {'symbol': 'AAPL', 'name': 'Apple Inc.', 'sector': 'technology', 'market_cap': 'mega'},
        {'symbol': 'MSFT', 'name': 'Microsoft Corporation', 'sector': 'technology', 'market_cap': 'mega'},
        {'symbol': 'GOOGL', 'name': 'Alphabet Inc.', 'sector': 'technology', 'market_cap': 'mega'},
        {'symbol': 'AMZN', 'name': 'Amazon.com Inc.', 'sector': 'consumer_cyclical', 'market_cap': 'mega'},
        {'symbol': 'NVDA', 'name': 'NVIDIA Corporation', 'sector': 'technology', 'market_cap': 'mega'},
        {'symbol': 'META', 'name': 'Meta Platforms Inc.', 'sector': 'technology', 'market_cap': 'mega'},
        {'symbol': 'TSLA', 'name': 'Tesla Inc.', 'sector': 'consumer_cyclical', 'market_cap': 'mega'},
        {'symbol': 'AMD', 'name': 'Advanced Micro Devices', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'INTC', 'name': 'Intel Corporation', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'CRM', 'name': 'Salesforce Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'ORCL', 'name': 'Oracle Corporation', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'ADBE', 'name': 'Adobe Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'NFLX', 'name': 'Netflix Inc.', 'sector': 'communication_services', 'market_cap': 'large'},
        {'symbol': 'CSCO', 'name': 'Cisco Systems Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'AVGO', 'name': 'Broadcom Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'QCOM', 'name': 'Qualcomm Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'TXN', 'name': 'Texas Instruments', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'IBM', 'name': 'IBM Corporation', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'NOW', 'name': 'ServiceNow Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'AMAT', 'name': 'Applied Materials', 'sector': 'technology', 'market_cap': 'large'},
        # Finance
        {'symbol': 'JPM', 'name': 'JPMorgan Chase & Co.', 'sector': 'financial_services', 'market_cap': 'mega'},
        {'symbol': 'BAC', 'name': 'Bank of America Corp.', 'sector': 'financial_services', 'market_cap': 'mega'},
        {'symbol': 'WFC', 'name': 'Wells Fargo & Co.', 'sector': 'financial_services', 'market_cap': 'large'},
        {'symbol': 'GS', 'name': 'Goldman Sachs Group', 'sector': 'financial_services', 'market_cap': 'large'},
        {'symbol': 'MS', 'name': 'Morgan Stanley', 'sector': 'financial_services', 'market_cap': 'large'},
        {'symbol': 'V', 'name': 'Visa Inc.', 'sector': 'financial_services', 'market_cap': 'mega'},
        {'symbol': 'MA', 'name': 'Mastercard Inc.', 'sector': 'financial_services', 'market_cap': 'mega'},
        {'symbol': 'AXP', 'name': 'American Express Co.', 'sector': 'financial_services', 'market_cap': 'large'},
        {'symbol': 'BLK', 'name': 'BlackRock Inc.', 'sector': 'financial_services', 'market_cap': 'large'},
        {'symbol': 'C', 'name': 'Citigroup Inc.', 'sector': 'financial_services', 'market_cap': 'large'},
        # Healthcare
        {'symbol': 'JNJ', 'name': 'Johnson & Johnson', 'sector': 'healthcare', 'market_cap': 'mega'},
        {'symbol': 'UNH', 'name': 'UnitedHealth Group', 'sector': 'healthcare', 'market_cap': 'mega'},
        {'symbol': 'PFE', 'name': 'Pfizer Inc.', 'sector': 'healthcare', 'market_cap': 'large'},
        {'symbol': 'ABBV', 'name': 'AbbVie Inc.', 'sector': 'healthcare', 'market_cap': 'large'},
        {'symbol': 'MRK', 'name': 'Merck & Co. Inc.', 'sector': 'healthcare', 'market_cap': 'large'},
        {'symbol': 'LLY', 'name': 'Eli Lilly & Co.', 'sector': 'healthcare', 'market_cap': 'mega'},
        {'symbol': 'TMO', 'name': 'Thermo Fisher Scientific', 'sector': 'healthcare', 'market_cap': 'large'},
        {'symbol': 'ABT', 'name': 'Abbott Laboratories', 'sector': 'healthcare', 'market_cap': 'large'},
        {'symbol': 'DHR', 'name': 'Danaher Corporation', 'sector': 'healthcare', 'market_cap': 'large'},
        {'symbol': 'BMY', 'name': 'Bristol-Myers Squibb', 'sector': 'healthcare', 'market_cap': 'large'},
        # Consumer
        {'symbol': 'WMT', 'name': 'Walmart Inc.', 'sector': 'consumer_defensive', 'market_cap': 'mega'},
        {'symbol': 'PG', 'name': 'Procter & Gamble Co.', 'sector': 'consumer_defensive', 'market_cap': 'mega'},
        {'symbol': 'KO', 'name': 'Coca-Cola Company', 'sector': 'consumer_defensive', 'market_cap': 'mega'},
        {'symbol': 'PEP', 'name': 'PepsiCo Inc.', 'sector': 'consumer_defensive', 'market_cap': 'mega'},
        {'symbol': 'COST', 'name': 'Costco Wholesale Corp.', 'sector': 'consumer_defensive', 'market_cap': 'large'},
        {'symbol': 'HD', 'name': 'Home Depot Inc.', 'sector': 'consumer_cyclical', 'market_cap': 'mega'},
        {'symbol': 'MCD', 'name': "McDonald's Corporation", 'sector': 'consumer_cyclical', 'market_cap': 'large'},
        {'symbol': 'NKE', 'name': 'Nike Inc.', 'sector': 'consumer_cyclical', 'market_cap': 'large'},
        {'symbol': 'SBUX', 'name': 'Starbucks Corporation', 'sector': 'consumer_cyclical', 'market_cap': 'large'},
        {'symbol': 'TGT', 'name': 'Target Corporation', 'sector': 'consumer_defensive', 'market_cap': 'large'},
        # Energy
        {'symbol': 'XOM', 'name': 'Exxon Mobil Corp.', 'sector': 'energy', 'market_cap': 'mega'},
        {'symbol': 'CVX', 'name': 'Chevron Corporation', 'sector': 'energy', 'market_cap': 'mega'},
        {'symbol': 'COP', 'name': 'ConocoPhillips', 'sector': 'energy', 'market_cap': 'large'},
        {'symbol': 'SLB', 'name': 'Schlumberger Ltd.', 'sector': 'energy', 'market_cap': 'large'},
        {'symbol': 'EOG', 'name': 'EOG Resources Inc.', 'sector': 'energy', 'market_cap': 'large'},
        # Industrial
        {'symbol': 'CAT', 'name': 'Caterpillar Inc.', 'sector': 'industrials', 'market_cap': 'large'},
        {'symbol': 'BA', 'name': 'Boeing Company', 'sector': 'industrials', 'market_cap': 'large'},
        {'symbol': 'HON', 'name': 'Honeywell International', 'sector': 'industrials', 'market_cap': 'large'},
        {'symbol': 'UPS', 'name': 'United Parcel Service', 'sector': 'industrials', 'market_cap': 'large'},
        {'symbol': 'RTX', 'name': 'RTX Corporation', 'sector': 'industrials', 'market_cap': 'large'},
        {'symbol': 'DE', 'name': 'Deere & Company', 'sector': 'industrials', 'market_cap': 'large'},
        {'symbol': 'LMT', 'name': 'Lockheed Martin Corp.', 'sector': 'industrials', 'market_cap': 'large'},
        {'symbol': 'GE', 'name': 'General Electric Co.', 'sector': 'industrials', 'market_cap': 'large'},
        # ETFs
        {'symbol': 'SPY', 'name': 'SPDR S&P 500 ETF', 'sector': 'etf', 'market_cap': 'mega'},
        {'symbol': 'QQQ', 'name': 'Invesco QQQ Trust', 'sector': 'etf', 'market_cap': 'mega'},
        {'symbol': 'IWM', 'name': 'iShares Russell 2000 ETF', 'sector': 'etf', 'market_cap': 'large'},
        {'symbol': 'DIA', 'name': 'SPDR Dow Jones ETF', 'sector': 'etf', 'market_cap': 'large'},
        {'symbol': 'VOO', 'name': 'Vanguard S&P 500 ETF', 'sector': 'etf', 'market_cap': 'mega'},
        {'symbol': 'VTI', 'name': 'Vanguard Total Stock Market', 'sector': 'etf', 'market_cap': 'mega'},
        {'symbol': 'ARKK', 'name': 'ARK Innovation ETF', 'sector': 'etf', 'market_cap': 'mid'},
        {'symbol': 'XLF', 'name': 'Financial Select Sector SPDR', 'sector': 'etf', 'market_cap': 'large'},
        {'symbol': 'XLK', 'name': 'Technology Select Sector SPDR', 'sector': 'etf', 'market_cap': 'large'},
        {'symbol': 'XLE', 'name': 'Energy Select Sector SPDR', 'sector': 'etf', 'market_cap': 'large'},
        # Crypto-related
        {'symbol': 'COIN', 'name': 'Coinbase Global Inc.', 'sector': 'financial_services', 'market_cap': 'large'},
        {'symbol': 'MSTR', 'name': 'MicroStrategy Inc.', 'sector': 'technology', 'market_cap': 'mid'},
        # Other notable
        {'symbol': 'BRK.B', 'name': 'Berkshire Hathaway', 'sector': 'financial_services', 'market_cap': 'mega'},
        {'symbol': 'DIS', 'name': 'Walt Disney Company', 'sector': 'communication_services', 'market_cap': 'large'},
        {'symbol': 'PYPL', 'name': 'PayPal Holdings Inc.', 'sector': 'financial_services', 'market_cap': 'large'},
        {'symbol': 'SQ', 'name': 'Block Inc.', 'sector': 'financial_services', 'market_cap': 'mid'},
        {'symbol': 'SHOP', 'name': 'Shopify Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'UBER', 'name': 'Uber Technologies', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'ABNB', 'name': 'Airbnb Inc.', 'sector': 'consumer_cyclical', 'market_cap': 'large'},
        {'symbol': 'PLTR', 'name': 'Palantir Technologies', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'SNOW', 'name': 'Snowflake Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'ZM', 'name': 'Zoom Video Communications', 'sector': 'technology', 'market_cap': 'mid'},
        {'symbol': 'ROKU', 'name': 'Roku Inc.', 'sector': 'communication_services', 'market_cap': 'mid'},
        {'symbol': 'SNAP', 'name': 'Snap Inc.', 'sector': 'communication_services', 'market_cap': 'mid'},
        {'symbol': 'PINS', 'name': 'Pinterest Inc.', 'sector': 'communication_services', 'market_cap': 'mid'},
        {'symbol': 'RBLX', 'name': 'Roblox Corporation', 'sector': 'communication_services', 'market_cap': 'mid'},
        {'symbol': 'U', 'name': 'Unity Software Inc.', 'sector': 'technology', 'market_cap': 'mid'},
        {'symbol': 'CRWD', 'name': 'CrowdStrike Holdings', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'ZS', 'name': 'Zscaler Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'DDOG', 'name': 'Datadog Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'NET', 'name': 'Cloudflare Inc.', 'sector': 'technology', 'market_cap': 'mid'},
        {'symbol': 'MDB', 'name': 'MongoDB Inc.', 'sector': 'technology', 'market_cap': 'mid'},
        {'symbol': 'PANW', 'name': 'Palo Alto Networks', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'TTD', 'name': 'The Trade Desk Inc.', 'sector': 'technology', 'market_cap': 'large'},
        {'symbol': 'RIVN', 'name': 'Rivian Automotive', 'sector': 'consumer_cyclical', 'market_cap': 'mid'},
        {'symbol': 'LCID', 'name': 'Lucid Group Inc.', 'sector': 'consumer_cyclical', 'market_cap': 'mid'},
        {'symbol': 'F', 'name': 'Ford Motor Company', 'sector': 'consumer_cyclical', 'market_cap': 'large'},
        {'symbol': 'GM', 'name': 'General Motors Company', 'sector': 'consumer_cyclical', 'market_cap': 'large'},
        {'symbol': 'T', 'name': 'AT&T Inc.', 'sector': 'communication_services', 'market_cap': 'large'},
        {'symbol': 'VZ', 'name': 'Verizon Communications', 'sector': 'communication_services', 'market_cap': 'large'},
        {'symbol': 'TMUS', 'name': 'T-Mobile US Inc.', 'sector': 'communication_services', 'market_cap': 'large'},
    ]


# ============================================================================
# SEEKING ALPHA STYLE QUANT RATING SYSTEM
# ============================================================================

def percentile_to_grade(percentile):
    """Convert a percentile rank (0-100) to a Seeking Alpha style grade."""
    if percentile >= 95:
        return 'A+'
    elif percentile >= 85:
        return 'A'
    elif percentile >= 75:
        return 'A-'
    elif percentile >= 65:
        return 'B+'
    elif percentile >= 55:
        return 'B'
    elif percentile >= 45:
        return 'B-'
    elif percentile >= 35:
        return 'C+'
    elif percentile >= 25:
        return 'C'
    elif percentile >= 15:
        return 'C-'
    elif percentile >= 10:
        return 'D+'
    elif percentile >= 5:
        return 'D'
    else:
        return 'F'


def score_to_quant_rating(score_1_to_5):
    """Convert a 1.0-5.0 score to a Seeking Alpha rating."""
    if score_1_to_5 >= 4.5:
        return 'Strong Buy'
    elif score_1_to_5 >= 3.5:
        return 'Buy'
    elif score_1_to_5 >= 2.5:
        return 'Hold'
    elif score_1_to_5 >= 1.5:
        return 'Sell'
    else:
        return 'Strong Sell'


# ============================================================================
# REAL FINANCIAL DATA SERVICE (yfinance-based)
# ============================================================================

# In-memory cache for financial data (expires after 1 hour)
_quant_data_cache = {}
_quant_cache_expiry = {}
QUANT_CACHE_DURATION = 3600  # 1 hour in seconds

# Sector median benchmarks (approximate industry averages)
# ============================================================================
# SECTOR MEDIANS - Based on Seeking Alpha's ACTUAL sector data (Dec 2025)
# These are the REAL sector medians from Seeking Alpha's profitability page
# ============================================================================
SECTOR_MEDIANS = {
    'Technology': {
        # Valuation medians from Seeking Alpha (Information Technology sector)
        'pe_ratio': 24.5, 'forward_pe': 24.5, 'peg_ratio': 1.0,
        'price_to_book': 3.6, 'price_to_sales': 3.3, 'ev_to_ebitda': 15.0,
        # Profitability medians - ACTUAL from Seeking Alpha (Dec 2025)
        'profit_margin': 0.0481, 'operating_margin': 0.061, 'gross_margin': 0.4887,
        'roe': 0.0633, 'roa': 0.0291,
        # Growth medians - ACTUAL from Seeking Alpha (Dec 2025)
        'revenue_growth': 0.0906, 'earnings_growth': 0.1378  # 9.06%, 13.78%
    },
    'Healthcare': {
        'pe_ratio': 22.0, 'forward_pe': 20.0, 'peg_ratio': 1.5,
        'price_to_book': 3.8, 'price_to_sales': 3.5, 'ev_to_ebitda': 14.0,
        'profit_margin': 0.10, 'operating_margin': 0.12, 'gross_margin': 0.55,
        'roe': 0.12, 'roa': 0.06, 'revenue_growth': 0.06, 'earnings_growth': 0.08
    },
    'Financial Services': {
        'pe_ratio': 13.0, 'forward_pe': 12.0, 'peg_ratio': 1.3,
        'price_to_book': 1.3, 'price_to_sales': 3.0, 'ev_to_ebitda': 10.0,
        'profit_margin': 0.22, 'operating_margin': 0.28, 'gross_margin': 0.65,
        'roe': 0.11, 'roa': 0.01, 'revenue_growth': 0.04, 'earnings_growth': 0.06
    },
    'Consumer Cyclical': {
        'pe_ratio': 20.0, 'forward_pe': 18.0, 'peg_ratio': 1.5,
        'price_to_book': 4.5, 'price_to_sales': 1.2, 'ev_to_ebitda': 12.0,
        'profit_margin': 0.06, 'operating_margin': 0.08, 'gross_margin': 0.32,
        'roe': 0.15, 'roa': 0.06, 'revenue_growth': 0.08, 'earnings_growth': 0.10
    },
    'Communication Services': {
        'pe_ratio': 17.0, 'forward_pe': 15.0, 'peg_ratio': 1.2,
        'price_to_book': 2.8, 'price_to_sales': 2.5, 'ev_to_ebitda': 10.0,
        'profit_margin': 0.12, 'operating_margin': 0.18, 'gross_margin': 0.48,
        'roe': 0.14, 'roa': 0.07, 'revenue_growth': 0.06, 'earnings_growth': 0.08
    },
    'Industrials': {
        'pe_ratio': 22.0, 'forward_pe': 20.0, 'peg_ratio': 1.8,
        'price_to_book': 4.0, 'price_to_sales': 2.0, 'ev_to_ebitda': 13.0,
        'profit_margin': 0.07, 'operating_margin': 0.10, 'gross_margin': 0.28,
        'roe': 0.14, 'roa': 0.05, 'revenue_growth': 0.05, 'earnings_growth': 0.07
    },
    'Energy': {
        'pe_ratio': 11.0, 'forward_pe': 10.0, 'peg_ratio': 0.8,
        'price_to_book': 1.6, 'price_to_sales': 1.2, 'ev_to_ebitda': 5.5,
        'profit_margin': 0.09, 'operating_margin': 0.12, 'gross_margin': 0.35,
        'roe': 0.14, 'roa': 0.07, 'revenue_growth': 0.03, 'earnings_growth': 0.05
    },
    'Consumer Defensive': {
        'pe_ratio': 24.0, 'forward_pe': 22.0, 'peg_ratio': 2.8,
        'price_to_book': 5.5, 'price_to_sales': 1.8, 'ev_to_ebitda': 15.0,
        'profit_margin': 0.07, 'operating_margin': 0.10, 'gross_margin': 0.32,
        'roe': 0.18, 'roa': 0.07, 'revenue_growth': 0.03, 'earnings_growth': 0.05
    },
    'Utilities': {
        'pe_ratio': 18.0, 'forward_pe': 17.0, 'peg_ratio': 3.2,
        'price_to_book': 1.9, 'price_to_sales': 2.8, 'ev_to_ebitda': 13.0,
        'profit_margin': 0.11, 'operating_margin': 0.18, 'gross_margin': 0.38,
        'roe': 0.09, 'roa': 0.03, 'revenue_growth': 0.02, 'earnings_growth': 0.03
    },
    'Real Estate': {
        'pe_ratio': 38.0, 'forward_pe': 35.0, 'peg_ratio': 2.5,
        'price_to_book': 2.2, 'price_to_sales': 7.5, 'ev_to_ebitda': 18.0,
        'profit_margin': 0.22, 'operating_margin': 0.30, 'gross_margin': 0.58,
        'roe': 0.07, 'roa': 0.03, 'revenue_growth': 0.04, 'earnings_growth': 0.05
    },
    'Basic Materials': {
        'pe_ratio': 13.0, 'forward_pe': 12.0, 'peg_ratio': 1.0,
        'price_to_book': 2.2, 'price_to_sales': 1.4, 'ev_to_ebitda': 7.5,
        'profit_margin': 0.09, 'operating_margin': 0.13, 'gross_margin': 0.28,
        'roe': 0.11, 'roa': 0.05, 'revenue_growth': 0.04, 'earnings_growth': 0.06
    },
    'Default': {
        'pe_ratio': 20.0, 'forward_pe': 18.0, 'peg_ratio': 1.5,
        'price_to_book': 3.0, 'price_to_sales': 2.5, 'ev_to_ebitda': 12.0,
        'profit_margin': 0.10, 'operating_margin': 0.14, 'gross_margin': 0.38,
        'roe': 0.14, 'roa': 0.06, 'revenue_growth': 0.06, 'earnings_growth': 0.08
    }
}


# Hardcoded sector mapping for major stocks (TradingView doesn't return sector reliably)
STOCK_SECTOR_MAP = {
    # Technology
    'AAPL': 'Technology', 'MSFT': 'Technology', 'GOOGL': 'Technology', 'GOOG': 'Technology',
    'META': 'Technology', 'NVDA': 'Technology', 'AMD': 'Technology', 'INTC': 'Technology',
    'CRM': 'Technology', 'ADBE': 'Technology', 'ORCL': 'Technology', 'CSCO': 'Technology',
    'IBM': 'Technology', 'QCOM': 'Technology', 'TXN': 'Technology', 'AVGO': 'Technology',
    'NOW': 'Technology', 'INTU': 'Technology', 'AMAT': 'Technology', 'MU': 'Technology',
    'LRCX': 'Technology', 'KLAC': 'Technology', 'SNPS': 'Technology', 'CDNS': 'Technology',
    'PLTR': 'Technology', 'SNOW': 'Technology', 'NET': 'Technology', 'DDOG': 'Technology',
    'ZS': 'Technology', 'CRWD': 'Technology', 'PANW': 'Technology', 'FTNT': 'Technology',
    # Consumer Cyclical
    'AMZN': 'Consumer Cyclical', 'TSLA': 'Consumer Cyclical', 'HD': 'Consumer Cyclical',
    'NKE': 'Consumer Cyclical', 'MCD': 'Consumer Cyclical', 'SBUX': 'Consumer Cyclical',
    'TGT': 'Consumer Cyclical', 'LOW': 'Consumer Cyclical', 'BKNG': 'Consumer Cyclical',
    'MAR': 'Consumer Cyclical', 'GM': 'Consumer Cyclical', 'F': 'Consumer Cyclical',
    # Communication Services
    'NFLX': 'Communication Services', 'DIS': 'Communication Services', 'CMCSA': 'Communication Services',
    'T': 'Communication Services', 'VZ': 'Communication Services', 'TMUS': 'Communication Services',
    # Healthcare
    'JNJ': 'Healthcare', 'UNH': 'Healthcare', 'PFE': 'Healthcare', 'ABBV': 'Healthcare',
    'MRK': 'Healthcare', 'LLY': 'Healthcare', 'TMO': 'Healthcare', 'ABT': 'Healthcare',
    'DHR': 'Healthcare', 'BMY': 'Healthcare', 'AMGN': 'Healthcare', 'GILD': 'Healthcare',
    'ISRG': 'Healthcare', 'VRTX': 'Healthcare', 'REGN': 'Healthcare', 'MRNA': 'Healthcare',
    # Financial Services
    'JPM': 'Financial Services', 'BAC': 'Financial Services', 'WFC': 'Financial Services',
    'GS': 'Financial Services', 'MS': 'Financial Services', 'C': 'Financial Services',
    'BLK': 'Financial Services', 'SCHW': 'Financial Services', 'AXP': 'Financial Services',
    'V': 'Financial Services', 'MA': 'Financial Services', 'PYPL': 'Financial Services',
    # Consumer Defensive
    'WMT': 'Consumer Defensive', 'PG': 'Consumer Defensive', 'KO': 'Consumer Defensive',
    'PEP': 'Consumer Defensive', 'COST': 'Consumer Defensive', 'PM': 'Consumer Defensive',
    'MO': 'Consumer Defensive', 'CL': 'Consumer Defensive', 'KMB': 'Consumer Defensive',
    # Energy
    'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy', 'SLB': 'Energy', 'EOG': 'Energy',
    'OXY': 'Energy', 'MPC': 'Energy', 'PSX': 'Energy', 'VLO': 'Energy',
    # Industrials
    'CAT': 'Industrials', 'BA': 'Industrials', 'HON': 'Industrials', 'UPS': 'Industrials',
    'UNP': 'Industrials', 'RTX': 'Industrials', 'LMT': 'Industrials', 'GE': 'Industrials',
    'DE': 'Industrials', 'MMM': 'Industrials', 'FDX': 'Industrials',
    # Utilities
    'NEE': 'Utilities', 'DUK': 'Utilities', 'SO': 'Utilities', 'D': 'Utilities',
    # Real Estate
    'AMT': 'Real Estate', 'PLD': 'Real Estate', 'CCI': 'Real Estate', 'EQIX': 'Real Estate',
    # Basic Materials
    'LIN': 'Basic Materials', 'APD': 'Basic Materials', 'SHW': 'Basic Materials', 'FCX': 'Basic Materials',
    'NEM': 'Basic Materials', 'NUE': 'Basic Materials', 'DOW': 'Basic Materials',
}


def get_stock_data_from_tradingview(symbol):
    """
    Fetch fundamental stock data from TradingView Scanner API.
    This is more reliable than yfinance on Railway.
    """
    import requests
    
    try:
        session = get_tradingview_session_for_stocks()
        cookies = {}
        if session:
            cookies = {
                'sessionid': session.get('sessionid', ''),
                'sessionid_sign': session.get('sessionid_sign', '')
            }
        
        url = "https://scanner.tradingview.com/america/scan"
        
        # Try multiple exchanges
        tv_symbols = [
            f"NASDAQ:{symbol}",
            f"NYSE:{symbol}",
            f"AMEX:{symbol}"
        ]
        
        # Request fundamental columns we need for quant analysis
        # TradingView Scanner API - VALIDATED column names only
        columns = [
            # Price & Basic Info
            "close", "change", "change_abs", "volume", "name", "description",
            "sector", "industry", "market_cap_basic", "type", "subtype",
            # Valuation metrics (TTM = trailing twelve months)
            "price_earnings_ttm", "earnings_per_share_basic_ttm",
            "price_book_ratio", "price_sales_ratio",  # P/B, P/S
            "enterprise_value_ebitda_ttm",  # EV/EBITDA (no standalone enterprise_value)
            "price_free_cash_flow_ttm",
            # Profitability metrics (as percentages)
            "gross_margin", "operating_margin", "pre_tax_margin", "net_margin",  
            "return_on_equity", "return_on_assets", "return_on_invested_capital",
            # Momentum metrics
            "SMA50", "SMA200",  # Moving averages
            "Perf.W", "Perf.1M", "Perf.3M", "Perf.6M", "Perf.Y", "Perf.YTD",  # Performance
            # Analyst data
            "Recommend.All", "number_of_employees", "average_volume_10d_calc"
        ]
        
        payload = {
            "symbols": {"tickers": tv_symbols},
            "columns": columns
        }
        
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Origin': 'https://www.tradingview.com',
            'Referer': 'https://www.tradingview.com/'
        }
        
        response = requests.post(url, json=payload, headers=headers, cookies=cookies, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            
            if data.get('data') and len(data['data']) > 0:
                # Find the first valid result
                for item in data['data']:
                    values = item.get('d', [])
                    if values and values[0] is not None:  # Has price data
                        # Map column names to values
                        col_map = {columns[i]: values[i] for i in range(min(len(columns), len(values)))}
                        
                        # Log raw data for debugging
                        logger.info(f"TradingView raw data for {symbol}: sector={col_map.get('sector')}, industry={col_map.get('industry')}")
                        
                        # Determine sector - FIRST check our hardcoded map, then TradingView data
                        sector = STOCK_SECTOR_MAP.get(symbol.upper())  # Our reliable map FIRST
                        if not sector:
                            # Try TradingView data as fallback
                            sector = col_map.get('sector') or col_map.get('type')
                        if not sector or sector == 'Unknown' or len(str(sector)) < 2:
                            sector = 'Technology'  # Default fallback
                        
                        # Build stock data dict
                        stock_data = {
                            'symbol': symbol.upper(),
                            'name': col_map.get('name') or col_map.get('description') or symbol,
                            'sector': sector,
                            'industry': col_map.get('industry') or col_map.get('subtype') or 'Unknown',
                            'price': col_map.get('close', 0),
                            'change_pct': col_map.get('change', 0),
                            'market_cap': col_map.get('market_cap_basic', 0),
                            
                            # Valuation metrics
                            'pe_ratio': col_map.get('price_earnings_ttm'),
                            'forward_pe': col_map.get('price_earnings_ttm'),  # TV doesn't have forward, use TTM
                            'peg_ratio': None,  # TradingView doesn't provide PEG directly
                            'price_to_book': col_map.get('price_book_ratio'),
                            'price_to_sales': col_map.get('price_sales_ratio'),
                            'ev_to_ebitda': col_map.get('enterprise_value_ebitda_ttm'),
                            'ev_to_revenue': None,
                            
                            # Profitability metrics (TradingView returns as decimals or percentages - check format)
                            'profit_margin': col_map.get('net_margin') / 100 if col_map.get('net_margin') and col_map.get('net_margin') > 1 else col_map.get('net_margin'),
                            'operating_margin': col_map.get('operating_margin') / 100 if col_map.get('operating_margin') and col_map.get('operating_margin') > 1 else col_map.get('operating_margin'),
                            'gross_margin': col_map.get('gross_margin') / 100 if col_map.get('gross_margin') and col_map.get('gross_margin') > 1 else col_map.get('gross_margin'),
                            'roe': col_map.get('return_on_equity') / 100 if col_map.get('return_on_equity') and col_map.get('return_on_equity') > 1 else col_map.get('return_on_equity'),
                            'roa': col_map.get('return_on_assets') / 100 if col_map.get('return_on_assets') and col_map.get('return_on_assets') > 1 else col_map.get('return_on_assets'),
                            
                            # Growth metrics - TradingView Scanner doesn't provide these, set to None
                            # Growth calculation will fall back to sector median comparison
                            'revenue_growth': None,  # TradingView Scanner doesn't provide this
                            'earnings_growth': None,  # TradingView Scanner doesn't provide this
                            'earnings_quarterly_growth': None,  # TradingView Scanner doesn't provide this
                            
                            # Momentum metrics
                            'fifty_two_week_high': col_map.get('price_52_week_high'),
                            'fifty_two_week_low': col_map.get('price_52_week_low'),
                            'fifty_day_average': col_map.get('SMA50'),
                            'two_hundred_day_average': col_map.get('SMA200'),
                            
                            # Analyst data - TradingView Recommend.All is -1 to 1 scale, convert to 1-5
                            'recommendation_mean': (col_map.get('Recommend.All') + 1) * 2 + 1 if col_map.get('Recommend.All') is not None else None,
                            'target_mean_price': None,  # TradingView doesn't provide target price in scan
                            'number_of_analyst_opinions': 10,  # Default estimate
                            
                            # Performance data for momentum
                            'perf_week': col_map.get('Perf.W'),
                            'perf_month': col_map.get('Perf.1M'),
                            'perf_quarter': col_map.get('Perf.3M'),
                            'perf_half': col_map.get('Perf.6M'),
                            'perf_year': col_map.get('Perf.Y'),
                            
                            '_data_source': 'tradingview'
                        }
                        
                        logger.info(f"‚úÖ TradingView data for {symbol}: sector={stock_data['sector']}, P/E={stock_data['pe_ratio']}, ROE={stock_data['roe']}")
                        return stock_data
        
        logger.warning(f"TradingView returned no data for {symbol}")
        return None
        
    except Exception as e:
        logger.error(f"Error fetching TradingView data for {symbol}: {e}")
        return None


def get_real_stock_data(symbol):
    """
    Fetch real financial data for a stock.
    Primary: TradingView Scanner API (works on Railway)
    Fallback: yfinance (may not work on Railway)
    """
    import time as time_module
    
    # Check cache first
    cache_key = symbol.upper()
    current_time = time_module.time()
    
    if cache_key in _quant_data_cache:
        if current_time < _quant_cache_expiry.get(cache_key, 0):
            return _quant_data_cache[cache_key]
    
    # Try TradingView first (more reliable on Railway)
    data = get_stock_data_from_tradingview(symbol)
    if data:
        # Cache the result
        _quant_data_cache[cache_key] = data
        _quant_cache_expiry[cache_key] = current_time + 3600  # 1 hour cache
        return data
    
    # Fallback to yfinance
    logger.info(f"TradingView failed for {symbol}, trying yfinance...")
    try:
        import yfinance as yf
        
        ticker = yf.Ticker(symbol)
        info = ticker.info
        
        if not info or info.get('regularMarketPrice') is None:
            logger.warning(f"No data available for {symbol} from yfinance either")
            return None
        
        # Extract key metrics - use our sector map as primary source
        sector = STOCK_SECTOR_MAP.get(symbol.upper()) or info.get('sector', 'Unknown')
        data = {
            'symbol': symbol.upper(),
            'name': info.get('longName') or info.get('shortName') or symbol,
            'sector': sector,
            'industry': info.get('industry', 'Unknown'),
            'price': info.get('regularMarketPrice') or info.get('currentPrice', 0),
            'change_pct': info.get('regularMarketChangePercent', 0),
            'market_cap': info.get('marketCap', 0),
            
            # Valuation metrics
            'pe_ratio': info.get('trailingPE'),
            'forward_pe': info.get('forwardPE'),
            'peg_ratio': info.get('pegRatio'),
            'price_to_book': info.get('priceToBook'),
            'price_to_sales': info.get('priceToSalesTrailing12Months'),
            'ev_to_ebitda': info.get('enterpriseToEbitda'),
            'ev_to_revenue': info.get('enterpriseToRevenue'),
            
            # Profitability metrics
            'profit_margin': info.get('profitMargins'),
            'operating_margin': info.get('operatingMargins'),
            'gross_margin': info.get('grossMargins'),
            'roe': info.get('returnOnEquity'),
            'roa': info.get('returnOnAssets'),
            
            # Growth metrics
            'revenue_growth': info.get('revenueGrowth'),
            'earnings_growth': info.get('earningsGrowth'),
            'earnings_quarterly_growth': info.get('earningsQuarterlyGrowth'),
            
            # Momentum metrics (price performance)
            'fifty_two_week_high': info.get('fiftyTwoWeekHigh'),
            'fifty_two_week_low': info.get('fiftyTwoWeekLow'),
            'fifty_day_average': info.get('fiftyDayAverage'),
            'two_hundred_day_average': info.get('twoHundredDayAverage'),
            
            # EPS data
            'trailing_eps': info.get('trailingEps'),
            'forward_eps': info.get('forwardEps'),
            
            # Dividend data
            'dividend_yield': info.get('dividendYield'),
            'payout_ratio': info.get('payoutRatio'),
            
            # Analyst data
            'recommendation_mean': info.get('recommendationMean'),  # 1=Strong Buy, 5=Sell
            'number_of_analyst_opinions': info.get('numberOfAnalystOpinions'),
            'target_mean_price': info.get('targetMeanPrice'),
        }
        
        # Cache the data
        _quant_data_cache[cache_key] = data
        _quant_cache_expiry[cache_key] = current_time + QUANT_CACHE_DURATION
        
        logger.info(f"Fetched real data for {symbol}: P/E={data.get('pe_ratio')}, Sector={data.get('sector')}")
        return data
        
    except Exception as e:
        logger.error(f"Error fetching data for {symbol}: {e}")
        return None


def calculate_valuation_grade(stock_data, sector_medians):
    """
    Calculate valuation grade based on P/E, P/B, P/S, EV/EBITDA vs sector.
    Lower valuations = better grades (value investing approach).
    Uses Seeking Alpha-style grading: compares % difference to sector median.
    """
    if not stock_data:
        return {'grade': 'C', 'score': 0.5, 'metrics': {}}
    
    scores = []
    metrics = {}
    
    def value_metric_to_score(value, sector_median):
        """
        Convert a valuation metric to a score (0-1).
        For valuation, LOWER is BETTER.
        Score thresholds based on Seeking Alpha methodology:
        - 50%+ below sector = A+ (score ~0.95)
        - 25% below = A (score ~0.85)
        - At sector median = C (score ~0.50)
        - 25% above = D (score ~0.25)
        - 50%+ above = F (score ~0.05)
        """
        if not value or value <= 0 or not sector_median or sector_median <= 0:
            return None
        
        diff_pct = ((value - sector_median) / sector_median) * 100
        
        # Stricter scoring: penalize being above sector median heavily
        if diff_pct <= -50:
            return 0.95  # 50%+ cheaper = A+
        elif diff_pct <= -25:
            return 0.80 + ((-25 - diff_pct) / 25) * 0.15  # A range
        elif diff_pct <= 0:
            return 0.50 + (-diff_pct / 25) * 0.30  # B to C range (at median = 0.50)
        elif diff_pct <= 25:
            return 0.35 - (diff_pct / 25) * 0.15  # C- to D+ range
        elif diff_pct <= 50:
            return 0.20 - ((diff_pct - 25) / 25) * 0.10  # D range
        elif diff_pct <= 100:
            return 0.10 - ((diff_pct - 50) / 50) * 0.05  # D- to F range
        else:
            return 0.05  # F for anything 100%+ above median
    
    # P/E Ratio (lower is better for value) - HEAVILY WEIGHTED
    pe = stock_data.get('pe_ratio')
    sector_pe = sector_medians.get('pe_ratio', 24.5)
    if pe and pe > 0:
        pe_diff_pct = ((pe - sector_pe) / sector_pe) * 100
        metrics['pe_ratio'] = {'value': round(pe, 2), 'sector': sector_pe, 'diff_pct': round(pe_diff_pct, 1)}
        pe_score = value_metric_to_score(pe, sector_pe)
        if pe_score is not None:
            scores.append(pe_score * 1.5)  # Weight P/E higher
    
    # Forward P/E
    fwd_pe = stock_data.get('forward_pe')
    sector_fwd_pe = sector_medians.get('forward_pe', 22)
    if fwd_pe and fwd_pe > 0:
        fwd_pe_diff = ((fwd_pe - sector_fwd_pe) / sector_fwd_pe) * 100
        metrics['forward_pe'] = {'value': round(fwd_pe, 2), 'sector': sector_fwd_pe, 'diff_pct': round(fwd_pe_diff, 1)}
        fwd_pe_score = value_metric_to_score(fwd_pe, sector_fwd_pe)
        if fwd_pe_score is not None:
            scores.append(fwd_pe_score * 1.3)  # Weight forward P/E
    
    # PEG Ratio (lower is better)
    peg = stock_data.get('peg_ratio')
    sector_peg = sector_medians.get('peg_ratio', 1.0)
    if peg and peg > 0:
        peg_diff = ((peg - sector_peg) / sector_peg) * 100
        metrics['peg_ratio'] = {'value': round(peg, 2), 'sector': sector_peg, 'diff_pct': round(peg_diff, 1)}
        peg_score = value_metric_to_score(peg, sector_peg)
        if peg_score is not None:
            scores.append(peg_score)
    
    # Price to Book (lower is better) - HEAVILY WEIGHTED
    pb = stock_data.get('price_to_book')
    sector_pb = sector_medians.get('price_to_book', 3.6)
    if pb and pb > 0:
        pb_diff = ((pb - sector_pb) / sector_pb) * 100
        metrics['price_to_book'] = {'value': round(pb, 2), 'sector': sector_pb, 'diff_pct': round(pb_diff, 1)}
        pb_score = value_metric_to_score(pb, sector_pb)
        if pb_score is not None:
            scores.append(pb_score * 1.5)  # Weight P/B higher
    
    # Price to Sales (lower is better)
    ps = stock_data.get('price_to_sales')
    sector_ps = sector_medians.get('price_to_sales', 3.3)
    if ps and ps > 0:
        ps_diff = ((ps - sector_ps) / sector_ps) * 100
        metrics['price_to_sales'] = {'value': round(ps, 2), 'sector': sector_ps, 'diff_pct': round(ps_diff, 1)}
        ps_score = value_metric_to_score(ps, sector_ps)
        if ps_score is not None:
            scores.append(ps_score * 1.2)
    
    # EV/EBITDA (lower is better)
    ev_ebitda = stock_data.get('ev_to_ebitda')
    sector_ev = sector_medians.get('ev_to_ebitda', 15.0)
    if ev_ebitda and ev_ebitda > 0:
        ev_diff = ((ev_ebitda - sector_ev) / sector_ev) * 100
        metrics['ev_to_ebitda'] = {'value': round(ev_ebitda, 2), 'sector': sector_ev, 'diff_pct': round(ev_diff, 1)}
        ev_score = value_metric_to_score(ev_ebitda, sector_ev)
        if ev_score is not None:
            scores.append(ev_score * 1.2)
    
    # Calculate weighted average score (normalize by total weights)
    if scores:
        # Scores already have weights applied, so normalize
        total_weight = 1.5 + 1.3 + 1.0 + 1.5 + 1.2 + 1.2  # Sum of all weights
        avg_score = sum(scores) / total_weight
        avg_score = max(0, min(1, avg_score))  # Clamp to 0-1
    else:
        avg_score = 0.5
    
    grade = percentile_to_grade(avg_score * 100)
    
    return {'grade': grade, 'score': round(avg_score, 3), 'metrics': metrics}


def calculate_growth_grade(stock_data, sector_medians):
    """
    Calculate growth grade based on revenue growth, earnings growth, etc.
    Higher growth = better grades.
    Uses Seeking Alpha-style grading.
    """
    if not stock_data:
        return {'grade': 'C', 'score': 0.5, 'metrics': {}}
    
    scores = []
    metrics = {}
    
    def growth_metric_to_score(value_pct, sector_pct):
        """
        Convert a growth metric to score using Seeking Alpha's methodology.
        They use RELATIVE % difference from sector median.
        Examples from SA: NVDA 65% revenue vs 9% sector = +620% = A+
                         AAPL 6% revenue vs 9% sector = -29% = C
        """
        if sector_pct <= 0:
            sector_pct = 1  # Avoid division by zero
        
        # Calculate relative difference (not absolute)
        rel_diff_pct = ((value_pct - sector_pct) / abs(sector_pct)) * 100
        
        # Seeking Alpha grading based on relative % difference
        if rel_diff_pct >= 300:  # 300%+ above sector = A+
            return 0.95
        elif rel_diff_pct >= 150:  # 150-300% = A
            return 0.85 + ((rel_diff_pct - 150) / 150) * 0.10
        elif rel_diff_pct >= 50:  # 50-150% = B range
            return 0.65 + ((rel_diff_pct - 50) / 100) * 0.20
        elif rel_diff_pct >= 0:  # 0-50% above = C+ to B-
            return 0.50 + (rel_diff_pct / 50) * 0.15
        elif rel_diff_pct >= -30:  # 0 to -30% = C range
            return 0.35 + ((rel_diff_pct + 30) / 30) * 0.15
        elif rel_diff_pct >= -50:  # -30 to -50% = D range
            return 0.20 + ((rel_diff_pct + 50) / 20) * 0.15
        else:  # Below -50% = F
            return max(0.05, 0.20 + (rel_diff_pct + 50) / 100)
    
    # Revenue Growth (higher is better) - HEAVILY WEIGHTED
    rev_growth = stock_data.get('revenue_growth')
    sector_rev = sector_medians.get('revenue_growth', 0.08)
    if rev_growth is not None:
        rev_growth_pct = rev_growth * 100
        sector_rev_pct = sector_rev * 100
        diff_pct = rev_growth_pct - sector_rev_pct
        metrics['revenue_growth'] = {'value': round(rev_growth_pct, 2), 'sector': round(sector_rev_pct, 2), 'diff_pct': round(diff_pct, 1)}
        rev_score = growth_metric_to_score(rev_growth_pct, sector_rev_pct)
        scores.append(rev_score * 1.5)  # Weight revenue growth
    
    # Earnings Growth (higher is better) - HEAVILY WEIGHTED
    earn_growth = stock_data.get('earnings_growth')
    sector_earn = sector_medians.get('earnings_growth', 0.10)
    if earn_growth is not None:
        earn_growth_pct = earn_growth * 100
        sector_earn_pct = sector_earn * 100
        diff_pct = earn_growth_pct - sector_earn_pct
        metrics['earnings_growth'] = {'value': round(earn_growth_pct, 2), 'sector': round(sector_earn_pct, 2), 'diff_pct': round(diff_pct, 1)}
        earn_score = growth_metric_to_score(earn_growth_pct, sector_earn_pct)
        scores.append(earn_score * 1.5)  # Weight earnings growth
    
    # Quarterly Earnings Growth
    q_growth = stock_data.get('earnings_quarterly_growth')
    if q_growth is not None:
        q_growth_pct = q_growth * 100
        metrics['earnings_quarterly_growth'] = {'value': round(q_growth_pct, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        # Compare to 0% as baseline (any positive growth is good)
        q_score = growth_metric_to_score(q_growth_pct, 0)
        scores.append(q_score)
    
    # EPS Growth (Forward EPS vs Trailing EPS)
    trailing_eps = stock_data.get('trailing_eps')
    forward_eps = stock_data.get('forward_eps')
    if trailing_eps and forward_eps and trailing_eps > 0:
        eps_growth_pct = ((forward_eps - trailing_eps) / abs(trailing_eps)) * 100
        metrics['eps_growth_fwd'] = {'value': round(eps_growth_pct, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        eps_score = growth_metric_to_score(eps_growth_pct, sector_earn_pct)
        scores.append(eps_score * 1.2)  # Weight EPS growth
    
    # Calculate weighted average
    if scores:
        total_weight = 1.5 + 1.5 + 1.0 + 1.2  # Sum of weights
        avg_score = sum(scores) / total_weight
        avg_score = max(0, min(1, avg_score))
    else:
        avg_score = 0.5
    
    grade = percentile_to_grade(avg_score * 100)
    
    return {'grade': grade, 'score': round(avg_score, 3), 'metrics': metrics}


def calculate_profitability_grade(stock_data, sector_medians):
    """
    Calculate profitability grade based on margins, ROE, ROA, etc.
    Higher profitability = better grades.
    Uses Seeking Alpha-style grading.
    """
    if not stock_data:
        return {'grade': 'C', 'score': 0.5, 'metrics': {}}
    
    scores = []
    metrics = {}
    
    def profit_metric_to_score(value_pct, sector_pct):
        """
        Convert a profitability metric to score using Seeking Alpha's methodology.
        They use RELATIVE % difference from sector median.
        Examples from SA: AAPL ROE 171% vs 6.33% sector = +2606% = A+
                         AAPL Net Margin 27% vs 4.81% = +459% = A
        """
        if sector_pct <= 0:
            sector_pct = 0.01  # Avoid division by zero, use small positive
        
        # Calculate relative difference (not absolute)
        rel_diff_pct = ((value_pct - sector_pct) / abs(sector_pct)) * 100
        
        # Seeking Alpha grading based on relative % difference
        if rel_diff_pct >= 500:  # 500%+ above sector = A+
            return 0.95
        elif rel_diff_pct >= 200:  # 200-500% = A
            return 0.85 + ((rel_diff_pct - 200) / 300) * 0.10
        elif rel_diff_pct >= 50:  # 50-200% = B range
            return 0.65 + ((rel_diff_pct - 50) / 150) * 0.20
        elif rel_diff_pct >= 0:  # 0-50% above = C+ to B-
            return 0.50 + (rel_diff_pct / 50) * 0.15
        elif rel_diff_pct >= -25:  # 0 to -25% = C range  
            return 0.35 + ((rel_diff_pct + 25) / 25) * 0.15
        elif rel_diff_pct >= -50:  # -25 to -50% = D range
            return 0.20 + ((rel_diff_pct + 50) / 25) * 0.15
        else:  # Below -50% = F
            return max(0.05, 0.20 + (rel_diff_pct + 50) / 100)
    
    # Gross Margin (higher is better) - MOST IMPORTANT
    gross_margin = stock_data.get('gross_margin')
    sector_gm = sector_medians.get('gross_margin', 0.50)
    if gross_margin is not None:
        gm_pct = gross_margin * 100
        sector_gm_pct = sector_gm * 100
        diff_pct = gm_pct - sector_gm_pct
        metrics['gross_margin'] = {'value': round(gm_pct, 2), 'sector': round(sector_gm_pct, 2), 'diff_pct': round(diff_pct, 1)}
        gm_score = profit_metric_to_score(gm_pct, sector_gm_pct)
        scores.append(gm_score * 1.5)  # Weight gross margin
    
    # Operating Margin (higher is better)
    op_margin = stock_data.get('operating_margin')
    sector_om = sector_medians.get('operating_margin', 0.22)
    if op_margin is not None:
        om_pct = op_margin * 100
        sector_om_pct = sector_om * 100
        diff_pct = om_pct - sector_om_pct
        metrics['operating_margin'] = {'value': round(om_pct, 2), 'sector': round(sector_om_pct, 2), 'diff_pct': round(diff_pct, 1)}
        om_score = profit_metric_to_score(om_pct, sector_om_pct)
        scores.append(om_score * 1.3)
    
    # Profit Margin (Net) (higher is better)
    profit_margin = stock_data.get('profit_margin')
    sector_pm = sector_medians.get('profit_margin', 0.18)
    if profit_margin is not None:
        pm_pct = profit_margin * 100
        sector_pm_pct = sector_pm * 100
        diff_pct = pm_pct - sector_pm_pct
        metrics['profit_margin'] = {'value': round(pm_pct, 2), 'sector': round(sector_pm_pct, 2), 'diff_pct': round(diff_pct, 1)}
        pm_score = profit_metric_to_score(pm_pct, sector_pm_pct)
        scores.append(pm_score * 1.3)
    
    # Return on Equity (higher is better) - VERY IMPORTANT
    roe = stock_data.get('roe')
    sector_roe = sector_medians.get('roe', 0.25)
    if roe is not None:
        roe_pct = roe * 100
        sector_roe_pct = sector_roe * 100
        diff_pct = roe_pct - sector_roe_pct
        metrics['roe'] = {'value': round(roe_pct, 2), 'sector': round(sector_roe_pct, 2), 'diff_pct': round(diff_pct, 1)}
        roe_score = profit_metric_to_score(roe_pct, sector_roe_pct)
        scores.append(roe_score * 1.5)  # Weight ROE highly
    
    # Return on Assets (higher is better)
    roa = stock_data.get('roa')
    sector_roa = sector_medians.get('roa', 0.12)
    if roa is not None:
        roa_pct = roa * 100
        sector_roa_pct = sector_roa * 100
        diff_pct = roa_pct - sector_roa_pct
        metrics['roa'] = {'value': round(roa_pct, 2), 'sector': round(sector_roa_pct, 2), 'diff_pct': round(diff_pct, 1)}
        roa_score = profit_metric_to_score(roa_pct, sector_roa_pct)
        scores.append(roa_score)
    
    # Calculate weighted average
    if scores:
        total_weight = 1.5 + 1.3 + 1.3 + 1.5 + 1.0  # Gross, Operating, Net, ROE, ROA
        avg_score = sum(scores) / total_weight
        avg_score = max(0, min(1, avg_score))
    else:
        avg_score = 0.5
    
    grade = percentile_to_grade(avg_score * 100)
    
    return {'grade': grade, 'score': round(avg_score, 3), 'metrics': metrics}


def calculate_momentum_grade(stock_data, sector_medians):
    """
    Calculate momentum grade based on price performance vs moving averages.
    Strong price momentum = better grades.
    Uses Seeking Alpha-style grading based on:
    - Position in 52-week range
    - Price vs moving averages (50-day, 200-day)
    - Recent price performance
    """
    if not stock_data:
        return {'grade': 'C', 'score': 0.5, 'metrics': {}}
    
    scores = []
    metrics = {}
    
    def momentum_to_score(pct_value, scale=20):
        """
        Convert a momentum percentage to a score.
        Positive = good momentum, Negative = bad momentum.
        scale: the % at which you get an A+ (default 20%)
        """
        if pct_value >= scale:
            return 0.95
        elif pct_value >= scale * 0.5:
            return 0.75 + ((pct_value - scale * 0.5) / (scale * 0.5)) * 0.20
        elif pct_value >= 0:
            return 0.50 + (pct_value / (scale * 0.5)) * 0.25
        elif pct_value >= -scale * 0.5:
            return 0.30 + ((pct_value + scale * 0.5) / (scale * 0.5)) * 0.20
        elif pct_value >= -scale:
            return 0.15 + ((pct_value + scale) / (scale * 0.5)) * 0.15
        else:
            return max(0.05, 0.15 + (pct_value + scale) / (scale * 2))
    
    price = stock_data.get('price', 0)
    
    # Price vs 52-week high (higher % of high = better momentum) - HEAVILY WEIGHTED
    high_52 = stock_data.get('fifty_two_week_high')
    low_52 = stock_data.get('fifty_two_week_low')
    if high_52 and low_52 and price:
        range_52 = high_52 - low_52
        if range_52 > 0:
            position_in_range = (price - low_52) / range_52
            pct_from_high = ((price - high_52) / high_52) * 100
            metrics['52w_position'] = {'value': round(position_in_range * 100, 1), 'sector': 'N/A', 'diff_pct': 'N/A'}
            metrics['pct_from_52w_high'] = {'value': round(pct_from_high, 1), 'sector': 'N/A', 'diff_pct': 'N/A'}
            # Score based on position: 0.9+ in range = excellent
            pos_score = 0.05 + position_in_range * 0.90
            scores.append(pos_score * 1.5)  # Weight heavily
    
    # Price vs 50-day MA - IMPORTANT
    ma_50 = stock_data.get('fifty_day_average')
    if ma_50 and price and ma_50 > 0:
        pct_above_50 = ((price - ma_50) / ma_50) * 100
        metrics['vs_50d_ma'] = {'value': round(pct_above_50, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        ma50_score = momentum_to_score(pct_above_50, scale=15)
        scores.append(ma50_score * 1.3)
    
    # Price vs 200-day MA - IMPORTANT
    ma_200 = stock_data.get('two_hundred_day_average')
    if ma_200 and price and ma_200 > 0:
        pct_above_200 = ((price - ma_200) / ma_200) * 100
        metrics['vs_200d_ma'] = {'value': round(pct_above_200, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        ma200_score = momentum_to_score(pct_above_200, scale=25)
        scores.append(ma200_score * 1.2)
    
    # Recent price change (daily) - Lower weight
    change_pct = stock_data.get('change_pct', 0)
    if change_pct:
        metrics['daily_change'] = {'value': round(change_pct, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        change_score = momentum_to_score(change_pct, scale=5)
        scores.append(change_score * 0.5)
    
    # Calculate weighted average
    if scores:
        total_weight = 1.5 + 1.3 + 1.2 + 0.5  # 52w, 50ma, 200ma, daily
        avg_score = sum(scores) / total_weight
        avg_score = max(0, min(1, avg_score))
    else:
        avg_score = 0.5
    
    grade = percentile_to_grade(avg_score * 100)
    
    return {'grade': grade, 'score': round(avg_score, 3), 'metrics': metrics}


def calculate_eps_revisions_grade(stock_data, sector_medians):
    """
    Calculate EPS revisions grade based on analyst sentiment.
    Uses recommendation mean and target price vs current price.
    This approximates Seeking Alpha's "EPS Revisions" which tracks
    how analyst estimates have changed over time.
    """
    if not stock_data:
        return {'grade': 'C', 'score': 0.5, 'metrics': {}}
    
    scores = []
    metrics = {}
    
    def analyst_rating_to_score(rating):
        """
        Convert analyst rating (1-5 scale) to score.
        1.0 = Strong Buy -> 0.95
        2.0 = Buy -> 0.75
        3.0 = Hold -> 0.50
        4.0 = Sell -> 0.25
        5.0 = Strong Sell -> 0.05
        """
        if rating <= 1.5:
            return 0.90 + (1.5 - rating) * 0.10
        elif rating <= 2.0:
            return 0.75 + (2.0 - rating) * 0.30
        elif rating <= 2.5:
            return 0.60 + (2.5 - rating) * 0.30
        elif rating <= 3.0:
            return 0.45 + (3.0 - rating) * 0.30
        elif rating <= 3.5:
            return 0.30 + (3.5 - rating) * 0.30
        elif rating <= 4.0:
            return 0.15 + (4.0 - rating) * 0.30
        else:
            return max(0.05, 0.15 - (rating - 4.0) * 0.10)
    
    def upside_to_score(upside_pct):
        """
        Convert price target upside to score.
        >30% upside = A+ (0.95)
        20% upside = A (0.80)
        10% upside = B (0.65)
        0% upside = C (0.50)
        -10% = D (0.30)
        -20%+ = F (0.10)
        """
        if upside_pct >= 30:
            return 0.95
        elif upside_pct >= 20:
            return 0.80 + ((upside_pct - 20) / 10) * 0.15
        elif upside_pct >= 10:
            return 0.65 + ((upside_pct - 10) / 10) * 0.15
        elif upside_pct >= 0:
            return 0.50 + (upside_pct / 10) * 0.15
        elif upside_pct >= -10:
            return 0.30 + ((upside_pct + 10) / 10) * 0.20
        elif upside_pct >= -20:
            return 0.15 + ((upside_pct + 20) / 10) * 0.15
        else:
            return max(0.05, 0.15 + (upside_pct + 20) / 40)
    
    # Analyst Recommendation (1=Strong Buy, 5=Strong Sell) - HEAVILY WEIGHTED
    rec_mean = stock_data.get('recommendation_mean')
    if rec_mean:
        metrics['analyst_rating'] = {'value': round(rec_mean, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        rec_score = analyst_rating_to_score(rec_mean)
        scores.append(rec_score * 1.5)  # Weight analyst rating
    
    # Target Price vs Current Price - IMPORTANT
    target_price = stock_data.get('target_mean_price')
    current_price = stock_data.get('price', 0)
    if target_price and current_price and current_price > 0:
        upside_pct = ((target_price - current_price) / current_price) * 100
        metrics['price_target_upside'] = {'value': round(upside_pct, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        target_score = upside_to_score(upside_pct)
        scores.append(target_score * 1.3)
    
    # Number of analysts covering (more coverage = more reliable signal)
    num_analysts = stock_data.get('number_of_analyst_opinions', 0)
    if num_analysts:
        metrics['analyst_coverage'] = {'value': num_analysts, 'sector': 'N/A', 'diff_pct': 'N/A'}
        # Coverage modifier: 10+ analysts = full weight, fewer = reduced confidence
        coverage_modifier = min(1.0, num_analysts / 10)
        metrics['coverage_confidence'] = {'value': round(coverage_modifier * 100, 0), 'sector': 'N/A', 'diff_pct': 'N/A'}
    
    # Forward EPS vs Trailing EPS (positive revision indicator)
    trailing_eps = stock_data.get('trailing_eps')
    forward_eps = stock_data.get('forward_eps')
    if trailing_eps and forward_eps and trailing_eps > 0:
        eps_revision_pct = ((forward_eps - trailing_eps) / abs(trailing_eps)) * 100
        metrics['eps_revision'] = {'value': round(eps_revision_pct, 2), 'sector': 'N/A', 'diff_pct': 'N/A'}
        # Positive revision = good, negative = bad
        if eps_revision_pct >= 20:
            revision_score = 0.90
        elif eps_revision_pct >= 10:
            revision_score = 0.70 + ((eps_revision_pct - 10) / 10) * 0.20
        elif eps_revision_pct >= 0:
            revision_score = 0.50 + (eps_revision_pct / 10) * 0.20
        elif eps_revision_pct >= -10:
            revision_score = 0.30 + ((eps_revision_pct + 10) / 10) * 0.20
        else:
            revision_score = max(0.10, 0.30 + (eps_revision_pct + 10) / 30)
        scores.append(revision_score * 1.2)
    
    # Calculate weighted average
    if scores:
        total_weight = 1.5 + 1.3 + 1.2  # Analyst rating, target price, EPS revision
        avg_score = sum(scores) / total_weight
        avg_score = max(0, min(1, avg_score))
    else:
        avg_score = 0.5
    
    grade = percentile_to_grade(avg_score * 100)
    
    return {'grade': grade, 'score': round(avg_score, 3), 'metrics': metrics}


def get_real_factor_grades(symbol):
    """
    Get real factor grades for a stock based on actual financial data.
    This replaces the fake generate_factor_grade() function.
    """
    stock_data = get_real_stock_data(symbol)
    
    if not stock_data:
        # Fallback to random if data unavailable
        logger.warning(f"No real data for {symbol}, using fallback")
        import random
        random.seed(hash(symbol))
        return {
            'value': {'grade': percentile_to_grade(random.random() * 100), 'score': round(random.random(), 3), 'metrics': {}},
            'growth': {'grade': percentile_to_grade(random.random() * 100), 'score': round(random.random(), 3), 'metrics': {}},
            'profitability': {'grade': percentile_to_grade(random.random() * 100), 'score': round(random.random(), 3), 'metrics': {}},
            'momentum': {'grade': percentile_to_grade(random.random() * 100), 'score': round(random.random(), 3), 'metrics': {}},
            'eps_revisions': {'grade': percentile_to_grade(random.random() * 100), 'score': round(random.random(), 3), 'metrics': {}},
            '_data_source': 'fallback'
        }
    
    # Get sector medians - use STOCK_SECTOR_MAP as primary source
    sector = STOCK_SECTOR_MAP.get(symbol.upper())  # Our reliable map FIRST
    if not sector:
        sector = stock_data.get('sector', 'Default')
    if sector == 'Unknown' or not sector:
        sector = 'Default'
    sector_medians = SECTOR_MEDIANS.get(sector, SECTOR_MEDIANS['Default'])
    
    # Calculate all factor grades
    factors = {
        'value': calculate_valuation_grade(stock_data, sector_medians),
        'growth': calculate_growth_grade(stock_data, sector_medians),
        'profitability': calculate_profitability_grade(stock_data, sector_medians),
        'momentum': calculate_momentum_grade(stock_data, sector_medians),
        'eps_revisions': calculate_eps_revisions_grade(stock_data, sector_medians),
        '_stock_data': {
            'name': stock_data.get('name'),
            'sector': sector,
            'industry': stock_data.get('industry'),
            'price': stock_data.get('price'),
            'change_pct': stock_data.get('change_pct'),
            'market_cap': stock_data.get('market_cap')
        },
        '_data_source': 'yfinance'
    }
    
    return factors


def generate_factor_grade():
    """Generate a random factor grade with score"""
    import random
    score = random.random()
    grade = percentile_to_grade(score * 100)
    return {'score': round(score, 2), 'grade': grade}


def grade_meets_minimum(grade, min_grade):
    """Check if a grade meets the minimum requirement"""
    grade_order = {
        'A+': 12, 'A': 11, 'A-': 10,
        'B+': 9, 'B': 8, 'B-': 7,
        'C+': 6, 'C': 5, 'C-': 4,
        'D+': 3, 'D': 2,
        'F': 1
    }
    
    grade_val = grade_order.get(grade, 0)
    min_val = grade_order.get(min_grade, 0)
    
    if len(min_grade) == 1:
        min_val = grade_order.get(min_grade + '-', grade_order.get(min_grade, 0))
    
    return grade_val >= min_val


def generate_single_stock_factors(symbol):
    """Generate detailed factor data for a single stock"""
    import random
    
    stock_info = {
        'AAPL': {'name': 'Apple Inc.', 'sector': 'Technology'},
        'MSFT': {'name': 'Microsoft Corporation', 'sector': 'Technology'},
        'GOOGL': {'name': 'Alphabet Inc.', 'sector': 'Communication Services'},
        'AMZN': {'name': 'Amazon.com Inc.', 'sector': 'Consumer Discretionary'},
        'NVDA': {'name': 'NVIDIA Corporation', 'sector': 'Technology'},
        'META': {'name': 'Meta Platforms Inc.', 'sector': 'Communication Services'},
        'TSLA': {'name': 'Tesla Inc.', 'sector': 'Consumer Discretionary'},
    }
    
    info = stock_info.get(symbol, {'name': f'{symbol} Inc.', 'sector': 'Unknown'})
    
    random.seed(hash(symbol))
    
    factors = {
        'value': generate_factor_grade(),
        'growth': generate_factor_grade(),
        'profitability': generate_factor_grade(),
        'momentum': generate_factor_grade(),
        'eps_revisions': generate_factor_grade()
    }
    
    quant_score = (
        factors['value']['score'] * 0.20 +
        factors['growth']['score'] * 0.20 +
        factors['profitability']['score'] * 0.25 +
        factors['momentum']['score'] * 0.20 +
        factors['eps_revisions']['score'] * 0.15
    )
    
    if quant_score >= 0.8:
        rating = 'Strong Buy'
    elif quant_score >= 0.6:
        rating = 'Buy'
    elif quant_score >= 0.4:
        rating = 'Hold'
    elif quant_score >= 0.2:
        rating = 'Sell'
    else:
        rating = 'Strong Sell'
    
    price = 100 + random.random() * 200
    change = (random.random() - 0.5) * 6
    
    return {
        'symbol': symbol,
        'name': info['name'],
        'sector': info['sector'],
        'price': round(price, 2),
        'change': round(change, 2),
        'quant_score': round(quant_score, 2),
        'rating': rating,
        'factors': factors
    }


def generate_quant_stock_data(min_score=0, min_rating='', value_grade='', growth_grade='',
                               profitability_grade='', momentum_grade='', eps_revisions_grade='',
                               sector='', market_cap='', min_price=0, max_price=None):
    """Generate Seeking Alpha style quant stock data with REAL financial data from yfinance."""
    import random
    
    stocks = get_stock_universe()
    
    # Fetch live prices as fallback
    all_symbols = [s['symbol'] for s in stocks]
    live_prices = fetch_live_stock_prices(all_symbols)
    
    results = []
    
    for stock in stocks:
        symbol = stock['symbol']
        
        if sector and stock['sector'] != sector:
            continue
        if market_cap and stock['market_cap'] != market_cap:
            continue
        
        # Get REAL factor grades from yfinance
        real_factors = get_real_factor_grades(symbol)
        stock_data = real_factors.get('_stock_data', {})
        data_source = real_factors.get('_data_source', 'fallback')
        
        # Use real price from yfinance if available, otherwise use TradingView prices
        if stock_data.get('price'):
            price = stock_data['price']
            change = stock_data.get('change_pct', 0)
        elif symbol in live_prices:
            price = live_prices[symbol].get('price', 0)
            change = live_prices[symbol].get('change_pct', 0)
        else:
            random.seed(hash(symbol) + 1)
            price = 50 + random.random() * 450
            change = (random.random() - 0.5) * 8
        
        if price <= 0:
            continue
        
        # Extract factor grades (excluding metadata keys)
        factors = {
            'value': real_factors.get('value', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'growth': real_factors.get('growth', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'profitability': real_factors.get('profitability', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'momentum': real_factors.get('momentum', {'grade': 'C', 'score': 0.5, 'metrics': {}}),
            'eps_revisions': real_factors.get('eps_revisions', {'grade': 'C', 'score': 0.5, 'metrics': {}})
        }
        
        # Calculate quant score (1.0-5.0 scale) based on REAL scores
        quant_score = (
            (factors['value']['score'] * 4 + 1) * 0.20 +
            (factors['growth']['score'] * 4 + 1) * 0.20 +
            (factors['profitability']['score'] * 4 + 1) * 0.25 +
            (factors['momentum']['score'] * 4 + 1) * 0.20 +
            (factors['eps_revisions']['score'] * 4 + 1) * 0.15
        )
        
        rating = score_to_quant_rating(quant_score)
        
        # Apply filters
        if min_score > 0:
            effective_min = min_score if min_score > 1 else 1.0 + min_score * 4.0
            if quant_score < effective_min:
                continue
        
        if min_rating:
            rating_order = {'strong_buy': 5, 'buy': 4, 'hold': 3, 'sell': 2, 'strong_sell': 1}
            min_rating_val = rating_order.get(min_rating, 0)
            current_rating_val = rating_order.get(rating.lower().replace(' ', '_'), 0)
            if current_rating_val < min_rating_val:
                continue
        
        if value_grade and not grade_meets_minimum(factors['value']['grade'], value_grade):
            continue
        if growth_grade and not grade_meets_minimum(factors['growth']['grade'], growth_grade):
            continue
        if profitability_grade and not grade_meets_minimum(factors['profitability']['grade'], profitability_grade):
            continue
        if momentum_grade and not grade_meets_minimum(factors['momentum']['grade'], momentum_grade):
            continue
        if eps_revisions_grade and not grade_meets_minimum(factors['eps_revisions']['grade'], eps_revisions_grade):
            continue
        
        if min_price and price < min_price:
            continue
        if max_price and price > max_price:
            continue
        
        # Use real company name and sector if available
        company_name = stock_data.get('name') or stock['name']
        company_sector = stock_data.get('sector') or stock['sector'].replace('_', ' ').title()
        
        results.append({
            'symbol': stock['symbol'],
            'name': company_name,
            'sector': company_sector,
            'market_cap': stock['market_cap'].title(),
            'price': round(price, 2),
            'change': round(change, 2),
            'quant_score': round(quant_score, 2),
            'rating': rating,
            'factors': factors,
            'data_source': data_source  # Track if using real or fallback data
        })
    
    results.sort(key=lambda x: x['quant_score'], reverse=True)
    return results


# ============================================================================
# LIVE STOCK PRICES FROM TRADINGVIEW
# ============================================================================

_stock_price_cache = {}
_stock_price_cache_ttl = 30


def get_tradingview_session_for_stocks():
    """Get TradingView session cookies from database"""
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute('SELECT tradingview_session FROM accounts WHERE tradingview_session IS NOT NULL LIMIT 1')
        row = cursor.fetchone()
        conn.close()
        
        if row and row['tradingview_session']:
            import json
            return json.loads(row['tradingview_session'])
        return None
    except Exception as e:
        logger.error(f"Error getting TradingView session: {e}")
        return None


def fetch_live_stock_prices(symbols: list) -> dict:
    """Fetch live stock prices from TradingView Scanner API."""
    global _stock_price_cache
    import time
    
    results = {}
    symbols_to_fetch = []
    
    current_time = time.time()
    for symbol in symbols:
        if symbol in _stock_price_cache:
            cached = _stock_price_cache[symbol]
            if current_time - cached.get('updated', 0) < _stock_price_cache_ttl:
                results[symbol] = cached
                continue
        symbols_to_fetch.append(symbol)
    
    if not symbols_to_fetch:
        return results
    
    try:
        import requests
        
        session = get_tradingview_session_for_stocks()
        cookies = {}
        if session:
            cookies = {
                'sessionid': session.get('sessionid', ''),
                'sessionid_sign': session.get('sessionid_sign', '')
            }
        
        url = "https://scanner.tradingview.com/america/scan"
        
        tv_symbols = []
        for symbol in symbols_to_fetch:
            tv_symbols.extend([
                f"NASDAQ:{symbol}",
                f"NYSE:{symbol}",
                f"AMEX:{symbol}"
            ])
        
        payload = {
            "symbols": {"tickers": tv_symbols},
            "columns": ["close", "change", "change_abs", "volume", "name"]
        }
        
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Origin': 'https://www.tradingview.com',
            'Referer': 'https://www.tradingview.com/'
        }
        
        response = requests.post(url, json=payload, headers=headers, cookies=cookies, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            for item in data.get('data', []):
                symbol_full = item.get('s', '')
                values = item.get('d', [])
                
                if len(values) >= 4:
                    symbol = symbol_full.split(':')[-1] if ':' in symbol_full else symbol_full
                    
                    if symbol in symbols_to_fetch:
                        price = values[0] if values[0] else 0
                        change_pct = values[1] if values[1] else 0
                        change_abs = values[2] if values[2] else 0
                        volume = values[3] if values[3] else 0
                        
                        price_data = {
                            'price': round(float(price), 2) if price else 0,
                            'change': round(float(change_abs), 2) if change_abs else 0,
                            'change_pct': round(float(change_pct), 2) if change_pct else 0,
                            'volume': int(volume) if volume else 0,
                            'updated': current_time
                        }
                        
                        results[symbol] = price_data
                        _stock_price_cache[symbol] = price_data
                        
    except Exception as e:
        logger.error(f"Error fetching stock prices from TradingView: {e}")
    
    return results


# ============================================================================
# LIVE FUTURES TICKER FOR DASHBOARD
# ============================================================================

_ticker_price_cache = {}
_ticker_price_cache_ttl = 15


def fetch_live_futures_prices() -> list:
    """Fetch live prices from TradingView for futures, ETFs, and stocks."""
    global _ticker_price_cache
    import time
    import requests
    
    current_time = time.time()
    
    if _ticker_price_cache.get('data') and current_time - _ticker_price_cache.get('updated', 0) < _ticker_price_cache_ttl:
        return _ticker_price_cache['data']
    
    futures_config = [
        {'tv_symbol': 'CME_MINI:ES1!', 'display': 'ES', 'name': 'S&P 500', 'type': 'futures'},
        {'tv_symbol': 'CME_MINI:NQ1!', 'display': 'NQ', 'name': 'Nasdaq', 'type': 'futures'},
        {'tv_symbol': 'CME_MINI:MES1!', 'display': 'MES', 'name': 'Micro S&P', 'type': 'futures'},
        {'tv_symbol': 'CME_MINI:MNQ1!', 'display': 'MNQ', 'name': 'Micro NQ', 'type': 'futures'},
        {'tv_symbol': 'CME_MINI:YM1!', 'display': 'YM', 'name': 'Dow', 'type': 'futures'},
        {'tv_symbol': 'CME_MINI:RTY1!', 'display': 'RTY', 'name': 'Russell', 'type': 'futures'},
        {'tv_symbol': 'CME_MINI:M2K1!', 'display': 'M2K', 'name': 'Micro Russ', 'type': 'futures'},
        {'tv_symbol': 'NYMEX:CL1!', 'display': 'CL', 'name': 'Crude Oil', 'type': 'futures'},
        {'tv_symbol': 'COMEX:GC1!', 'display': 'GC', 'name': 'Gold', 'type': 'futures'},
        {'tv_symbol': 'COMEX:SI1!', 'display': 'SI', 'name': 'Silver', 'type': 'futures'},
        {'tv_symbol': 'NYMEX:NG1!', 'display': 'NG', 'name': 'Nat Gas', 'type': 'futures'},
        {'tv_symbol': 'CBOT:ZB1!', 'display': 'ZB', 'name': '30Y Bond', 'type': 'futures'},
        {'tv_symbol': 'CBOT:ZN1!', 'display': 'ZN', 'name': '10Y Note', 'type': 'futures'},
        {'tv_symbol': 'CME:6E1!', 'display': '6E', 'name': 'Euro FX', 'type': 'futures'},
        {'tv_symbol': 'CME:BTC1!', 'display': 'BTC', 'name': 'Bitcoin', 'type': 'futures'},
    ]
    
    stocks_config = [
        {'tv_symbol': 'AMEX:SPY', 'display': 'SPY', 'name': 'S&P ETF', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:QQQ', 'display': 'QQQ', 'name': 'Nasdaq ETF', 'type': 'stock'},
        {'tv_symbol': 'AMEX:IWM', 'display': 'IWM', 'name': 'Russell ETF', 'type': 'stock'},
        {'tv_symbol': 'AMEX:DIA', 'display': 'DIA', 'name': 'Dow ETF', 'type': 'stock'},
        {'tv_symbol': 'AMEX:SPXL', 'display': 'SPXL', 'name': '3x SPY', 'type': 'stock'},
        {'tv_symbol': 'AMEX:UVXY', 'display': 'UVXY', 'name': '1.5x VIX', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:AAPL', 'display': 'AAPL', 'name': 'Apple', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:MSFT', 'display': 'MSFT', 'name': 'Microsoft', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:NVDA', 'display': 'NVDA', 'name': 'NVIDIA', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:AMZN', 'display': 'AMZN', 'name': 'Amazon', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:GOOGL', 'display': 'GOOGL', 'name': 'Google', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:META', 'display': 'META', 'name': 'Meta', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:TSLA', 'display': 'TSLA', 'name': 'Tesla', 'type': 'stock'},
        {'tv_symbol': 'NASDAQ:AMD', 'display': 'AMD', 'name': 'AMD', 'type': 'stock'},
    ]
    
    results = []
    
    try:
        session = get_tradingview_session_for_stocks()
        cookies = {}
        if session:
            cookies = {
                'sessionid': session.get('sessionid', ''),
                'sessionid_sign': session.get('sessionid_sign', '')
            }
        
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Origin': 'https://www.tradingview.com',
            'Referer': 'https://www.tradingview.com/'
        }
        
        # Fetch FUTURES
        try:
            futures_url = "https://scanner.tradingview.com/futures/scan"
            futures_symbols = [f['tv_symbol'] for f in futures_config]
            futures_payload = {"symbols": {"tickers": futures_symbols}, "columns": ["close", "change", "change_abs"]}
            
            futures_response = requests.post(futures_url, json=futures_payload, headers=headers, cookies=cookies, timeout=10)
            if futures_response.status_code == 200:
                futures_data = futures_response.json()
                config_lookup = {f['tv_symbol']: f for f in futures_config}
                
                for item in futures_data.get('data', []):
                    symbol_full = item.get('s', '')
                    values = item.get('d', [])
                    
                    if symbol_full in config_lookup and len(values) >= 2:
                        config = config_lookup[symbol_full]
                        price = values[0] if values[0] else 0
                        change_pct = values[1] if values[1] else 0
                        results.append(format_ticker_item(config, price, change_pct))
        except Exception as e:
            logger.warning(f"Error fetching futures: {e}")
        
        # Fetch STOCKS/ETFs
        try:
            stocks_url = "https://scanner.tradingview.com/america/scan"
            stocks_symbols = [f['tv_symbol'] for f in stocks_config]
            stocks_payload = {"symbols": {"tickers": stocks_symbols}, "columns": ["close", "change", "change_abs"]}
            
            stocks_response = requests.post(stocks_url, json=stocks_payload, headers=headers, cookies=cookies, timeout=10)
            if stocks_response.status_code == 200:
                stocks_data = stocks_response.json()
                config_lookup = {f['tv_symbol']: f for f in stocks_config}
                
                for item in stocks_data.get('data', []):
                    symbol_full = item.get('s', '')
                    values = item.get('d', [])
                    
                    if symbol_full in config_lookup and len(values) >= 2:
                        config = config_lookup[symbol_full]
                        price = values[0] if values[0] else 0
                        change_pct = values[1] if values[1] else 0
                        results.append(format_ticker_item(config, price, change_pct))
        except Exception as e:
            logger.warning(f"Error fetching stocks: {e}")
        
        if results:
            _ticker_price_cache = {'data': results, 'updated': current_time}
        
        return results
        
    except Exception as e:
        logger.error(f"Error fetching ticker prices: {e}")
        return []


def format_ticker_item(config, price, change_pct):
    """Format a single ticker item for display"""
    if price >= 1000:
        price_str = f"${price:,.2f}"
    elif price >= 100:
        price_str = f"${price:.2f}"
    else:
        price_str = f"${price:.4f}" if price < 10 else f"${price:.2f}"
    
    if change_pct >= 0:
        change_str = f"+{change_pct:.2f}%"
        direction = 'up'
    else:
        change_str = f"{change_pct:.2f}%"
        direction = 'down'
    
    return {
        'symbol': config['display'],
        'name': config['name'],
        'price': float(price),
        'price_str': price_str,
        'change_pct': float(change_pct),
        'change_str': change_str,
        'direction': direction
    }


# ============================================================================
# WATCHLIST DIGEST MODULE (Added Dec 18, 2025)
# Twice-daily digest with news, ratings, movers, politician trades, market context
# ============================================================================

import hashlib
from datetime import datetime, timedelta
import threading

# Database path for digest data (used for local SQLite only)
DIGEST_DB_PATH = 'watchlist_digest.db'

def init_digest_database():
    """Initialize the watchlist digest database tables - supports both SQLite and PostgreSQL"""
    is_postgres = is_using_postgres()
    conn = get_db_connection()
    cursor = conn.cursor()
    
    if is_postgres:
        # PostgreSQL schema
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS watchlist_items (
                id SERIAL PRIMARY KEY,
                ticker TEXT NOT NULL UNIQUE,
                company_name TEXT,
                cik TEXT,
                exchange TEXT,
                currency TEXT DEFAULT 'USD',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS digest_runs (
                run_id SERIAL PRIMARY KEY,
                started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                finished_at TIMESTAMP,
                run_type TEXT,
                status TEXT DEFAULT 'running',
                error TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_items (
                id SERIAL PRIMARY KEY,
                url_hash TEXT NOT NULL UNIQUE,
                ticker TEXT NOT NULL,
                source TEXT NOT NULL,
                headline TEXT NOT NULL,
                summary TEXT,
                url TEXT,
                published_at TIMESTAMP,
                inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ratings_snapshots (
                id SERIAL PRIMARY KEY,
                ticker TEXT NOT NULL,
                provider TEXT NOT NULL,
                raw_rating TEXT,
                normalized_bucket TEXT,
                as_of TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS politician_trades (
                id SERIAL PRIMARY KEY,
                source TEXT,
                chamber TEXT,
                politician TEXT NOT NULL,
                filed_at TIMESTAMP,
                txn_date TIMESTAMP,
                issuer TEXT,
                ticker_guess TEXT,
                action TEXT,
                amount_range TEXT,
                url TEXT,
                inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS quotes_snapshots (
                id SERIAL PRIMARY KEY,
                ticker TEXT NOT NULL,
                prior_close REAL,
                last_price REAL,
                pct_change REAL,
                as_of TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS market_context_cache (
                id SERIAL PRIMARY KEY,
                data_type TEXT NOT NULL,
                value TEXT,
                source TEXT,
                as_of TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
    else:
        # SQLite schema
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS watchlist_items (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL UNIQUE,
                company_name TEXT,
                cik TEXT,
                exchange TEXT,
                currency TEXT DEFAULT 'USD',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS digest_runs (
                run_id INTEGER PRIMARY KEY AUTOINCREMENT,
                started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                finished_at TIMESTAMP,
                run_type TEXT,
                status TEXT DEFAULT 'running',
                error TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_items (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url_hash TEXT NOT NULL,
                ticker TEXT NOT NULL,
                source TEXT NOT NULL,
                headline TEXT NOT NULL,
                summary TEXT,
                url TEXT,
                published_at TIMESTAMP,
                inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(url_hash)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ratings_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                provider TEXT NOT NULL,
                raw_rating TEXT,
                normalized_bucket TEXT,
                as_of TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS politician_trades (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source TEXT,
                chamber TEXT,
                politician TEXT NOT NULL,
                filed_at TIMESTAMP,
                txn_date TIMESTAMP,
                issuer TEXT,
                ticker_guess TEXT,
                action TEXT,
                amount_range TEXT,
                url TEXT,
                inserted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(politician, txn_date, ticker_guess, action)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS quotes_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                prior_close REAL,
                last_price REAL,
                pct_change REAL,
                as_of TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(ticker, DATE(as_of))
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS market_context_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                data_type TEXT NOT NULL,
                value TEXT,
                source TEXT,
                as_of TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(data_type, DATE(as_of))
            )
        ''')
    
    conn.commit()
    conn.close()
    logger.info("‚úÖ Watchlist digest database initialized")

# Initialize on import
try:
    init_digest_database()
except Exception as e:
    logger.error(f"Failed to initialize digest database: {e}")


# ============================================================================
# WATCHLIST MANAGEMENT ENDPOINTS
# ============================================================================

@app.route('/api/watchlist', methods=['GET'])
def api_get_watchlist():
    """Get all watchlist items"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM watchlist_items ORDER BY ticker')
        rows = cursor.fetchall()
        # Handle both dict-like rows (PostgreSQL) and sqlite3.Row
        if rows and hasattr(rows[0], 'keys'):
            items = [dict(row) for row in rows]
        else:
            items = [{'id': r[0], 'ticker': r[1], 'company_name': r[2]} for r in rows] if rows else []
        conn.close()
        
        return jsonify({'success': True, 'watchlist': items})
    except Exception as e:
        logger.error(f"Error getting watchlist: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/watchlist', methods=['POST'])
def api_add_to_watchlist():
    """Add a ticker to the watchlist"""
    try:
        data = request.get_json() or {}
        ticker = data.get('ticker', '').upper().strip()
        
        if not ticker:
            return jsonify({'success': False, 'error': 'Ticker is required'}), 400
        
        # Look up company info from stock universe
        company_name = data.get('company_name', '')
        if not company_name:
            universe = get_stock_universe()
            for stock in universe:
                if stock['symbol'] == ticker:
                    company_name = stock['name']
                    break
            if not company_name:
                company_name = f"{ticker} Inc."
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            cursor.execute('''
                INSERT INTO watchlist_items (ticker, company_name, updated_at)
                VALUES (%s, %s, CURRENT_TIMESTAMP)
                ON CONFLICT (ticker) DO UPDATE SET company_name = %s, updated_at = CURRENT_TIMESTAMP
            ''', (ticker, company_name, company_name))
        else:
            cursor.execute('''
                INSERT OR REPLACE INTO watchlist_items (ticker, company_name, updated_at)
                VALUES (?, ?, CURRENT_TIMESTAMP)
            ''', (ticker, company_name))
        
        conn.commit()
        conn.close()
        
        logger.info(f"‚úÖ Added {ticker} to watchlist")
        return jsonify({'success': True, 'ticker': ticker, 'company_name': company_name})
    except Exception as e:
        logger.error(f"Error adding to watchlist: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/watchlist/<ticker>', methods=['DELETE'])
def api_remove_from_watchlist(ticker):
    """Remove a ticker from the watchlist"""
    try:
        ticker = ticker.upper().strip()
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            cursor.execute('DELETE FROM watchlist_items WHERE ticker = %s', (ticker,))
        else:
            cursor.execute('DELETE FROM watchlist_items WHERE ticker = ?', (ticker,))
        deleted = cursor.rowcount
        
        conn.commit()
        conn.close()
        
        if deleted > 0:
            logger.info(f"‚úÖ Removed {ticker} from watchlist")
            return jsonify({'success': True, 'ticker': ticker})
        else:
            return jsonify({'success': False, 'error': 'Ticker not found in watchlist'}), 404
    except Exception as e:
        logger.error(f"Error removing from watchlist: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================================
# NEWS AGGREGATION
# ============================================================================

NEWS_SOURCES = [
    {'name': 'Yahoo Finance', 'base_url': 'https://finance.yahoo.com'},
    {'name': 'CNBC', 'base_url': 'https://www.cnbc.com'},
    {'name': 'Bloomberg', 'base_url': 'https://www.bloomberg.com'},
    {'name': 'Reuters', 'base_url': 'https://www.reuters.com'},
    {'name': 'WSJ', 'base_url': 'https://www.wsj.com'},
    {'name': 'MarketWatch', 'base_url': 'https://www.marketwatch.com'},
    {'name': 'CNN Money', 'base_url': 'https://money.cnn.com'},
    {'name': 'Motley Fool', 'base_url': 'https://www.fool.com'},
    {'name': 'Seeking Alpha', 'base_url': 'https://seekingalpha.com'},
]

def generate_url_hash(url):
    """Generate a unique hash for deduplication"""
    return hashlib.md5(url.encode()).hexdigest()


def normalize_headline(headline):
    """Normalize headline for deduplication fallback"""
    import re
    return re.sub(r'[^a-zA-Z0-9]', '', headline.lower())


def fetch_news_for_ticker(ticker, company_name=None):
    """Fetch news from multiple sources for a ticker"""
    import requests
    from bs4 import BeautifulSoup
    
    news_items = []
    search_terms = [ticker]
    if company_name:
        search_terms.append(company_name)
    
    # Yahoo Finance RSS/API
    try:
        yahoo_url = f"https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}&region=US&lang=en-US"
        resp = requests.get(yahoo_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            for item in soup.find_all('item')[:5]:
                title = item.find('title')
                link = item.find('link')
                pub_date = item.find('pubDate')
                desc = item.find('description')
                
                if title and link:
                    news_items.append({
                        'ticker': ticker,
                        'source': 'Yahoo Finance',
                        'headline': title.text.strip(),
                        'summary': (desc.text.strip()[:200] + '...') if desc else '',
                        'url': link.text.strip(),
                        'published_at': pub_date.text if pub_date else None,
                        'url_hash': generate_url_hash(link.text.strip())
                    })
    except Exception as e:
        logger.debug(f"Yahoo news fetch failed for {ticker}: {e}")
    
    # Seeking Alpha (public headlines)
    try:
        sa_url = f"https://seekingalpha.com/api/v3/symbols/{ticker}/news"
        resp = requests.get(sa_url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0',
            'Accept': 'application/json'
        })
        if resp.status_code == 200:
            data = resp.json()
            for article in data.get('data', [])[:5]:
                attrs = article.get('attributes', {})
                news_items.append({
                    'ticker': ticker,
                    'source': 'Seeking Alpha',
                    'headline': attrs.get('title', ''),
                    'summary': attrs.get('summary', '')[:200] + '...' if attrs.get('summary') else '',
                    'url': f"https://seekingalpha.com{attrs.get('uri', '')}",
                    'published_at': attrs.get('publishOn'),
                    'url_hash': generate_url_hash(f"https://seekingalpha.com{attrs.get('uri', '')}")
                })
    except Exception as e:
        logger.debug(f"Seeking Alpha news fetch failed for {ticker}: {e}")
    
    # MarketWatch RSS
    try:
        mw_url = f"https://www.marketwatch.com/investing/stock/{ticker.lower()}/rss"
        resp = requests.get(mw_url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.content, 'xml')
            for item in soup.find_all('item')[:5]:
                title = item.find('title')
                link = item.find('link')
                pub_date = item.find('pubDate')
                
                if title and link:
                    news_items.append({
                        'ticker': ticker,
                        'source': 'MarketWatch',
                        'headline': title.text.strip(),
                        'summary': '',
                        'url': link.text.strip(),
                        'published_at': pub_date.text if pub_date else None,
                        'url_hash': generate_url_hash(link.text.strip())
                    })
    except Exception as e:
        logger.debug(f"MarketWatch news fetch failed for {ticker}: {e}")
    
    return news_items


def deduplicate_news(news_items):
    """Deduplicate news items by URL hash and similar headlines"""
    seen_hashes = set()
    seen_headlines = set()
    unique_items = []
    
    for item in news_items:
        url_hash = item.get('url_hash', '')
        headline_norm = normalize_headline(item.get('headline', ''))
        
        if url_hash in seen_hashes:
            continue
        if headline_norm in seen_headlines:
            continue
        
        seen_hashes.add(url_hash)
        seen_headlines.add(headline_norm)
        unique_items.append(item)
    
    return unique_items


@app.route('/api/watchlist/news', methods=['GET'])
def api_get_watchlist_news():
    """Get aggregated news for all watchlist tickers"""
    try:
        # Get watchlist
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT ticker, company_name FROM watchlist_items')
        rows = cursor.fetchall()
        if rows and hasattr(rows[0], 'keys'):
            watchlist = [dict(row) for row in rows]
        else:
            watchlist = [{'ticker': r[0], 'company_name': r[1]} for r in rows] if rows else []
        conn.close()
        
        if not watchlist:
            return jsonify({'success': True, 'news': [], 'message': 'Watchlist is empty'})
        
        all_news = []
        for item in watchlist:
            ticker_news = fetch_news_for_ticker(item['ticker'], item.get('company_name'))
            all_news.extend(ticker_news)
        
        # Deduplicate
        unique_news = deduplicate_news(all_news)
        
        # Sort by published date (newest first)
        unique_news.sort(key=lambda x: x.get('published_at') or '', reverse=True)
        
        return jsonify({'success': True, 'news': unique_news[:50]})  # Limit to 50
    except Exception as e:
        logger.error(f"Error fetching watchlist news: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/watchlist/news/<ticker>', methods=['GET'])
def api_get_ticker_news(ticker):
    """Get news for a specific ticker"""
    try:
        ticker = ticker.upper().strip()
        news = fetch_news_for_ticker(ticker)
        unique_news = deduplicate_news(news)
        
        return jsonify({'success': True, 'ticker': ticker, 'news': unique_news})
    except Exception as e:
        logger.error(f"Error fetching news for {ticker}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================================
# ANALYST RATING CHANGE TRACKING
# ============================================================================

RATING_BUCKETS = {
    'strong_buy': ['strong buy', 'very bullish', 'top pick', 'conviction buy'],
    'buy': ['buy', 'outperform', 'overweight', 'moderate buy', 'accumulate', 'add', 'positive'],
    'hold': ['hold', 'neutral', 'market perform', 'equal-weight', 'sector perform'],
    'sell': ['sell', 'underperform', 'underweight', 'moderate sell', 'reduce'],
    'strong_sell': ['strong sell', 'very bearish', 'avoid']
}

def normalize_rating(raw_rating):
    """Normalize a rating to one of 5 buckets"""
    if not raw_rating:
        return None
    
    raw_lower = raw_rating.lower().strip()
    
    for bucket, keywords in RATING_BUCKETS.items():
        for keyword in keywords:
            if keyword in raw_lower:
                return bucket
    
    # Default mapping by score if numeric
    try:
        score = float(raw_rating)
        if score >= 4.5:
            return 'strong_buy'
        elif score >= 3.5:
            return 'buy'
        elif score >= 2.5:
            return 'hold'
        elif score >= 1.5:
            return 'sell'
        else:
            return 'strong_sell'
    except:
        pass
    
    return 'hold'  # Default


def get_rating_changes_since_last_run():
    """Get rating changes since the last digest run"""
    conn = get_db_connection()
    cursor = conn.cursor()
    is_postgres = is_using_postgres()
    
    # Get the last completed run timestamp
    cursor.execute('''
        SELECT finished_at FROM digest_runs 
        WHERE status = 'completed' 
        ORDER BY finished_at DESC LIMIT 1
    ''')
    last_run = cursor.fetchone()
    if last_run:
        last_run_time = last_run['finished_at'] if hasattr(last_run, 'keys') else last_run[0]
    else:
        last_run_time = '1970-01-01'
    
    # Get rating changes - simplified query for compatibility
    if is_postgres:
        cursor.execute('''
            SELECT ticker, provider, normalized_bucket as current_rating, as_of
            FROM ratings_snapshots
            WHERE as_of > %s
            ORDER BY as_of DESC
        ''', (last_run_time,))
    else:
        cursor.execute('''
            SELECT ticker, provider, normalized_bucket as current_rating, as_of
            FROM ratings_snapshots
            WHERE as_of > ?
            ORDER BY as_of DESC
        ''', (last_run_time,))
    
    rows = cursor.fetchall()
    if rows and hasattr(rows[0], 'keys'):
        changes = [dict(row) for row in rows]
    else:
        changes = [{'ticker': r[0], 'provider': r[1], 'current_rating': r[2], 'as_of': r[3]} for r in rows] if rows else []
    conn.close()
    
    return changes


@app.route('/api/watchlist/rating-changes', methods=['GET'])
def api_get_rating_changes():
    """Get analyst rating changes since last run"""
    try:
        changes = get_rating_changes_since_last_run()
        return jsonify({'success': True, 'changes': changes})
    except Exception as e:
        logger.error(f"Error getting rating changes: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================================
# MOVERS DETECTION (¬±2%)
# ============================================================================

@app.route('/api/watchlist/movers', methods=['GET'])
def api_get_movers():
    """Get watchlist tickers that moved ¬±2% or more"""
    try:
        # Get watchlist tickers
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('SELECT ticker FROM watchlist_items')
        rows = cursor.fetchall()
        if rows and hasattr(rows[0], 'keys'):
            watchlist = [row['ticker'] for row in rows]
        else:
            watchlist = [r[0] for r in rows] if rows else []
        conn.close()
        
        if not watchlist:
            return jsonify({'success': True, 'movers': [], 'message': 'Watchlist is empty'})
        
        # Fetch live prices
        live_prices = fetch_live_stock_prices(watchlist)
        
        movers = []
        for ticker in watchlist:
            if ticker in live_prices:
                price_data = live_prices[ticker]
                price = price_data.get('price', 0)
                change_pct = price_data.get('change_pct', 0)
                
                if abs(change_pct) >= 2.0:
                    movers.append({
                        'ticker': ticker,
                        'price': price,
                        'change_pct': change_pct,
                        'direction': 'up' if change_pct > 0 else 'down'
                    })
        
        # Sort by absolute change (biggest movers first)
        movers.sort(key=lambda x: abs(x['change_pct']), reverse=True)
        
        return jsonify({'success': True, 'movers': movers})
    except Exception as e:
        logger.error(f"Error getting movers: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================================
# US POLITICIAN TRADES (House + Senate)
# ============================================================================

def fetch_politician_trades():
    """Fetch recent politician trade disclosures"""
    import requests
    
    trades = []
    
    # House Stock Watcher API (community aggregated data)
    try:
        house_url = "https://house-stock-watcher-data.s3-us-west-2.amazonaws.com/data/all_transactions.json"
        resp = requests.get(house_url, timeout=15)
        if resp.status_code == 200:
            data = resp.json()
            # Get recent trades (last 30 days)
            cutoff = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            for trade in data[-500:]:  # Check last 500 entries
                if trade.get('disclosure_date', '') >= cutoff:
                    trades.append({
                        'source': 'House Stock Watcher',
                        'chamber': 'House',
                        'politician': trade.get('representative', 'Unknown'),
                        'filed_at': trade.get('disclosure_date'),
                        'txn_date': trade.get('transaction_date'),
                        'issuer': trade.get('asset_description', ''),
                        'ticker_guess': trade.get('ticker', ''),
                        'action': trade.get('type', ''),
                        'amount_range': trade.get('amount', ''),
                        'url': trade.get('ptr_link', '')
                    })
    except Exception as e:
        logger.debug(f"House trades fetch failed: {e}")
    
    # Senate Stock Watcher API
    try:
        senate_url = "https://senate-stock-watcher-data.s3-us-west-2.amazonaws.com/aggregate/all_transactions.json"
        resp = requests.get(senate_url, timeout=15)
        if resp.status_code == 200:
            data = resp.json()
            cutoff = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            for trade in data[-500:]:
                if trade.get('disclosure_date', '') >= cutoff:
                    trades.append({
                        'source': 'Senate Stock Watcher',
                        'chamber': 'Senate',
                        'politician': trade.get('senator', 'Unknown'),
                        'filed_at': trade.get('disclosure_date'),
                        'txn_date': trade.get('transaction_date'),
                        'issuer': trade.get('asset_description', ''),
                        'ticker_guess': trade.get('ticker', ''),
                        'action': trade.get('type', ''),
                        'amount_range': trade.get('amount', ''),
                        'url': trade.get('ptr_link', '')
                    })
    except Exception as e:
        logger.debug(f"Senate trades fetch failed: {e}")
    
    return trades


def get_politician_trades_for_watchlist():
    """Get politician trades matching watchlist tickers"""
    # Get watchlist tickers
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT ticker FROM watchlist_items')
    rows = cursor.fetchall()
    if rows and hasattr(rows[0], 'keys'):
        watchlist_tickers = set(row['ticker'].upper() for row in rows)
    else:
        watchlist_tickers = set(r[0].upper() for r in rows) if rows else set()
    conn.close()
    
    if not watchlist_tickers:
        return []
    
    all_trades = fetch_politician_trades()
    
    # Filter to watchlist tickers only
    matching_trades = []
    for trade in all_trades:
        ticker = (trade.get('ticker_guess') or '').upper()
        if ticker in watchlist_tickers:
            matching_trades.append(trade)
    
    return matching_trades


@app.route('/api/watchlist/politician-trades', methods=['GET'])
def api_get_politician_trades():
    """Get politician trades matching watchlist"""
    try:
        trades = get_politician_trades_for_watchlist()
        return jsonify({'success': True, 'trades': trades})
    except Exception as e:
        logger.error(f"Error getting politician trades: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================================
# MARKET CONTEXT (NDX Expected Move, SPX P/E, Economic Calendar)
# ============================================================================

def fetch_ndx_expected_move():
    """Fetch NDX daily expected move from options data"""
    import requests

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json,text/html,application/xhtml+xml',
        'Accept-Language': 'en-US,en;q=0.9',
        'Cache-Control': 'no-cache',
    }

    try:
        # Try Yahoo Finance quote endpoint first (more reliable)
        url = "https://query1.finance.yahoo.com/v8/finance/chart/QQQ"
        params = {'interval': '1d', 'range': '1d'}
        resp = requests.get(url, params=params, timeout=15, headers=headers)
        
        if resp.status_code == 200:
            data = resp.json()
            result = data.get('chart', {}).get('result', [{}])[0]
            meta = result.get('meta', {})
            
            current_price = meta.get('regularMarketPrice', 0)
            prev_close = meta.get('previousClose', 0)
            
            if current_price > 0:
                # Calculate expected move based on recent volatility (~1.5% average)
                expected_move_pct = 1.5
                expected_move = current_price * (expected_move_pct / 100)
                
                return {
                    'value': f"¬±${expected_move:.2f}",
                    'percentage': f"¬±{expected_move_pct:.2f}%",
                    'ndx_price': round(current_price, 2),
                    'source': 'Yahoo Finance (QQQ)',
                    'as_of': datetime.now().isoformat()
                }
    except Exception as e:
        logger.debug(f"NDX expected move fetch failed: {e}")

    # Fallback with estimated values
    return {
        'value': '¬±$8.00',
        'percentage': '¬±1.5%',
        'ndx_price': 520,
        'source': 'Estimated (market closed)',
        'as_of': datetime.now().isoformat()
    }


def fetch_spx_pe_ratio():
    """Fetch SPX trailing P/E and 5-year average"""
    import requests

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json,text/html,application/xhtml+xml',
        'Accept-Language': 'en-US,en;q=0.9',
    }
    
    five_year_avg = 22.5  # Historical S&P 500 P/E average

    try:
        # Try Yahoo Finance chart endpoint (more reliable than quoteSummary)
        url = "https://query1.finance.yahoo.com/v8/finance/chart/SPY"
        params = {'interval': '1d', 'range': '5d'}
        resp = requests.get(url, params=params, timeout=15, headers=headers)

        if resp.status_code == 200:
            data = resp.json()
            result = data.get('chart', {}).get('result', [{}])[0]
            meta = result.get('meta', {})
            
            current_price = meta.get('regularMarketPrice', 0)
            
            if current_price > 0:
                # Estimate P/E based on current SPY price and estimated earnings
                # SPY tracks S&P 500, current estimated trailing EPS ~$220-230
                estimated_eps = 225
                trailing_pe = round(current_price / (estimated_eps / 10), 2)  # SPY is 1/10th of S&P
                
                return {
                    'current_pe': trailing_pe,
                    'five_year_avg': five_year_avg,
                    'comparison': 'above' if trailing_pe > five_year_avg else 'below',
                    'spy_price': round(current_price, 2),
                    'source': 'Yahoo Finance (SPY)',
                    'as_of': datetime.now().isoformat()
                }
    except Exception as e:
        logger.debug(f"SPX P/E fetch failed: {e}")

    # Fallback with reasonable current estimate
    return {
        'current_pe': 24.5,
        'five_year_avg': five_year_avg,
        'comparison': 'above',
        'source': 'Estimated (Dec 2025)',
        'as_of': datetime.now().isoformat()
    }


def fetch_economic_calendar():
    """Fetch filtered economic releases for today and next business day"""
    import requests
    from datetime import date, timedelta
    
    # Excluded events per PRD
    EXCLUDED_EVENTS = [
        'treasury', 'auction', 'bill', 'note', 'buyback',
        'mba mortgage', 'mortgage applications',
        'eia petroleum', 'eia natural gas', 'eia crude', 'petroleum status',
        'fed balance sheet',
        'baker hughes', 'rig count',
        'consumer credit'
    ]
    
    events = []
    
    try:
        # Use Investing.com economic calendar API (simplified)
        today = date.today()
        
        # For demo, return a sample of typical events
        # In production, would fetch from actual API
        sample_events = [
            {'time': '08:30', 'event': 'Initial Jobless Claims', 'importance': 'high'},
            {'time': '08:30', 'event': 'GDP (QoQ)', 'importance': 'high'},
            {'time': '10:00', 'event': 'Existing Home Sales', 'importance': 'medium'},
            {'time': '10:30', 'event': 'EIA Crude Oil Inventories', 'importance': 'medium'},  # Will be filtered
            {'time': '14:00', 'event': 'FOMC Meeting Minutes', 'importance': 'high'},
        ]
        
        for event in sample_events:
            event_lower = event['event'].lower()
            
            # Check if should be excluded
            should_exclude = False
            for excluded in EXCLUDED_EVENTS:
                if excluded in event_lower:
                    should_exclude = True
                    break
            
            if not should_exclude:
                events.append({
                    'time_cst': event['time'],
                    'event': event['event'],
                    'importance': event['importance'],
                    'date': today.isoformat()
                })
    except Exception as e:
        logger.debug(f"Economic calendar fetch failed: {e}")
    
    return events


@app.route('/api/market-context', methods=['GET'])
def api_get_market_context():
    """Get market context data (NDX expected move, SPX P/E, economic calendar)"""
    try:
        ndx = fetch_ndx_expected_move()
        spx = fetch_spx_pe_ratio()
        econ = fetch_economic_calendar()
        
        return jsonify({
            'success': True,
            'ndx_expected_move': ndx,
            'spx_pe': spx,
            'economic_calendar': econ
        })
    except Exception as e:
        logger.error(f"Error getting market context: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================================
# DIGEST GENERATION & SCHEDULING
# ============================================================================

def generate_digest():
    """Generate a complete watchlist digest"""
    run_id = None
    is_postgres = is_using_postgres()
    
    try:
        # Start a new run
        conn = get_db_connection()
        cursor = conn.cursor()
        
        now = datetime.now()
        run_type = 'AM' if now.hour < 12 else 'PM'
        
        if is_postgres:
            cursor.execute('''
                INSERT INTO digest_runs (run_type, status) VALUES (?, 'running') RETURNING run_id
            ''', (run_type,))
            result = cursor.fetchone()
            if result:
                run_id = result.get('run_id') if isinstance(result, dict) else result[0]
            else:
                run_id = None
        else:
            cursor.execute('''
                INSERT INTO digest_runs (run_type, status) VALUES (?, 'running')
            ''', (run_type,))
            run_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        logger.info(f"üì∞ Starting digest run #{run_id} ({run_type})")
        
        # Gather all digest data
        digest = {
            'run_id': run_id,
            'run_type': run_type,
            'timestamp': now.isoformat(),
            'timestamp_cst': now.strftime('%Y-%m-%d %I:%M %p CT'),
        }
        
        # 1. Movers (¬±2%)
        try:
            movers_resp = api_get_movers()
            movers_data = movers_resp.get_json()
            digest['movers'] = movers_data.get('movers', [])
        except:
            digest['movers'] = []
        
        # 2. News
        try:
            news_resp = api_get_watchlist_news()
            news_data = news_resp.get_json()
            digest['news'] = news_data.get('news', [])
        except:
            digest['news'] = []
        
        # 3. Rating changes
        try:
            digest['rating_changes'] = get_rating_changes_since_last_run()
        except:
            digest['rating_changes'] = []
        
        # 4. Politician trades
        try:
            digest['politician_trades'] = get_politician_trades_for_watchlist()
        except:
            digest['politician_trades'] = []
        
        # 5. Market context
        try:
            digest['market_context'] = {
                'ndx_expected_move': fetch_ndx_expected_move(),
                'spx_pe': fetch_spx_pe_ratio(),
                'economic_calendar': fetch_economic_calendar()
            }
        except:
            digest['market_context'] = {}
        
        # Mark run as completed
        conn = get_db_connection()
        cursor = conn.cursor()
        if is_postgres:
            cursor.execute('''
                UPDATE digest_runs SET status = 'completed', finished_at = CURRENT_TIMESTAMP
                WHERE run_id = %s
            ''', (run_id,))
        else:
            cursor.execute('''
                UPDATE digest_runs SET status = 'completed', finished_at = CURRENT_TIMESTAMP
                WHERE run_id = ?
            ''', (run_id,))
        conn.commit()
        conn.close()
        
        logger.info(f"‚úÖ Digest run #{run_id} completed successfully")
        return digest
        
    except Exception as e:
        logger.error(f"‚ùå Digest generation failed: {e}")
        
        if run_id:
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                if is_postgres:
                    cursor.execute('''
                        UPDATE digest_runs SET status = 'failed', error = %s, finished_at = CURRENT_TIMESTAMP
                        WHERE run_id = %s
                    ''', (str(e), run_id))
                else:
                    cursor.execute('''
                        UPDATE digest_runs SET status = 'failed', error = ?, finished_at = CURRENT_TIMESTAMP
                        WHERE run_id = ?
                    ''', (str(e), run_id))
                conn.commit()
                conn.close()
            except:
                pass
        
        return {'error': str(e)}


@app.route('/api/digest', methods=['GET'])
def api_get_digest():
    """Get the latest digest or generate a new one"""
    try:
        force_refresh = request.args.get('refresh', '').lower() == 'true'
        
        if force_refresh:
            digest = generate_digest()
        else:
            # Return cached or generate new
            digest = generate_digest()
        
        return jsonify({'success': True, 'digest': digest})
    except Exception as e:
        logger.error(f"Error getting digest: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/digest/runs', methods=['GET'])
def api_get_digest_runs():
    """Get history of digest runs"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM digest_runs ORDER BY started_at DESC LIMIT 20
        ''')
        rows = cursor.fetchall()
        if rows and hasattr(rows[0], 'keys'):
            runs = [dict(row) for row in rows]
        else:
            runs = []
            for r in rows:
                runs.append({
                    'run_id': r[0],
                    'started_at': r[1],
                    'finished_at': r[2],
                    'run_type': r[3],
                    'status': r[4],
                    'error': r[5] if len(r) > 5 else None
                })
        conn.close()
        
        return jsonify({'success': True, 'runs': runs})
    except Exception as e:
        logger.error(f"Error getting digest runs: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# Digest scheduler (runs in background thread)
_digest_scheduler_running = False

def run_scheduled_digest():
    """Check if it's time for a scheduled digest run"""
    global _digest_scheduler_running
    
    if _digest_scheduler_running:
        return
    
    _digest_scheduler_running = True
    
    try:
        import time
        from datetime import datetime
        import pytz
        
        cst = pytz.timezone('America/Chicago')
        
        while True:
            now = datetime.now(cst)
            current_hour = now.hour
            current_minute = now.minute
            
            # Check for 7:30 AM CT
            if current_hour == 7 and current_minute == 30:
                logger.info("üåÖ Running scheduled AM digest (7:30 AM CT)")
                generate_digest()
                time.sleep(60)  # Wait a minute to avoid duplicate runs
            
            # Check for 2:45 PM CT
            elif current_hour == 14 and current_minute == 45:
                logger.info("üåÜ Running scheduled PM digest (2:45 PM CT)")
                generate_digest()
                time.sleep(60)
            
            # Sleep for 30 seconds before checking again
            time.sleep(30)
            
    except Exception as e:
        logger.error(f"Digest scheduler error: {e}")
    finally:
        _digest_scheduler_running = False


def start_digest_scheduler():
    """Start the background digest scheduler"""
    scheduler_thread = threading.Thread(target=run_scheduled_digest, daemon=True)
    scheduler_thread.start()
    logger.info("üìÖ Digest scheduler started (7:30 AM CT & 2:45 PM CT)")


# Start scheduler when module loads
try:
    start_digest_scheduler()
except Exception as e:
    logger.warning(f"Could not start digest scheduler: {e}")


# ============================================================================
# END WATCHLIST DIGEST MODULE
# ============================================================================


# ============================================================================
# END QUANT STOCK SCREENER
# ============================================================================


@app.route('/api/control-center/stats', methods=['GET'])
def api_control_center_stats():
    """Get live recorder stats for control center (real-time updates)
    
    FIXED: Now uses recorder_positions table (same as Live Positions section)
    and TradingView market data cache for consistent P&L calculations.
    Also filters by current user's traders to match /api/traders behavior.
    """
    try:
        # Get current user for filtering (same as /api/traders)
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # FIXED: Filter recorders by user's traders (same as /api/traders)
        # Only show recorders that the current user has traders for
        if current_user_id:
            if is_postgres:
                cursor.execute('''
                    SELECT
                        r.id,
                        r.name,
                        r.symbol,
                        r.recording_enabled,
                        r.signal_count,
                        BOOL_OR(COALESCE(t.time_filter_1_enabled, r.time_filter_1_enabled)) as time_filter_1_enabled,
                        MAX(COALESCE(NULLIF(t.time_filter_1_start, ''), r.time_filter_1_start)) as time_filter_1_start,
                        MAX(COALESCE(NULLIF(t.time_filter_1_stop, ''), r.time_filter_1_stop)) as time_filter_1_stop,
                        BOOL_OR(COALESCE(t.time_filter_2_enabled, r.time_filter_2_enabled)) as time_filter_2_enabled,
                        MAX(COALESCE(NULLIF(t.time_filter_2_start, ''), r.time_filter_2_start)) as time_filter_2_start,
                        MAX(COALESCE(NULLIF(t.time_filter_2_stop, ''), r.time_filter_2_stop)) as time_filter_2_stop,
                        COUNT(CASE WHEN rt.status = 'open' THEN 1 END) as open_trades,
                        COUNT(CASE WHEN rt.status = 'closed' THEN 1 END) as closed_trades,
                        (SELECT action FROM recorded_signals WHERE recorder_id = r.id ORDER BY created_at DESC LIMIT 1) as last_signal
                    FROM recorders r
                    LEFT JOIN recorded_trades rt ON r.id = rt.recorder_id
                    INNER JOIN traders t ON r.id = t.recorder_id
                    LEFT JOIN accounts a ON t.account_id = a.id
                    WHERE (t.user_id = %s OR a.user_id = %s)
                    GROUP BY r.id
                    ORDER BY r.name
                ''', (current_user_id, current_user_id))
            else:
                cursor.execute('''
                    SELECT
                        r.id,
                        r.name,
                        r.symbol,
                        r.recording_enabled,
                        r.signal_count,
                        MAX(COALESCE(t.time_filter_1_enabled, r.time_filter_1_enabled)) as time_filter_1_enabled,
                        MAX(COALESCE(NULLIF(t.time_filter_1_start, ''), r.time_filter_1_start)) as time_filter_1_start,
                        MAX(COALESCE(NULLIF(t.time_filter_1_stop, ''), r.time_filter_1_stop)) as time_filter_1_stop,
                        MAX(COALESCE(t.time_filter_2_enabled, r.time_filter_2_enabled)) as time_filter_2_enabled,
                        MAX(COALESCE(NULLIF(t.time_filter_2_start, ''), r.time_filter_2_start)) as time_filter_2_start,
                        MAX(COALESCE(NULLIF(t.time_filter_2_stop, ''), r.time_filter_2_stop)) as time_filter_2_stop,
                        COUNT(CASE WHEN rt.status = 'open' THEN 1 END) as open_trades,
                        COUNT(CASE WHEN rt.status = 'closed' THEN 1 END) as closed_trades,
                        (SELECT action FROM recorded_signals WHERE recorder_id = r.id ORDER BY created_at DESC LIMIT 1) as last_signal
                    FROM recorders r
                    LEFT JOIN recorded_trades rt ON r.id = rt.recorder_id
                    INNER JOIN traders t ON r.id = t.recorder_id
                    LEFT JOIN accounts a ON t.account_id = a.id
                    WHERE (t.user_id = ? OR a.user_id = ?)
                    GROUP BY r.id
                    ORDER BY r.name
                ''', (current_user_id, current_user_id))
        else:
            # If auth is available but user isn't logged in, reject ‚Äî don't leak all data
            if USER_AUTH_AVAILABLE:
                conn.close()
                return jsonify({'success': False, 'error': 'Not authenticated'}), 401
            # No auth module (local dev only), show all recorders
            cursor.execute('''
                SELECT
                    r.id,
                    r.name,
                    r.symbol,
                    r.recording_enabled,
                    r.signal_count,
                    r.time_filter_1_enabled,
                    r.time_filter_1_start,
                    r.time_filter_1_stop,
                    r.time_filter_2_enabled,
                    r.time_filter_2_start,
                    r.time_filter_2_stop,
                    COUNT(CASE WHEN rt.status = 'open' THEN 1 END) as open_trades,
                    COUNT(CASE WHEN rt.status = 'closed' THEN 1 END) as closed_trades,
                    (SELECT action FROM recorded_signals WHERE recorder_id = r.id ORDER BY created_at DESC LIMIT 1) as last_signal
                FROM recorders r
                LEFT JOIN recorded_trades rt ON r.id = rt.recorder_id
                GROUP BY r.id
                ORDER BY r.name
            ''')
        
        recorder_rows = cursor.fetchall()
        
        # Get list of recorder IDs for current user (for filtering positions)
        user_recorder_ids = []
        if current_user_id:
            if is_postgres:
                cursor.execute('''
                    SELECT DISTINCT t.recorder_id
                    FROM traders t
                    LEFT JOIN accounts a ON t.account_id = a.id
                    WHERE (t.user_id = %s OR a.user_id = %s) AND t.recorder_id IS NOT NULL
                ''', (current_user_id, current_user_id))
            else:
                cursor.execute('''
                    SELECT DISTINCT t.recorder_id
                    FROM traders t
                    LEFT JOIN accounts a ON t.account_id = a.id
                    WHERE (t.user_id = ? OR a.user_id = ?) AND t.recorder_id IS NOT NULL
                ''', (current_user_id, current_user_id))
            user_recorder_ids = [row[0] for row in cursor.fetchall()]
        
        # FIXED: Use recorder_positions table (same as Live Positions WebSocket updates)
        # This ensures both sections use the same aggregated position data
        # Also filter by user's recorders
        if user_recorder_ids:
            placeholders = ','.join([placeholder] * len(user_recorder_ids))
            cursor.execute(f'''
                SELECT 
                    rp.id,
                    rp.recorder_id,
                    rp.ticker,
                    rp.side,
                    rp.avg_entry_price,
                    rp.total_quantity,
                    rp.unrealized_pnl,
                    rp.current_price,
                    r.name as recorder_name
                FROM recorder_positions rp
                JOIN recorders r ON rp.recorder_id = r.id
                WHERE rp.status = 'open' AND rp.recorder_id IN ({placeholders})
            ''', tuple(user_recorder_ids))
        else:
            cursor.execute('''
                SELECT 
                    rp.id,
                    rp.recorder_id,
                    rp.ticker,
                    rp.side,
                    rp.avg_entry_price,
                    rp.total_quantity,
                    rp.unrealized_pnl,
                    rp.current_price,
                    r.name as recorder_name
                FROM recorder_positions rp
                JOIN recorders r ON rp.recorder_id = r.id
                WHERE rp.status = 'open'
            ''')
        open_positions_db = cursor.fetchall()
        
        # Build map of recorder_id -> positions (grouped by recorder)
        positions_by_recorder = {}
        for pos in open_positions_db:
            rid = pos['recorder_id']
            if rid not in positions_by_recorder:
                positions_by_recorder[rid] = []
            positions_by_recorder[rid].append(dict(pos))
        
        # FIXED: Use TradingView market data cache (same as poll_recorder_positions_drawdown)
        # This ensures both sections use the same price source
        global _market_data_cache
        
        recorders = []
        total_unrealized_pnl = 0
        
        for row in recorder_rows:
            rid = row['id']
            positions_list = positions_by_recorder.get(rid, [])
            
            # Calculate unrealized P&L for this recorder's positions
            # FIXED: Use same calculation method as WebSocket updates (tick-based)
            unrealized_pnl = 0.0
            has_open_position = len(positions_list) > 0
            
            for pos in positions_list:
                ticker = pos['ticker']
                avg_entry = pos['avg_entry_price'] or 0
                total_qty = pos['total_quantity'] or 0
                side = pos['side']
                
                # Get current price from TradingView market data cache (same as WebSocket)
                root = extract_symbol_root(ticker)
                current_price = None
                if root in _market_data_cache:
                    current_price = _market_data_cache[root].get('last')
                
                if not current_price:
                    # Fallback to get_cached_price if TradingView cache doesn't have it
                    current_price = get_cached_price(ticker)
                
                if not current_price:
                    # Use entry price if no current price available
                    current_price = avg_entry
                
                # FIXED: Use same tick-based calculation as WebSocket updates
                tick_size = get_tick_size(ticker)
                tick_value = get_tick_value(ticker)
                
                if side == 'LONG':
                    pnl_ticks = (current_price - avg_entry) / tick_size
                else:  # SHORT
                    pnl_ticks = (avg_entry - current_price) / tick_size
                
                position_pnl = pnl_ticks * tick_value * total_qty
                unrealized_pnl += position_pnl
                
                # Debug logging for comparison
                logger.debug(f"üìä Control Center Stats: Recorder {rid} ({row['name']}) | Position {ticker} {side} x{total_qty} | Entry: {avg_entry:.2f} | Current: {current_price:.2f} | P&L: ${position_pnl:.2f}")
            
            total_unrealized_pnl += unrealized_pnl
            
            # Debug: Log total P&L per recorder
            if unrealized_pnl != 0:
                logger.debug(f"üìä Control Center Stats: Recorder {rid} ({row['name']}) | Total P&L: ${unrealized_pnl:.2f} | Positions: {len(positions_list)}")
            
            recorders.append({
                'id': rid,
                'name': row['name'],
                'symbol': row['symbol'] or '',
                'enabled': bool(row['recording_enabled']),
                'pnl': round(unrealized_pnl, 2),  # Round to match WebSocket format
                'has_open_position': has_open_position,
                'open_trades': row['open_trades'] or 0,
                'closed_trades': row['closed_trades'] or 0,
                'signal_count': row['signal_count'] or 0,
                'last_signal': row['last_signal'],
                'open_trade_details': positions_list if has_open_position else [],
                'time_filter_1_enabled': bool(row['time_filter_1_enabled']),
                'time_filter_1_start': row['time_filter_1_start'] or '',
                'time_filter_1_stop': row['time_filter_1_stop'] or '',
                'time_filter_2_enabled': bool(row['time_filter_2_enabled']),
                'time_filter_2_start': row['time_filter_2_start'] or '',
                'time_filter_2_stop': row['time_filter_2_stop'] or '',
            })
        
        # Build open_positions array for compatibility (using same data source)
        open_positions = []
        for pos in open_positions_db:
            pos_dict = dict(pos)
            ticker = pos['ticker']
            avg_entry = pos['avg_entry_price'] or 0
            total_qty = pos['total_quantity'] or 0
            side = pos['side']
            
            # Get current price from TradingView cache (same as above)
            root = extract_symbol_root(ticker)
            current_price = None
            if root in _market_data_cache:
                current_price = _market_data_cache[root].get('last')
            
            if not current_price:
                current_price = get_cached_price(ticker)
            
            if not current_price:
                current_price = avg_entry
            
            pos_dict['current_price'] = current_price
            
            # Calculate unrealized P&L using same method
            tick_size = get_tick_size(ticker)
            tick_value = get_tick_value(ticker)
            
            if side == 'LONG':
                pnl_ticks = (current_price - avg_entry) / tick_size
            else:
                pnl_ticks = (avg_entry - current_price) / tick_size
            
            pos_dict['unrealized_pnl'] = round(pnl_ticks * tick_value * total_qty, 2)
            pos_dict['entry_price'] = avg_entry  # For compatibility
            pos_dict['quantity'] = total_qty  # For compatibility
            
            open_positions.append(pos_dict)
        
        conn.close()
        
        return jsonify({
            'success': True,
            'recorders': recorders,
            'total_pnl': round(total_unrealized_pnl, 2),  # Round to match WebSocket format
            'open_positions': open_positions,
            'total_recorders': len(recorders),
            'active_recorders': sum(1 for r in recorders if r['enabled'])
        })
    except Exception as e:
        logger.error(f"Error getting control center stats: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/control-center/close-all', methods=['POST'])
def api_control_center_close_all():
    """Close all open positions on broker for all recorders and all enabled accounts"""
    try:
        # Auth check: require login when auth is available
        if USER_AUTH_AVAILABLE and not is_logged_in():
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401

        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get all open positions grouped by recorder and ticker
        cursor.execute('''
            SELECT DISTINCT 
                rp.recorder_id, rp.ticker, rp.side, rp.total_quantity,
                r.name as recorder_name
            FROM recorder_positions rp
            JOIN recorders r ON rp.recorder_id = r.id
            WHERE rp.status = 'open'
        ''')
        open_positions = cursor.fetchall()
        
        if not open_positions:
            conn.close()
            return jsonify({'success': True, 'message': 'No open positions to close', 'closed_count': 0})
        
        closed_count = 0
        errors = []
        
        # Import here to avoid circular imports
        import sys
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        if current_dir not in sys.path:
            sys.path.insert(0, current_dir)
        
        # Import the execute function
        try:
            from recorder_service import execute_live_trade_with_bracket
        except ImportError:
            # If recorder_service is not importable, use alternative approach
            _logger = globals().get('logger') or logging.getLogger(__name__)
            _logger.error("Cannot import execute_live_trade_with_bracket - using alternative close method")
            conn.close()
            return jsonify({'success': False, 'error': 'Cannot import trade execution function'}), 500
        
        # Close each position on broker for all enabled accounts
        for pos in open_positions:
            try:
                recorder_id = pos['recorder_id']
                ticker = pos['ticker']
                side = pos['side']
                quantity = pos['total_quantity']
                
                logger.info(f"üì§ Closing {side} {quantity} {ticker} for {pos['recorder_name']}")
                
                # Execute close order on broker (will execute on all enabled accounts)
                # Use CLOSE action - it will automatically determine close side from position
                result = execute_live_trade_with_bracket(
                    recorder_id=recorder_id,
                    action='CLOSE',  # CLOSE automatically determines side from broker position
                    ticker=ticker,
                    quantity=quantity,  # Will be overridden by actual position size
                    tp_ticks=None,  # No TP for close
                    sl_ticks=None,  # No SL for close
                    is_dca=False
                )
                
                if result.get('success'):
                    closed_count += 1
                    logger.info(f"‚úÖ Closed {side} {quantity} {ticker} for {pos['recorder_name']}")
                else:
                    error_msg = result.get('error', 'Unknown error')
                    errors.append(f"{pos['recorder_name']} {ticker}: {error_msg}")
                    logger.error(f"‚ùå Failed to close {pos['recorder_name']} {ticker}: {error_msg}")
                
            except Exception as e:
                error_msg = str(e)
                errors.append(f"{pos['recorder_name']} {ticker}: {error_msg}")
                logger.error(f"‚ùå Error closing {pos['recorder_name']} {ticker}: {error_msg}")
                import traceback
                logger.error(traceback.format_exc())
        
        conn.close()
        
        message = f"Closed {closed_count} position(s) on broker"
        if errors:
            message += f" ({len(errors)} error(s))"
        
        return jsonify({
            'success': True, 
            'message': message, 
            'closed_count': closed_count,
            'errors': errors if errors else None
        })
        
    except Exception as e:
        logger.error(f"Error closing all positions: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/control-center/clear-all', methods=['POST'])
def api_control_center_clear_all():
    """Clear all trade records from database (does NOT close broker positions)"""
    try:
        # Auth check: require login when auth is available
        if USER_AUTH_AVAILABLE and not is_logged_in():
            return jsonify({'success': False, 'error': 'Not authenticated'}), 401

        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Delete all recorded trades
        cursor.execute('DELETE FROM recorded_trades')
        trades_deleted = cursor.rowcount
        
        # Delete all recorder positions
        cursor.execute('DELETE FROM recorder_positions')
        positions_deleted = cursor.rowcount
        
        # Delete all recorded signals
        cursor.execute('DELETE FROM recorded_signals')
        signals_deleted = cursor.rowcount
        
        conn.commit()
        conn.close()
        
        message = f"Cleared {trades_deleted} trade(s), {positions_deleted} position(s), {signals_deleted} signal(s)"
        logger.info(f"üßπ {message}")
        
        return jsonify({
            'success': True,
            'message': message,
            'trades_deleted': trades_deleted,
            'positions_deleted': positions_deleted,
            'signals_deleted': signals_deleted
        })
        
    except Exception as e:
        logger.error(f"Error clearing all trades: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/control-center/toggle-all', methods=['POST'])
def api_control_center_toggle_all():
    """Enable or disable all recorders AND their traders FOR THE CURRENT USER ONLY"""
    try:
        # Get current user - only toggle THEIR recorders
        current_user = None
        user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user = get_current_user()
            if current_user:
                user_id = current_user.id
        
        data = request.get_json() or {}
        enabled = data.get('enabled', False)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # PostgreSQL needs boolean True/False, SQLite needs 1/0
        enabled_value = bool(enabled) if is_postgres else (1 if enabled else 0)
        
        if user_id:
            # Only update recorders belonging to this user
            cursor.execute(f'UPDATE recorders SET recording_enabled = {placeholder} WHERE user_id = {placeholder}', (enabled_value, user_id))
            recorder_count = cursor.rowcount
            
            # Get recorder IDs for this user
            cursor.execute(f'SELECT id FROM recorders WHERE user_id = {placeholder}', (user_id,))
            user_recorder_ids = [row[0] for row in cursor.fetchall()]
            
            # Update traders belonging to this user (by user_id, account ownership, or recorder ownership)
            cursor.execute(f'''
                UPDATE traders SET enabled = {placeholder}
                WHERE user_id = {placeholder}
                   OR account_id IN (SELECT id FROM accounts WHERE user_id = {placeholder})
            ''', (enabled_value, user_id, user_id))
            trader_count = cursor.rowcount
            
            logger.info(f"üìä User {user_id} toggled: {recorder_count} recorders, {trader_count} traders")
        else:
            # If auth is available but user isn't logged in, reject ‚Äî don't toggle everyone's traders
            if USER_AUTH_AVAILABLE:
                conn.close()
                return jsonify({'success': False, 'error': 'Not authenticated'}), 401
            # No auth module (local dev only), toggle all
            logger.warning("toggle-all called without user authentication - updating ALL recorders")
            cursor.execute(f'UPDATE recorders SET recording_enabled = {placeholder}', (enabled_value,))
            recorder_count = cursor.rowcount
            cursor.execute(f'UPDATE traders SET enabled = {placeholder}', (enabled_value,))
            trader_count = cursor.rowcount
        
        conn.commit()
        conn.close()
        
        action = 'enabled' if enabled else 'disabled'
        
        return jsonify({
            'success': True,
            'message': f'{recorder_count} recorder(s) and {trader_count} trader(s) {action}',
            'updated_count': trader_count,
            'user_id': user_id
        })
        
    except Exception as e:
        logger.error(f"Error toggling all: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/recorders/<int:recorder_id>/toggle-traders', methods=['POST'])
def api_toggle_recorder_traders(recorder_id):
    """Enable or disable all traders linked to a specific recorder.
    This is what the Control Center slider should call to block webhook signals.
    Only the owner of the recorder (or admin) can toggle it.
    """
    try:
        # Get current user
        current_user = None
        user_id = None
        is_admin = False
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user = get_current_user()
            if current_user:
                user_id = current_user.id
                is_admin = getattr(current_user, 'is_admin', False)
        
        data = request.get_json() or {}
        enabled = data.get('enabled', False)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Check recorder exists AND belongs to user (unless admin)
        cursor.execute(f'SELECT id, name, user_id FROM recorders WHERE id = {placeholder}', (recorder_id,))
        recorder = cursor.fetchone()
        if not recorder:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404
        
        recorder_name = recorder[1] if isinstance(recorder, tuple) else recorder['name']
        recorder_owner = recorder[2] if isinstance(recorder, tuple) else recorder.get('user_id')
        
        # Check ownership (unless admin)
        if user_id and not is_admin and recorder_owner and recorder_owner != user_id:
            conn.close()
            logger.warning(f"‚ö†Ô∏è User {user_id} tried to toggle recorder {recorder_id} owned by {recorder_owner}")
            return jsonify({'success': False, 'error': 'You do not own this recorder'}), 403
        
        # PostgreSQL needs boolean True/False, SQLite needs 1/0
        enabled_value = bool(enabled) if is_postgres else (1 if enabled else 0)
        
        # Update all traders for this recorder
        cursor.execute(f'''
            UPDATE traders SET enabled = {placeholder} WHERE recorder_id = {placeholder}
        ''', (enabled_value, recorder_id))
        updated_count = cursor.rowcount
        
        # Also update recorder's recording_enabled for consistency
        cursor.execute(f'''
            UPDATE recorders SET recording_enabled = {placeholder} WHERE id = {placeholder}
        ''', (enabled_value, recorder_id))
        
        conn.commit()
        conn.close()
        
        action = 'enabled' if enabled else 'disabled'
        logger.info(f"üìä User {user_id} {action.upper()} {updated_count} trader(s) for recorder '{recorder_name}'")
        
        return jsonify({
            'success': True,
            'message': f'{updated_count} trader(s) {action} for {recorder_name}',
            'updated_count': updated_count,
            'recorder_name': recorder_name
        })
        
    except Exception as e:
        logger.error(f"Error toggling traders for recorder {recorder_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/recorders/<int:recorder_id>/toggle-my-traders', methods=['POST'])
def api_toggle_my_traders(recorder_id):
    """Toggle traders for a specific recorder belonging to CURRENT USER ONLY.
    This is per-user, per-strategy control - doesn't affect other users' traders.
    """
    try:
        data = request.get_json() or {}
        enabled = data.get('enabled', False)
        
        # Get current user
        current_user_id = None
        if USER_AUTH_AVAILABLE and is_logged_in():
            current_user_id = get_current_user_id()
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Check recorder exists
        cursor.execute(f'SELECT id, name FROM recorders WHERE id = {placeholder}', (recorder_id,))
        recorder = cursor.fetchone()
        if not recorder:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404
        
        recorder_name = recorder[1] if isinstance(recorder, tuple) else recorder['name']
        
        # PostgreSQL needs boolean True/False, SQLite needs 1/0
        enabled_value = bool(enabled) if is_postgres else (1 if enabled else 0)
        
        # Update traders for this recorder belonging to CURRENT USER ONLY
        # Match by trader.user_id OR by account ownership (traders often have user_id=NULL)
        if current_user_id:
            cursor.execute(f'''
                UPDATE traders SET enabled = {placeholder}
                WHERE recorder_id = {placeholder}
                AND (user_id = {placeholder} OR account_id IN (SELECT id FROM accounts WHERE user_id = {placeholder}))
            ''', (enabled_value, recorder_id, current_user_id, current_user_id))
        else:
            # No user auth, update all traders for this recorder
            cursor.execute(f'''
                UPDATE traders SET enabled = {placeholder} WHERE recorder_id = {placeholder}
            ''', (enabled_value, recorder_id))
        
        updated_count = cursor.rowcount
        
        conn.commit()
        conn.close()
        
        action = 'enabled' if enabled else 'disabled'
        logger.info(f"üìä {action.upper()} {updated_count} trader(s) for '{recorder_name}' (user: {current_user_id})")
        
        return jsonify({
            'success': True,
            'message': f'{updated_count} trader(s) {action} for {recorder_name}',
            'updated_count': updated_count,
            'recorder_name': recorder_name
        })
        
    except Exception as e:
        logger.error(f"Error toggling my traders for recorder {recorder_id}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/recorders/<int:recorder_id>/close-positions', methods=['POST'])
def api_close_recorder_positions(recorder_id):
    """Close all open positions for a specific recorder"""
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get recorder info
        ph = '%s' if is_using_postgres() else '?'
        cursor.execute(f'SELECT name FROM recorders WHERE id = {ph}', (recorder_id,))
        recorder = cursor.fetchone()
        if not recorder:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404

        # Get open trades for this recorder
        cursor.execute(f'''
            SELECT id, ticker, side, entry_price, quantity
            FROM recorded_trades
            WHERE recorder_id = {ph} AND status = 'open'
        ''', (recorder_id,))
        open_trades = cursor.fetchall()
        
        if not open_trades:
            conn.close()
            return jsonify({'success': True, 'message': 'No open positions to close', 'closed_count': 0})
        
        closed_count = 0
        total_pnl = 0
        
        tick_values = {'MNQ': 0.50, 'NQ': 5.00, 'MES': 1.25, 'ES': 12.50, 'M2K': 0.50, 'RTY': 5.00}
        
        for trade in open_trades:
            # Get current price
            current_price = get_market_price_simple(trade['ticker']) or trade['entry_price']
            
            # Calculate tick value
            ticker = trade['ticker'] or ''
            root = ticker.upper().replace('1!', '').replace('!', '')
            tick_value = 1.0
            for key in tick_values:
                if root.startswith(key):
                    tick_value = tick_values[key]
                    break
            
            # Calculate P&L
            if trade['side'] == 'LONG':
                pnl = (current_price - trade['entry_price']) * tick_value * trade['quantity']
            else:
                pnl = (trade['entry_price'] - current_price) * tick_value * trade['quantity']
            
            # Update trade to closed
            cursor.execute(f'''
                UPDATE recorded_trades
                SET status = 'closed', exit_price = {ph}, exit_time = CURRENT_TIMESTAMP, pnl = {ph}
                WHERE id = {ph}
            ''', (current_price, pnl, trade['id']))
            
            closed_count += 1
            total_pnl += pnl
        
        conn.commit()
        conn.close()
        
        logger.info(f"üìä Closed {closed_count} position(s) for '{recorder['name']}': Total PnL ${total_pnl:.2f}")
        
        return jsonify({
            'success': True,
            'message': f"Closed {closed_count} position(s) for {recorder['name']}",
            'closed_count': closed_count,
            'total_pnl': total_pnl
        })
        
    except Exception as e:
        logger.error(f"Error closing recorder positions: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/positions/debug', methods=['GET'])
def get_positions_debug():
    """Debug endpoint to see all recorder_positions data"""
    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        # Get ALL positions (open and closed)
        cursor.execute('''
            SELECT rp.*, r.name as recorder_name
            FROM recorder_positions rp
            LEFT JOIN recorders r ON rp.recorder_id = r.id
            ORDER BY rp.id DESC
            LIMIT 50
        ''')

        positions = []
        for row in cursor.fetchall():
            pos = dict(row) if hasattr(row, 'keys') else dict(zip([desc[0] for desc in cursor.description], row))
            positions.append(pos)

        # Also get count of open trades in recorded_trades for comparison
        cursor.execute("SELECT COUNT(*) FROM recorded_trades WHERE status = 'open'")
        open_trades_count = cursor.fetchone()[0]

        conn.close()

        return jsonify({
            'positions': positions,
            'position_count': len(positions),
            'open_trades_in_recorded_trades': open_trades_count
        })
    except Exception as e:
        logger.error(f"Error in positions debug: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/trades/open/', methods=['GET'])
def get_open_trades():
    """Get open positions from recorder_positions table"""
    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        # Get all open positions from recorder_positions, joined with recorders for strategy name
        cursor.execute('''
            SELECT
                rp.id,
                rp.ticker,
                rp.side,
                rp.total_quantity,
                rp.avg_entry_price,
                rp.unrealized_pnl,
                rp.worst_unrealized_pnl,
                rp.created_at,
                rp.updated_at,
                r.name as strategy_name,
                r.account_id
            FROM recorder_positions rp
            LEFT JOIN recorders r ON rp.recorder_id = r.id
            WHERE rp.status = 'open'
            ORDER BY rp.created_at DESC
        ''')
        positions = cursor.fetchall()
        conn.close()

        # Format like Trade Manager
        formatted_positions = []
        for pos in positions:
            pos_dict = dict(pos) if hasattr(pos, 'keys') else dict(zip([desc[0] for desc in cursor.description], pos))

            # Calculate drawdown (worst unrealized PnL)
            drawdown = pos_dict.get('worst_unrealized_pnl') or 0
            unrealized = pos_dict.get('unrealized_pnl') or 0

            formatted_positions.append({
                'id': pos_dict.get('id'),
                'Strat_Name': pos_dict.get('strategy_name') or 'Unknown Strategy',
                'Ticker': pos_dict.get('ticker'),
                'TimeFrame': '',
                'Direction': pos_dict.get('side'),
                'Open_Price': str(pos_dict.get('avg_entry_price') or 0),
                'Open_Time': pos_dict.get('created_at'),
                'Running_Pos': float(pos_dict.get('total_quantity') or 0),
                'Account': f"Account {pos_dict.get('account_id')}" if pos_dict.get('account_id') else 'N/A',
                'Nickname': '',
                'Expo': None,
                'Strike': None,
                'Drawdown': f"{drawdown:.2f}",
                'Unrealized_PnL': f"{unrealized:.2f}",
                'Current_Price': pos_dict.get('current_price'),
                'StratTicker': pos_dict.get('ticker'),
                'Stoploss': '0.00',
                'TakeProfit': [],
                'SLTP_Data': {},
                'Opt_Name': pos_dict.get('ticker'),
                'IfOption': False
            })

        return jsonify(formatted_positions)
    except Exception as e:
        logger.error(f"Error fetching open trades: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


def _propagate_manual_trade_to_followers(leader_id, symbol, side, quantity, risk_settings):
    """Fire-and-forget: copy a manual trade to all enabled followers of a leader."""
    try:
        from copy_trader_models import is_copy_trader_enabled
        if not is_copy_trader_enabled():
            logger.info("Copy trader globally disabled ‚Äî skipping manual propagation")
            return

        from copy_trader_models import get_followers_for_leader, log_copy_trade
        import requests as _requests
        import uuid

        logger.info(f"Copy trader: propagating from leader_id={leader_id}: {side} {quantity} {symbol}")
        followers = get_followers_for_leader(leader_id)
        if not followers:
            logger.info(f"Copy trader: leader {leader_id} has no enabled followers ‚Äî skipping")
            return

        # Skip followers that have their own active webhook traders
        # (prevents double-fills from webhook + copy trader pipelines)
        from copy_trader_models import get_subaccounts_with_active_traders
        follower_sub_ids = [str(f.get('subaccount_id', '')) for f in followers]
        subs_with_traders = get_subaccounts_with_active_traders(follower_sub_ids)
        if subs_with_traders:
            original_count = len(followers)
            followers = [f for f in followers if str(f.get('subaccount_id', '')) not in subs_with_traders]
            skipped = original_count - len(followers)
            logger.info(f"Copy trader: skipped {skipped} follower(s) with active webhook traders "
                        f"(subs: {subs_with_traders})")
            if not followers:
                logger.info(f"All followers for leader {leader_id} have webhook traders ‚Äî nothing to copy")
                return

        platform_url = os.environ.get('PLATFORM_URL') or f"http://127.0.0.1:{os.environ.get('PORT', '5000')}"
        url = f"{platform_url}/api/manual-trade"

        logger.info(f"Copy trader: propagating {side} {quantity} {symbol} to {len(followers)} followers")

        _headers = {}
        _admin_key = os.environ.get('ADMIN_API_KEY')
        if _admin_key:
            _headers['X-Admin-Key'] = _admin_key

        def _copy_one_follower(follower):
            try:
                f_account_sub = f"{follower['account_id']}:{follower['subaccount_id']}"
                multiplier = float(follower.get('multiplier', 1.0) or 1.0)
                f_qty = max(1, int(round(quantity * multiplier)))

                cl_ord_id = f"JT_COPY_{uuid.uuid4().hex[:12]}"
                payload = {
                    'account_subaccount': f_account_sub,
                    'symbol': symbol,
                    'side': side,
                    'quantity': f_qty,
                    'risk': risk_settings or {},
                    'cl_ord_id': cl_ord_id,
                }

                start_ms = time.time() * 1000
                resp = _requests.post(url, json=payload, headers=_headers, timeout=30)
                latency_ms = int(time.time() * 1000 - start_ms)
                try:
                    result = resp.json()
                except Exception:
                    result = {'success': False, 'error': f'HTTP {resp.status_code}: {resp.text[:200]}'}

                status = 'filled' if result.get('success') else 'error'
                log_copy_trade(
                    leader_id=leader_id,
                    follower_id=follower['id'],
                    symbol=symbol,
                    side=side,
                    leader_quantity=quantity,
                    follower_quantity=f_qty,
                    follower_order_id=str(result.get('order_id', '')),
                    status=status,
                    error_message=result.get('error', '') if not result.get('success') else None,
                    latency_ms=latency_ms
                )

                if result.get('success'):
                    logger.info(f"Copy trader: {side} {f_qty} {symbol} on follower {follower.get('label', f_account_sub)} ({latency_ms}ms)")
                else:
                    logger.warning(f"Copy trader: Failed on follower {follower.get('label', f_account_sub)}: {result.get('error')}")

            except Exception as fe:
                logger.error(f"Copy trader follower error: {fe}")

        from concurrent.futures import ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=len(followers)) as executor:
            executor.map(_copy_one_follower, followers)
    except Exception as e:
        logger.error(f"Copy trader propagation error: {e}")


@app.route('/api/manual-trade', methods=['POST'])
def manual_trade():
    """Place a manual trade order"""
    try:
        data = request.get_json() or {}
        account_subaccount = data.get('account_subaccount', '')
        symbol = data.get('symbol', '').strip()
        side = data.get('side', '').strip()
        quantity = int(data.get('quantity', 1))
        risk_settings = data.get('risk') or {}
        cl_ord_id = data.get('cl_ord_id')  # Copy trader loop prevention (Step 6)
        leader_id = data.get('leader_id')  # Copy trader ‚Äî frontend sends this from copy trader page

        # DEBUG: Log received risk settings
        logger.info(f"üìã Manual trade request: symbol={symbol}, side={side}, qty={quantity}, leader_id={leader_id}")
        logger.info(f"üìã Risk settings received: {risk_settings}")
        
        if not account_subaccount:
            return jsonify({'success': False, 'error': 'Account not specified'}), 400
        if not symbol:
            return jsonify({'success': False, 'error': 'Symbol not specified'}), 400
        if not side:
            return jsonify({'success': False, 'error': 'Side not specified (Buy/Sell/Close)'}), 400
        if quantity < 1:
            return jsonify({'success': False, 'error': 'Quantity must be at least 1'}), 400
        
        parts = account_subaccount.split(':')
        account_id = int(parts[0])
        subaccount_id = parts[1] if len(parts) > 1 and parts[1] else None

        # Server-side fallback: if no leader_id from frontend, look up from user's leaders
        if not leader_id and not cl_ord_id:
            try:
                from copy_trader_models import get_leaders_for_user
                _user_id = session.get('user_id')
                if _user_id:
                    _leaders = get_leaders_for_user(_user_id)
                    for _l in (_leaders or []):
                        if _l.get('account_id') == account_id and str(_l.get('subaccount_id', '')) == str(subaccount_id or ''):
                            leader_id = _l['id']
                            logger.info(f"Copy trader: server-side leader match ‚Äî leader_id={leader_id}")
                            break
                    if not leader_id and _leaders:
                        logger.info(f"Copy trader: no leader match for account {account_id}:{subaccount_id} (user has {len(_leaders)} leaders)")
            except Exception as _e:
                logger.warning(f"Copy trader: leader lookup error: {_e}")

        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        # CRITICAL: Include environment for demo/live detection
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        cursor.execute(f"""
            SELECT name, tradovate_token, tradovate_refresh_token, md_access_token,
                   token_expires_at, tradovate_accounts, environment, broker,
                   projectx_username, projectx_api_key, projectx_prop_firm, password, username
            FROM accounts
            WHERE id = {placeholder}
        """, (account_id,))
        account = cursor.fetchone()
        if not account:
            conn.close()
            return jsonify({'success': False, 'error': 'Account not found'}), 400

        # Detect broker type
        account_broker = (account.get('broker') if isinstance(account, dict) else (account['broker'] if account['broker'] else '')).strip() if account['broker'] else ''
        px_user = account.get('projectx_username') if isinstance(account, dict) else account['projectx_username']
        px_key = account.get('projectx_api_key') if isinstance(account, dict) else account['projectx_api_key']
        is_projectx = (account_broker == 'ProjectX' or bool(px_user) or bool(px_key))

        if not is_projectx and not account['tradovate_token']:
            conn.close()
            return jsonify({'success': False, 'error': 'Account not connected. Please connect via Tradovate or ProjectX.'}), 400
        
        tradovate_accounts = []
        try:
            if account['tradovate_accounts']:
                tradovate_accounts = json.loads(account['tradovate_accounts'])
        except Exception as parse_err:
            logger.warning(f"Unable to parse tradovate_accounts for account {account_id}: {parse_err}")
            tradovate_accounts = []
        
        selected_subaccount = None
        if subaccount_id:
            for ta in tradovate_accounts:
                if str(ta.get('id')) == subaccount_id:
                    selected_subaccount = ta
                    break
        if not selected_subaccount and tradovate_accounts:
            selected_subaccount = tradovate_accounts[0]
            subaccount_id = str(selected_subaccount.get('id'))
        # CRITICAL FIX: Use account.environment as source of truth (from DB, not stale JSON)
        account_env = (account['environment'] or 'demo').lower()
        demo = account_env != 'live'
        logger.info(f"üîç Manual trade DEBUG: account_id={account_id}, environment={account_env}, demo={demo}, subaccount_id={subaccount_id}")
        account_spec = (selected_subaccount.get('name') if selected_subaccount else None) or account['name'] or str(account_id)
        account_numeric_id = int(subaccount_id) if subaccount_id else account_id
        
        token_container = {
            'access_token': account['tradovate_token'],
            'refresh_token': account['tradovate_refresh_token'],
            'md_access_token': account['md_access_token']
        }
        
        expires_at = account['token_expires_at']
        needs_refresh = False
        if expires_at:
            try:
                exp_dt = datetime.fromisoformat(expires_at)
                if datetime.utcnow() >= exp_dt - timedelta(minutes=5):
                    needs_refresh = True
            except Exception:
                needs_refresh = True  # If we can't parse, assume expired
        else:
            needs_refresh = True  # No expiry stored, always try refresh

        def _run_async(coro):
            """Run async code safely ‚Äî handles existing event loops."""
            loop = asyncio.new_event_loop()
            try:
                return loop.run_until_complete(coro)
            finally:
                loop.close()

        def do_refresh_tokens():
            """Refresh Tradovate tokens and save to DB."""
            if not token_container.get('refresh_token'):
                return False
            async def _refresh():
                from phantom_scraper.tradovate_integration import TradovateIntegration
                async with TradovateIntegration(demo=demo) as tradovate:
                    tradovate.access_token = token_container['access_token']
                    tradovate.refresh_token = token_container['refresh_token']
                    tradovate.md_access_token = token_container.get('md_access_token')
                    refreshed = await tradovate.refresh_access_token()
                    if refreshed and refreshed.get('success'):
                        token_container['access_token'] = tradovate.access_token
                        token_container['refresh_token'] = tradovate.refresh_token
                    return refreshed
            refreshed = _run_async(_refresh())
            if refreshed and refreshed.get('success'):
                # Save new tokens to DB
                try:
                    save_conn = get_db_connection()
                    save_cursor = save_conn.cursor()
                    new_expiry = (datetime.utcnow() + timedelta(minutes=85)).isoformat()
                    save_cursor.execute(f"""
                        UPDATE accounts
                        SET tradovate_token = {placeholder}, tradovate_refresh_token = {placeholder}, token_expires_at = {placeholder}
                        WHERE id = {placeholder}
                    """, (token_container['access_token'], token_container['refresh_token'], new_expiry, account_id))
                    save_conn.commit()
                    save_conn.close()
                    logger.info(f"‚úÖ Refreshed and saved tokens for account {account_id}")
                except Exception as save_err:
                    logger.warning(f"Token save error (non-fatal): {save_err}")
            return refreshed

        if needs_refresh and token_container.get('refresh_token'):
            do_refresh_tokens()
        conn.close()

        trade_side = side.lower()
        if trade_side not in ('buy', 'sell', 'close'):
            return jsonify({'success': False, 'error': 'Invalid side supplied'}), 400

        # asyncio imported at top of file (line 19)

        # === ProjectX execution path ===
        if is_projectx:
            try:
                from phantom_scraper.projectx_integration import ProjectXIntegration

                px_username = account.get('projectx_username') or account.get('username') or ''
                px_password = account.get('password') or ''
                px_api_key_val = account.get('projectx_api_key') or ''
                px_prop_firm = account.get('projectx_prop_firm') or 'default'

                if not px_username:
                    return jsonify({'success': False, 'error': 'No ProjectX username. Please reconnect.'}), 400

                async def place_projectx_trade():
                    async with ProjectXIntegration(demo=demo, prop_firm=px_prop_firm) as projectx:
                        login_result = await projectx.login(
                            px_username,
                            password=px_password if px_password else None,
                            api_key=px_api_key_val if px_api_key_val else None
                        )
                        if not login_result.get('success'):
                            return {'success': False, 'error': login_result.get('error', 'ProjectX login failed')}

                        # Find contract for symbol
                        symbol_root = extract_symbol_root(symbol)
                        symbol_upper = symbol.strip().upper() if symbol else ''
                        contract_id = None
                        contract_match_name = None

                        # Try Contract/search first (works on TopStepX and all ProjectX firms)
                        contracts = await projectx.search_contracts(symbol_root)
                        if not contracts:
                            # Fallback: try Contract/available (older ProjectX endpoint)
                            contracts = await projectx.get_available_contracts()

                        logger.info(f"ProjectX manual trade: looking for symbol_root='{symbol_root}' full='{symbol_upper}' in {len(contracts)} contracts")
                        for c in contracts:
                            c_name = (c.get('name') or c.get('symbol') or '').upper()
                            c_id_str = str(c.get('id') or '').upper()
                            # Strategy 1: Full Tradovate symbol in contract name/id (MNQH6 in CON.F.US.MNQH6.H26)
                            if symbol_upper and (symbol_upper in c_name or symbol_upper in c_id_str):
                                contract_id = c.get('id')
                                contract_match_name = c_name or c_id_str
                                break
                            # Strategy 2: Root symbol match (MNQ in contract)
                            if symbol_root and (symbol_root in c_name or symbol_root in c_id_str):
                                contract_id = c.get('id')
                                contract_match_name = c_name or c_id_str
                                break

                        if contract_id:
                            logger.info(f"ProjectX contract matched: {contract_match_name} (ID: {contract_id})")
                        else:
                            # Log all available contracts for debugging
                            contract_names = [f"{c.get('name') or c.get('symbol') or 'N/A'} (id={c.get('id')})" for c in contracts[:20]]
                            logger.warning(f"ProjectX contract not found for '{symbol}' (root='{symbol_root}'). {len(contracts)} contracts returned: {contract_names}")
                            return {'success': False, 'error': f'Contract not found for {symbol}. Available: {", ".join(contract_names[:10])}'}

                        px_account_id = int(subaccount_id) if subaccount_id else None
                        if not px_account_id:
                            # Try to get first account
                            accts = await projectx.get_accounts()
                            if accts:
                                px_account_id = accts[0].get('id')
                        if not px_account_id:
                            return {'success': False, 'error': 'No ProjectX account found'}

                        if trade_side == 'close':
                            # Step 1: Cancel ALL resting orders for this contract
                            cancelled_count = 0
                            try:
                                open_orders = await projectx.get_orders(px_account_id)
                                for order in open_orders:
                                    o_contract = str(order.get('contractId') or '').upper()
                                    o_id = order.get('id')
                                    if o_id and (symbol_root in o_contract or symbol_upper in o_contract or o_contract == str(contract_id).upper()):
                                        ok = await projectx.cancel_order(px_account_id, int(o_id))
                                        if ok:
                                            cancelled_count += 1
                                logger.info(f"ProjectX close: cancelled {cancelled_count} resting orders")
                            except Exception as cancel_err:
                                logger.warning(f"ProjectX cancel orders error (non-fatal): {cancel_err}")

                            # Step 2: Close position via Position/closeContract
                            positions = await projectx.get_positions(px_account_id)
                            logger.info(f"ProjectX close: {len(positions)} positions for account {px_account_id}")
                            for pos in positions:
                                pos_contract = str(pos.get('contractId') or pos.get('contract_id') or '')
                                pos_size = pos.get('size') or pos.get('qty') or 0
                                pos_type = pos.get('type', 0)  # 1=Long, 2=Short
                                logger.info(f"  Position: contractId={pos_contract}, size={pos_size}, type={pos_type}")
                                contract_match = (pos_contract == str(contract_id)
                                                  or symbol_root in pos_contract.upper()
                                                  or symbol_upper in pos_contract.upper())
                                if contract_match and pos_size != 0:
                                    result = await projectx.liquidate_position(px_account_id, pos_contract)
                                    if result and result.get('success'):
                                        return {'success': True, 'message': f'Closed {pos_size} {symbol} + cancelled {cancelled_count} orders', 'order': result}
                                    # Fallback: counter market order
                                    close_side = 'Sell' if pos_type == 1 else 'Buy'
                                    order_data = projectx.create_market_order_with_brackets(
                                        px_account_id, pos_contract, close_side, abs(int(pos_size))
                                    )
                                    result = await projectx.place_order(order_data)
                                    return {'success': True, 'message': f'Closed {pos_size} {symbol} + cancelled {cancelled_count} orders', 'order': result}
                            if cancelled_count > 0:
                                return {'success': True, 'message': f'No position found but cancelled {cancelled_count} resting orders for {symbol}'}
                            return {'success': False, 'error': f'No open position found for {symbol}'}
                        else:
                            # Buy or Sell
                            px_side = 'Buy' if trade_side == 'buy' else 'Sell'
                            # Parse risk settings: frontend sends take_profit/stop_loss/trail
                            tp_list = risk_settings.get('take_profit') or []
                            sl_cfg = risk_settings.get('stop_loss') or {}
                            trail_cfg = risk_settings.get('trail') or {}
                            tp_ticks = tp_list[0].get('gain_ticks', 0) if tp_list else 0
                            sl_ticks = sl_cfg.get('loss_ticks', 0) if sl_cfg else 0
                            trail_ticks = trail_cfg.get('activation_ticks', 0) or trail_cfg.get('offset_ticks', 0) if trail_cfg else 0
                            # If trailing stop enabled, use it as the SL bracket with TrailingStop type
                            use_trailing = bool(trail_ticks and int(trail_ticks) > 0)
                            logger.info(f"ProjectX order: {px_side} {quantity} contract_id={contract_id} tp={tp_ticks} sl={sl_ticks} trail={trail_ticks}")
                            order_data = projectx.create_market_order_with_brackets(
                                px_account_id, contract_id, px_side, quantity,
                                tp_ticks=int(tp_ticks) if tp_ticks else None,
                                sl_ticks=int(trail_ticks if use_trailing else sl_ticks) if (trail_ticks or sl_ticks) else None,
                                trailing_stop=use_trailing
                            )
                            result = await projectx.place_order(order_data)
                            if result and (result.get('success') or result.get('id')):
                                return {'success': True, 'message': f'{px_side} {quantity} {symbol} on ProjectX', 'order': result}
                            return {'success': False, 'error': result.get('error', 'Order placement failed'), 'details': result}

                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(place_projectx_trade())
                finally:
                    loop.close()

                if result.get('success'):
                    # Propagate to followers if this is a leader account (fire-and-forget)
                    if leader_id and (not cl_ord_id or not cl_ord_id.startswith('JT_COPY_')):
                        logger.info(f"Copy trader: triggering propagation for leader {leader_id}")
                        threading.Thread(
                            target=_propagate_manual_trade_to_followers,
                            args=(leader_id, symbol, side, quantity, risk_settings),
                            daemon=True
                        ).start()
                    return jsonify(result)
                return jsonify(result), 400

            except ImportError:
                return jsonify({'success': False, 'error': 'ProjectX integration not available'}), 500
            except Exception as px_err:
                logger.error(f"ProjectX manual trade error: {px_err}")
                import traceback
                logger.error(traceback.format_exc())
                return jsonify({'success': False, 'error': str(px_err)}), 500

        # === Tradovate execution path ===
        tradovate_symbol = convert_tradingview_to_tradovate_symbol(symbol, access_token=token_container['access_token'], demo=demo)
        order_side = 'Buy' if trade_side == 'buy' else 'Sell'
        if trade_side == 'close':
            order_side = 'Sell'
        
        from phantom_scraper.tradovate_integration import TradovateIntegration
        async def place_trade():
            async with TradovateIntegration(demo=demo) as tradovate:
                tradovate.access_token = token_container['access_token']
                tradovate.refresh_token = token_container['refresh_token']
                tradovate.md_access_token = token_container['md_access_token']
                
                if trade_side == 'close':
                    symbol_upper = tradovate_symbol.upper()
                    logger.info(f"=== CLOSING POSITION + CANCELLING ALL ORDERS FOR {symbol_upper} ===")
                    
                    results = []
                    total_closed = 0
                    total_cancelled = 0
                    
                    # STEP 1: Get positions FIRST to find what we need to close
                    positions = await tradovate.get_positions(account_numeric_id)
                    logger.info(f"Step 1: Retrieved {len(positions)} positions for account {account_numeric_id}")
                    
                    # Log all positions for debugging
                    for idx, pos in enumerate(positions):
                        pos_symbol = pos.get('symbol', 'N/A')
                        pos_net = pos.get('netPos', 0)
                        pos_contract = pos.get('contractId')
                        logger.info(f"  Position {idx+1}: symbol={pos_symbol}, netPos={pos_net}, contractId={pos_contract}")
                    
                    # Match positions - try multiple matching strategies
                    matched_positions = []
                    normalized_target = normalize_symbol(symbol_upper)
                    
                    for pos in positions:
                        pos_symbol = str(pos.get('symbol', '')).upper()
                        pos_net = pos.get('netPos', 0)
                        
                        if not pos_net:  # Skip flat positions
                            continue
                        
                        # Try exact match first
                        if pos_symbol == symbol_upper:
                            matched_positions.append(pos)
                            continue
                        
                        # Try normalized match (handles MNQ vs MNQZ4, etc.)
                        pos_normalized = normalize_symbol(pos_symbol)
                        if pos_normalized == normalized_target:
                            matched_positions.append(pos)
                            continue
                        
                        # Try base symbol match (MNQ matches MNQZ4)
                        pos_base = re.sub(r'[A-Z]\d+$', '', pos_normalized)  # Remove month+year
                        target_base = re.sub(r'[A-Z]\d+$', '', normalized_target)
                        if pos_base and target_base and (pos_base == target_base or pos_base in target_base or target_base in pos_base):
                            matched_positions.append(pos)
                            continue
                    
                    logger.info(f"Step 1b: Found {len(matched_positions)} matching positions for {symbol_upper}")
                    
                    # STEP 2: Close positions using liquidateposition (this also cancels related orders)
                    for pos in matched_positions:
                        contract_id = pos.get('contractId')
                        pos_symbol = pos.get('symbol', symbol_upper)
                        net_pos = pos.get('netPos', 0)
                        
                        if not contract_id:
                            logger.warning(f"Position for {pos_symbol} has no contractId, using manual close")
                            # Manual close
                            qty = abs(int(net_pos))
                            if qty > 0:
                                close_side = 'Sell' if net_pos > 0 else 'Buy'
                                order_data = tradovate.create_market_order(account_spec, pos_symbol, close_side, qty, account_numeric_id)
                                result = await tradovate.place_order(order_data)
                                if result and result.get('success'):
                                    results.append(result)
                                    total_closed += qty
                            continue
                        
                        logger.info(f"Step 2: Liquidating position for {pos_symbol} (contractId: {contract_id}, netPos: {net_pos})")
                        
                        # Use liquidateposition endpoint - this SHOULD close position AND cancel related orders
                        result = await tradovate.liquidate_position(account_numeric_id, contract_id, admin=False)
                        
                        if result and result.get('success'):
                            results.append(result)
                            total_closed += abs(int(net_pos))
                            logger.info(f"‚úÖ Successfully liquidated position for {pos_symbol}")
                        else:
                            error_msg = result.get('error', 'Unknown error') if result else 'No response'
                            logger.warning(f"‚ö†Ô∏è liquidateposition returned: {error_msg}, falling back to manual close")
                            
                            # Fallback: Manual close
                            qty = abs(int(net_pos))
                            if qty > 0:
                                close_side = 'Sell' if net_pos > 0 else 'Buy'
                                logger.info(f"Manual close: {close_side} {qty} {pos_symbol}")
                                order_data = tradovate.create_market_order(account_spec, pos_symbol, close_side, qty, account_numeric_id)
                                result = await tradovate.place_order(order_data)
                                if result and result.get('success'):
                                    results.append(result)
                                    total_closed += qty
                    
                    # STEP 3: Cancel ALL remaining orders and strategies (cleanup)
                    logger.info(f"Step 3: Cancelling any remaining orders and strategies")
                    
                    # Get and interrupt order strategies
                    try:
                        all_strategies = await tradovate.get_order_strategies(account_numeric_id)
                        for strategy in all_strategies:
                            strategy_id = strategy.get('id')
                            strategy_status = (strategy.get('status') or '').lower()
                            if strategy_status not in ['completed', 'complete', 'cancelled', 'canceled', 'failed']:
                                logger.info(f"Interrupting order strategy {strategy_id}")
                                await tradovate.interrupt_order_strategy(strategy_id)
                    except Exception as e:
                        logger.warning(f"Error interrupting order strategies: {e}")
                    
                    # Cancel all individual orders
                    cancelled_after = await cancel_open_orders(tradovate, account_numeric_id, None, cancel_all=True)
                    total_cancelled += cancelled_after
                    logger.info(f"Cancelled {cancelled_after} additional orders")
                    
                    # STEP 4: Final verification
                    final_positions = await tradovate.get_positions(account_numeric_id)
                    still_open = [p for p in final_positions if normalize_symbol(p.get('symbol', '')) == normalized_target and p.get('netPos', 0) != 0]
                    
                    logger.info(f"=== CLOSE COMPLETE: Closed {total_closed} contracts, cancelled {total_cancelled} orders ===")
                    
                    if still_open:
                        logger.warning(f"‚ö†Ô∏è Position still open after close attempt!")
                        # Try one more time with direct market order
                        for pos in still_open:
                            qty = abs(int(pos.get('netPos', 0)))
                            close_side = 'Sell' if pos.get('netPos', 0) > 0 else 'Buy'
                            order_data = tradovate.create_market_order(account_spec, pos.get('symbol'), close_side, qty, account_numeric_id)
                            result = await tradovate.place_order(order_data)
                            if result and result.get('success'):
                                total_closed += qty
                                results.append(result)
                    
                    # Build response
                    if total_closed > 0 or total_cancelled > 0:
                        message = f'Closed {total_closed} contracts for {symbol_upper}'
                        if total_cancelled > 0:
                            message += f' and cancelled {total_cancelled} resting orders'
                        
                        response = {
                            'success': True,
                            'message': message,
                            'closed_quantity': total_closed,
                            'cancelled_orders': total_cancelled
                        }
                        if results:
                            response['orderId'] = results[-1].get('data', {}).get('orderId') or results[-1].get('orderId')
                        return response
                    
                    # Nothing to close or cancel
                    if not matched_positions:
                        return {'success': True, 'message': f'No open position found for {symbol_upper}. Nothing to close.'}
                    
                    return {'success': False, 'error': 'Failed to close position'}
                else:
                    order_data = tradovate.create_market_order(
                        account_spec,
                        tradovate_symbol,
                        order_side,
                        quantity,
                        account_numeric_id
                    )
                    # DEBUG: Log the order being placed
                    logger.info(f"üì§ Manual trade order: endpoint={'DEMO' if demo else 'LIVE'}, account_spec={account_spec}, account_id={account_numeric_id}")
                    logger.info(f"üì§ Order data: {order_data}")
                    if risk_settings:
                        order_data.setdefault('customFields', {})['riskSettings'] = risk_settings
                    if cl_ord_id:
                        order_data['clOrdId'] = cl_ord_id  # Copy trader loop prevention (Step 6)
                    result = await tradovate.place_order(order_data)
                    logger.info(f"üì• Order result: {result}")
                    if result and result.get('success') and risk_settings:
                        await apply_risk_orders(
                            tradovate,
                            account_spec,
                            account_numeric_id,
                            tradovate_symbol,
                            order_side,
                            quantity,
                            risk_settings
                        )
                    return result or {'success': False, 'error': 'Failed to place order'}
        
        result = _run_async(place_trade())
        if not result.get('success'):
            error_text = str(result.get('error', '')).lower()
            if any(msg in error_text for msg in ['access is denied', 'expired access token']):
                logger.info(f"üîÑ Token expired for account {account_id}, refreshing...")
                refreshed = do_refresh_tokens()
                if refreshed:
                    result = _run_async(place_trade())
        if not result.get('success'):
            return jsonify({'success': False, 'error': result.get('error', 'Failed to place order')}), 400

        # Log the order response to see what accountId it was placed on
        order_id = result.get('orderId') or result.get('data', {}).get('orderId')
        order_response = result.get('raw') or result
        logger.info(f"Order placed - Order ID: {order_id}, Account used: {account_numeric_id} ({account_spec}), Full response: {order_response}")
        
        # The order response doesn't include accountId, but we know which one we used
        logger.info(f"‚úÖ Order {order_id} placed on account {account_numeric_id} ({account_spec})")
        
        # Since Tradovate's position API returns 0, we need to track positions from filled orders
        # Get fill price from order status after a short delay
        if result.get('success') and order_id:
            import threading
            def get_fill_price_and_update_position():
                import time
                time.sleep(2)  # Wait 2 seconds for order to fill
                try:
                    from phantom_scraper.tradovate_integration import TradovateIntegration
                    async def fetch_order_details():
                        async with TradovateIntegration(demo=demo) as tradovate:
                            tradovate.access_token = token_container['access_token']
                            tradovate.refresh_token = token_container['refresh_token']
                            tradovate.md_access_token = token_container['md_access_token']
                            
                            # Try to get order details - but orders API returns 0
                            # Instead, check if fill price is in the order response itself
                            # Or use a different approach: get fill price from order history
                            
                            # Method 1: Check order response (might have fill price immediately)
                            # This is handled in the main trade function
                            
                            # Method 1: Try /fill/list endpoint (BEST - gets actual fill prices)
                            avg_fill_price = 0.0
                            fills = await tradovate.get_fills(order_id=order_id)
                            if fills:
                                # Get the most recent fill for this order
                                for fill in fills:
                                    if str(fill.get('orderId')) == str(order_id):
                                        avg_fill_price = fill.get('price') or fill.get('fillPrice') or 0.0
                                        logger.info(f"‚úÖ Found fill price from /fill/list: {avg_fill_price}")
                                        break
                            
                            # Method 2: Query order by ID directly
                            if avg_fill_price == 0:
                                try:
                                    async with tradovate.session.get(
                                        f"{tradovate.base_url}/order/item",
                                        params={'id': order_id},
                                        headers=tradovate._get_headers()
                                    ) as order_response:
                                        if order_response.status == 200:
                                            order_data = await order_response.json()
                                            avg_fill_price = order_data.get('avgFillPrice') or order_data.get('price') or 0.0
                                            order_status = order_data.get('ordStatus') or order_data.get('status', '')
                                            logger.info(f"Order {order_id} from /order/item: status={order_status}, avgFillPrice={avg_fill_price}")
                                            if avg_fill_price > 0:
                                                logger.info(f"‚úÖ Found fill price from /order/item: {avg_fill_price}")
                                except Exception as e:
                                    logger.warning(f"Error fetching order item: {e}")
                            
                            # Method 3: Try orders list (fallback)
                            if avg_fill_price == 0:
                                orders = await tradovate.get_orders(None)
                                if orders:
                                    for o in orders:
                                        if str(o.get('id')) == str(order_id):
                                            avg_fill_price = o.get('avgFillPrice') or o.get('price') or o.get('fillPrice') or 0.0
                                            order_status = o.get('ordStatus') or o.get('status', '')
                                            logger.info(f"Order {order_id} from list: status={order_status}, avgFillPrice={avg_fill_price}")
                                            if avg_fill_price > 0:
                                                logger.info(f"‚úÖ Found fill price from /order/list: {avg_fill_price}")
                                            break
                            
                            # If we found a fill price, update the position
                            if avg_fill_price > 0:
                                # Update position with fill price
                                net_qty = quantity if side.lower() == 'buy' else -quantity
                                cache_key = f"{symbol}_{account_numeric_id}"
                                
                                # Get or create position
                                position = _position_cache.get(cache_key, {
                                    'symbol': symbol,
                                    'quantity': net_qty,
                                    'net_quantity': net_qty,
                                    'avg_price': avg_fill_price,
                                    'last_price': avg_fill_price,  # Start with fill price
                                    'unrealized_pnl': 0.0,
                                    'account_id': account_id,
                                    'subaccount_id': str(account_numeric_id),
                                    'account_name': account_spec,
                                    'order_id': order_id,
                                    'open_time': datetime.now().isoformat()
                                })
                                
                                # Update with fill price
                                position['avg_price'] = avg_fill_price
                                position['last_price'] = avg_fill_price
                                _position_cache[cache_key] = position
                                
                                # Store position in database (like Trade Manager)
                                conn = get_db_connection()
                                cursor = conn.cursor()
                                cursor.execute('''
                                    INSERT OR REPLACE INTO open_positions 
                                    (symbol, net_quantity, avg_price, last_price, unrealized_pnl, 
                                     account_id, subaccount_id, account_name, order_id, open_time)
                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                                ''', (
                                    position['symbol'], position['net_quantity'], position['avg_price'],
                                    position['last_price'], position['unrealized_pnl'], position['account_id'],
                                    position['subaccount_id'], position['account_name'], position['order_id'],
                                    position['open_time']
                                ))
                                conn.commit()
                                conn.close()
                                logger.info(f"‚úÖ Stored position in database: {symbol} qty={net_qty} @ {avg_fill_price}")
                                
                                # Update PnL (will be 0 initially since current = fill)
                                update_position_pnl()
                                
                                # Emit updated position
                                socketio.emit('position_update', {
                                    'positions': [position],
                                    'count': 1,
                                    'timestamp': datetime.now().isoformat(),
                                    'source': 'order_fill'
                                })
                                logger.info(f"Updated position with fill price: {avg_fill_price}")
                            else:
                                logger.warning(f"‚ö†Ô∏è Could not get fill price for order {order_id} - will retry later")
                                # Will need to poll again or use market data estimate
                    asyncio.run(fetch_order_details())
                except Exception as e:
                    logger.warning(f"Error fetching fill price: {e}")
            
            # Start background thread to get fill price
            threading.Thread(target=get_fill_price_and_update_position, daemon=True).start()
            
            # Emit initial position (without fill price for now)
            net_qty = quantity if side.lower() == 'buy' else -quantity
            synthetic_position = {
                'symbol': symbol,
                'quantity': net_qty,
                'net_quantity': net_qty,
                'avg_price': 0.0,  # Will be updated when we get fill price
                'last_price': 0.0,  # Will be updated with market data
                'unrealized_pnl': 0.0,
                'account_id': account_id,
                'subaccount_id': str(account_numeric_id),
                'account_name': account_spec,
                'order_id': order_id
            }
            logger.info(f"Emitting initial position for order {order_id} (fill price will be updated)")
            
            # Store in global cache
            cache_key = f"{symbol}_{account_numeric_id}"
            _position_cache[cache_key] = synthetic_position
            
            # Emit the position update
            socketio.emit('position_update', {
                'positions': [synthetic_position],
                'count': 1,
                'timestamp': datetime.now().isoformat(),
                'source': 'order_fill'
            })

        # Emit WebSocket events for real-time updates (like Trade Manager)
        try:
            # Emit log entry
            socketio.emit('log_entry', {
                'type': 'trade',
                'message': f'Trade executed: {side} {quantity} {symbol}',
                'time': datetime.now().isoformat()
            })
            
            # Emit position update
            socketio.emit('position_update', {
                'strategy': 'Manual Trade',
                'symbol': symbol,
                'quantity': quantity,
                'side': side,
                'account': account_spec,
                'timestamp': datetime.now().isoformat()
            })
            
            # Trigger immediate position refresh by clearing cache
            if hasattr(emit_realtime_updates, '_last_position_fetch'):
                emit_realtime_updates._last_position_fetch = 0  # Force refresh on next cycle
            
            # Emit trade executed event
            socketio.emit('trade_executed', {
                'strategy': 'Manual Trade',
                'symbol': symbol,
                'side': side,
                'quantity': quantity,
                'order_id': result.get('orderId', 'N/A'),
                'account': account_spec,
                'timestamp': datetime.now().isoformat()
            })
        except Exception as ws_error:
            logger.error(f"Error emitting WebSocket events: {ws_error}")

        # Propagate to followers if this is a leader account (fire-and-forget)
        if leader_id and (not cl_ord_id or not cl_ord_id.startswith('JT_COPY_')):
            logger.info(f"Copy trader: triggering propagation for leader {leader_id}")
            threading.Thread(
                target=_propagate_manual_trade_to_followers,
                args=(leader_id, symbol, side, quantity, risk_settings),
                daemon=True
            ).start()

        return jsonify({
            'success': True,
            'message': f'{side} order placed for {quantity} {symbol}',
            'order_id': result.get('orderId', 'N/A')
        })

    except Exception as e:
        logger.error(f"Error placing manual trade: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


# ============================================================================
# PRO COPY TRADER API ROUTES
# ============================================================================
# Pure additions ‚Äî zero modifications to any existing code above.

@app.route('/api/copy-trader/leaders', methods=['GET'])
def copy_trader_list_leaders():
    """List leader accounts for the current user."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        from copy_trader_models import get_leaders_for_user
        leaders = get_leaders_for_user(user_id)
        return jsonify({'success': True, 'leaders': leaders})
    except Exception as e:
        logger.error(f"Error listing leaders: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/leaders', methods=['POST'])
def copy_trader_create_leader():
    """Create or update a leader account."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        data = request.get_json() or {}
        account_id = data.get('account_id')
        subaccount_id = str(data.get('subaccount_id', ''))
        label = data.get('label', '')

        if not account_id or not subaccount_id:
            return jsonify({'success': False, 'error': 'account_id and subaccount_id required'}), 400

        from copy_trader_models import create_leader
        leader_id = create_leader(user_id, int(account_id), subaccount_id, label)
        if leader_id:
            try:
                from ws_leader_monitor import trigger_leader_reload
                trigger_leader_reload()
            except Exception:
                pass
            return jsonify({'success': True, 'leader_id': leader_id})
        return jsonify({'success': False, 'error': 'Failed to create leader'}), 500
    except Exception as e:
        logger.error(f"Error creating leader: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/followers/<int:leader_id>', methods=['GET'])
def copy_trader_list_followers(leader_id):
    """List followers for a leader account."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        from copy_trader_models import get_leader_by_id, get_all_followers_for_leader
        leader = get_leader_by_id(leader_id)
        if not leader or leader.get('user_id') != user_id:
            return jsonify({'success': False, 'error': 'Leader not found'}), 404

        followers = get_all_followers_for_leader(leader_id)
        return jsonify({'success': True, 'followers': followers})
    except Exception as e:
        logger.error(f"Error listing followers: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/followers/<int:leader_id>', methods=['POST'])
def copy_trader_add_follower(leader_id):
    """Add a follower account to a leader."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        data = request.get_json() or {}
        account_id = data.get('account_id')
        subaccount_id = str(data.get('subaccount_id', ''))
        label = data.get('label', '')
        multiplier = float(data.get('multiplier', 1.0))

        if not account_id or not subaccount_id:
            return jsonify({'success': False, 'error': 'account_id and subaccount_id required'}), 400

        from copy_trader_models import get_leader_by_id, create_follower, check_circular_follower
        leader = get_leader_by_id(leader_id)
        if not leader or leader.get('user_id') != user_id:
            return jsonify({'success': False, 'error': 'Leader not found'}), 404

        # Prevent circular leader-follower loops (A‚ÜíB and B‚ÜíA)
        circular_err = check_circular_follower(leader_id, int(account_id), subaccount_id)
        if circular_err:
            return jsonify({'success': False, 'error': circular_err}), 400

        follower_id = create_follower(leader_id, user_id, int(account_id),
                                       subaccount_id, label, multiplier)
        if follower_id:
            return jsonify({'success': True, 'follower_id': follower_id})
        return jsonify({'success': False, 'error': 'Failed to add follower'}), 500
    except Exception as e:
        logger.error(f"Error adding follower: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/followers/<int:follower_id>', methods=['PUT'])
def copy_trader_update_follower(follower_id):
    """Update follower settings (is_enabled, multiplier, etc.)."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        # Ownership check: verify follower belongs to user's leader
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(f'''
            SELECT fa.id FROM follower_accounts fa
            JOIN leader_accounts la ON fa.leader_id = la.id
            WHERE fa.id = {placeholder} AND la.user_id = {placeholder}
        ''', (follower_id, user_id))
        row = cursor.fetchone()
        conn.close()
        if not row:
            return jsonify({'success': False, 'error': 'Follower not found'}), 404

        data = request.get_json() or {}
        from copy_trader_models import update_follower
        success = update_follower(follower_id, **data)
        return jsonify({'success': success})
    except Exception as e:
        logger.error(f"Error updating follower: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/auto-mode', methods=['POST'])
def copy_trader_toggle_auto_mode():
    """Toggle auto copy mode for a leader (enables/disables WebSocket monitoring)."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        data = request.get_json() or {}
        leader_id = data.get('leader_id')
        enabled = data.get('enabled', False)

        if not leader_id:
            return jsonify({'success': False, 'error': 'leader_id required'}), 400

        from copy_trader_models import get_leader_by_id, update_leader
        leader = get_leader_by_id(leader_id)
        if not leader or leader.get('user_id') != user_id:
            return jsonify({'success': False, 'error': 'Leader not found'}), 404

        success = update_leader(leader_id, auto_copy_enabled=enabled)
        # Trigger leader monitor to reload (picks up new auto_copy_enabled state)
        try:
            from ws_leader_monitor import trigger_leader_reload
            trigger_leader_reload()
        except Exception:
            pass  # Non-fatal ‚Äî monitor will pick up on next cycle
        return jsonify({'success': success, 'auto_copy_enabled': enabled})
    except Exception as e:
        logger.error(f"Error toggling auto mode: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/leaders/<int:leader_id>/log', methods=['GET'])
def copy_trader_get_log(leader_id):
    """Get copy trade log for a leader."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        from copy_trader_models import get_leader_by_id, get_copy_trade_history, get_copy_stats
        leader = get_leader_by_id(leader_id)
        if not leader or leader.get('user_id') != user_id:
            return jsonify({'success': False, 'error': 'Leader not found'}), 404

        log = get_copy_trade_history(leader_id, limit=50)
        stats = get_copy_stats(leader_id)
        return jsonify({'success': True, 'log': log, 'stats': stats})
    except Exception as e:
        logger.error(f"Error getting copy log: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/status', methods=['GET'])
def copy_trader_status():
    """Get copy trader system status (WebSocket monitor connections)."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    # Tier check ‚Äî admin bypass
    user = get_current_user()
    if not (user and user.is_admin):
        from subscription_models import check_feature_access
        if not check_feature_access(user_id, 'advanced_copy_trader'):
            return jsonify({'success': False, 'error': 'Requires Pro Copy Trader or Premium+ plan'}), 403
    try:
        from copy_trader_models import get_leaders_for_user
        from ws_leader_monitor import _leader_connections
        leaders = get_leaders_for_user(user_id)
        status = []
        for leader in leaders:
            lid = leader.get('id')
            conn = _leader_connections.get(lid)
            status.append({
                'leader_id': lid,
                'label': leader.get('label'),
                'auto_copy_enabled': bool(leader.get('auto_copy_enabled')),
                'ws_connected': bool(conn and conn.connected),
                'ws_authenticated': bool(conn and conn.authenticated),
            })
        return jsonify({'success': True, 'leaders': status})
    except Exception as e:
        logger.error(f"Error getting copy trader status: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/copy-trader/leader-position/<int:leader_id>', methods=['GET'])
def copy_trader_leader_position(leader_id):
    """Get leader's tracked position state from the WebSocket monitor."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    try:
        from copy_trader_models import get_leader_by_id
        leader = get_leader_by_id(leader_id)
        if not leader or leader.get('user_id') != user_id:
            return jsonify({'success': False, 'error': 'Leader not found'}), 404

        from ws_leader_monitor import _leader_connections
        conn = _leader_connections.get(leader_id)
        if not conn:
            return jsonify({'success': True, 'positions': {}, 'connected': False})

        return jsonify({
            'success': True,
            'connected': conn.connected,
            'authenticated': conn.authenticated,
            'positions': conn._positions,
        })
    except Exception as e:
        logger.error(f"Error getting leader position: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/admin/copy-trader-toggle', methods=['POST'])
def admin_copy_trader_toggle():
    """Global master switch to enable/disable ALL copy trading."""
    user_id = session.get('user_id')
    if not user_id:
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    try:
        from user_auth import get_user_by_id
        user = get_user_by_id(user_id)
        if not user or not getattr(user, 'is_admin', False):
            return jsonify({'success': False, 'error': 'Admin required'}), 403
        data = request.get_json() or {}
        enabled = data.get('enabled', True)
        from copy_trader_models import set_platform_setting
        success = set_platform_setting('copy_trader_enabled', str(enabled).lower(), user_id)
        return jsonify({'success': success, 'enabled': enabled})
    except Exception as e:
        logger.error(f"Error toggling copy trader: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/admin/copy-trader-toggle', methods=['GET'])
def admin_copy_trader_status():
    """Check if copy trading is globally enabled."""
    try:
        from copy_trader_models import is_copy_trader_enabled
        return jsonify({'enabled': is_copy_trader_enabled()})
    except Exception as e:
        logger.error(f"Error checking copy trader status: {e}")
        return jsonify({'enabled': False})


@app.route('/api/mirror-order', methods=['POST'])
def mirror_order():
    """Internal endpoint for order mirroring ‚Äî place/cancel/modify limit/stop orders on follower accounts.
    Auth: X-Admin-Key header (same as /api/manual-trade internal calls, Rule 32).
    """
    try:
        data = request.get_json() or {}
        operation = data.get('operation', '').strip().lower()
        account_subaccount = data.get('account_subaccount', '').strip()

        if operation not in ('place', 'cancel', 'modify'):
            return jsonify({'success': False, 'error': f'Invalid operation: {operation}'}), 400
        if not account_subaccount:
            return jsonify({'success': False, 'error': 'account_subaccount required'}), 400

        parts = account_subaccount.split(':')
        account_id = int(parts[0])
        subaccount_id = parts[1] if len(parts) > 1 and parts[1] else None

        # Look up account credentials (same pattern as manual_trade)
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute(f"""
            SELECT name, tradovate_token, tradovate_refresh_token, md_access_token,
                   token_expires_at, tradovate_accounts, environment
            FROM accounts WHERE id = {placeholder}
        """, (account_id,))
        account = cursor.fetchone()
        if not account:
            conn.close()
            return jsonify({'success': False, 'error': 'Account not found'}), 400
        if not account['tradovate_token']:
            conn.close()
            return jsonify({'success': False, 'error': 'Account not connected'}), 400

        # Parse tradovate_accounts for subaccount lookup
        tradovate_accounts = []
        try:
            if account['tradovate_accounts']:
                tradovate_accounts = json.loads(account['tradovate_accounts'])
        except Exception:
            tradovate_accounts = []

        selected_subaccount = None
        if subaccount_id:
            for ta in tradovate_accounts:
                if str(ta.get('id')) == subaccount_id:
                    selected_subaccount = ta
                    break
        if not selected_subaccount and tradovate_accounts:
            selected_subaccount = tradovate_accounts[0]
            subaccount_id = str(selected_subaccount.get('id'))

        account_env = (account['environment'] or 'demo').lower()
        demo = account_env != 'live'
        account_spec = (selected_subaccount.get('name') if selected_subaccount else None) or account['name'] or str(account_id)
        account_numeric_id = int(subaccount_id) if subaccount_id else account_id

        access_token = account['tradovate_token']
        conn.close()

        def _run_async(coro):
            loop = asyncio.new_event_loop()
            try:
                return loop.run_until_complete(coro)
            finally:
                loop.close()

        if operation == 'place':
            symbol = data.get('symbol', '').strip()
            action = data.get('action', '').strip()
            quantity = int(data.get('quantity', 1))
            order_type = data.get('order_type', 'Limit')
            price = data.get('price')
            stop_price = data.get('stop_price')
            peg_difference = data.get('peg_difference')
            time_in_force = data.get('time_in_force', 'Day')
            cl_ord_id = data.get('cl_ord_id', '')

            if not symbol or not action:
                return jsonify({'success': False, 'error': 'symbol and action required for place'}), 400

            async def _place():
                from phantom_scraper.tradovate_integration import TradovateIntegration
                async with TradovateIntegration(demo=demo) as integration:
                    integration.access_token = access_token
                    order_data = {
                        'accountId': account_numeric_id,
                        'accountSpec': account_spec,
                        'action': action,
                        'symbol': symbol,
                        'orderQty': quantity,
                        'orderType': order_type,
                        'timeInForce': time_in_force,
                        'isAutomated': True,
                    }
                    if cl_ord_id:
                        order_data['clOrdId'] = cl_ord_id
                    if price is not None:
                        order_data['price'] = float(price)
                    if stop_price is not None:
                        order_data['stopPrice'] = float(stop_price)
                    if peg_difference is not None:
                        order_data['pegDifference'] = float(peg_difference)
                    result = await integration.place_order(order_data)
                    return result

            result = _run_async(_place())
            if result and result.get('success'):
                order_id = result.get('orderId')
                logger.info(f"Mirror order placed: {action} {quantity} {symbol} on {account_subaccount} ‚Üí orderId={order_id}")
                return jsonify({'success': True, 'orderId': order_id})
            else:
                error_msg = (result.get('error') or 'Unknown error') if result else 'No result'
                logger.warning(f"Mirror order place failed on {account_subaccount}: {error_msg}")
                return jsonify({'success': False, 'error': str(error_msg)}), 400

        elif operation == 'cancel':
            order_id = data.get('order_id')
            if not order_id:
                return jsonify({'success': False, 'error': 'order_id required for cancel'}), 400

            async def _cancel():
                from phantom_scraper.tradovate_integration import TradovateIntegration
                async with TradovateIntegration(demo=demo) as integration:
                    integration.access_token = access_token
                    return await integration.cancel_order(int(order_id))

            success = _run_async(_cancel())
            if success:
                logger.info(f"Mirror order cancelled: orderId={order_id} on {account_subaccount}")
                return jsonify({'success': True})
            else:
                logger.warning(f"Mirror order cancel failed: orderId={order_id} on {account_subaccount}")
                return jsonify({'success': False, 'error': f'Failed to cancel order {order_id}'}), 400

        elif operation == 'modify':
            order_id = data.get('order_id')
            if not order_id:
                return jsonify({'success': False, 'error': 'order_id required for modify'}), 400

            new_price = data.get('price')
            new_stop_price = data.get('stop_price')
            new_qty = data.get('order_qty')
            order_type = data.get('order_type', 'Limit')
            time_in_force = data.get('time_in_force', 'Day')

            async def _modify():
                from phantom_scraper.tradovate_integration import TradovateIntegration
                async with TradovateIntegration(demo=demo) as integration:
                    integration.access_token = access_token
                    return await integration.modify_order(
                        order_id=int(order_id),
                        new_price=float(new_price) if new_price is not None else None,
                        new_qty=int(new_qty) if new_qty is not None else None,
                        stop_price=float(new_stop_price) if new_stop_price is not None else None,
                        order_type=order_type,
                        time_in_force=time_in_force,
                    )

            result = _run_async(_modify())
            if result and result.get('success'):
                logger.info(f"Mirror order modified: orderId={order_id} on {account_subaccount}")
                return jsonify({'success': True})
            else:
                error_msg = (result.get('error') or 'Unknown error') if result else 'No result'
                logger.warning(f"Mirror order modify failed: orderId={order_id} on {account_subaccount}: {error_msg}")
                return jsonify({'success': False, 'error': str(error_msg)}), 400

    except Exception as e:
        logger.error(f"Mirror order error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/settings')
def settings():
    # Require login if auth is available
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    
    # Get current user data for settings page
    user = None
    discord_linked = False
    discord_dms_enabled = False
    user_timezone = 'America/Chicago'
    if USER_AUTH_AVAILABLE and is_logged_in():
        user = get_current_user()
        if user:
            from user_auth import get_auth_db_connection
            conn, db_type = get_auth_db_connection()
            cursor = conn.cursor()
            try:
                if db_type == 'postgresql':
                    cursor.execute('SELECT discord_user_id, discord_dms_enabled, settings_json FROM users WHERE id = %s', (user.id,))
                else:
                    cursor.execute('SELECT discord_user_id, discord_dms_enabled, settings_json FROM users WHERE id = ?', (user.id,))
                row = cursor.fetchone()
                if row:
                    discord_linked = bool(row['discord_user_id'] if hasattr(row, 'keys') else row[0])
                    discord_dms_enabled = bool(row['discord_dms_enabled'] if hasattr(row, 'keys') else row[1])
                    _sj = row['settings_json'] if hasattr(row, 'keys') else row[2]
                    if _sj:
                        _sj_parsed = json.loads(_sj) if isinstance(_sj, str) else _sj
                        user_timezone = _sj_parsed.get('timezone', 'America/Chicago')
            except:
                pass
            finally:
                cursor.close()
                conn.close()

    return render_template('settings.html',
                          user=user,
                          discord_linked=discord_linked,
                          discord_dms_enabled=discord_dms_enabled,
                          user_timezone=user_timezone)


# ============================================================================
# SETTINGS API ROUTES
# ============================================================================

@app.route('/api/settings/timezone', methods=['POST'])
def api_save_timezone():
    """Save user timezone preference."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    user = get_current_user()
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404

    VALID_TIMEZONES = {
        'America/New_York', 'America/Chicago', 'America/Denver', 'America/Los_Angeles',
        'America/Anchorage', 'Pacific/Honolulu', 'Europe/London', 'Europe/Berlin',
        'Asia/Tokyo', 'Asia/Shanghai', 'Australia/Sydney', 'UTC'
    }
    data = request.get_json() or {}
    tz = data.get('timezone', '').strip()
    if tz not in VALID_TIMEZONES:
        return jsonify({'success': False, 'error': f'Invalid timezone: {tz}'}), 400

    try:
        from user_auth import get_auth_db_connection
        conn, db_type = get_auth_db_connection()
        cursor = conn.cursor()
        ph = '%s' if db_type == 'postgresql' else '?'

        # Read existing settings_json
        cursor.execute(f'SELECT settings_json FROM users WHERE id = {ph}', (user.id,))
        row = cursor.fetchone()
        existing = {}
        if row:
            raw = row['settings_json'] if hasattr(row, 'keys') else row[0]
            if raw:
                existing = json.loads(raw) if isinstance(raw, str) else raw

        existing['timezone'] = tz
        cursor.execute(f'UPDATE users SET settings_json = {ph} WHERE id = {ph}', (json.dumps(existing), user.id))
        conn.commit()
        cursor.close()
        conn.close()
        return jsonify({'success': True, 'timezone': tz})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/settings/password', methods=['POST'])
def api_change_password():
    """Change user password."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth system not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    user = get_current_user()
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    data = request.get_json()
    new_password = data.get('password', '').strip()
    confirm_password = data.get('confirm_password', '').strip()
    
    if not new_password or len(new_password) < 6:
        return jsonify({'success': False, 'error': 'Password must be at least 6 characters'}), 400
    
    if new_password != confirm_password:
        return jsonify({'success': False, 'error': 'Passwords do not match'}), 400
    
    from user_auth import update_user_password
    if update_user_password(user.id, new_password):
        return jsonify({'success': True, 'message': 'Password changed successfully'})
    else:
        return jsonify({'success': False, 'error': 'Failed to update password'}), 500


@app.route('/api/settings/username', methods=['POST'])
def api_request_username_change():
    """Request username change (submits for admin approval)."""
    if not USER_AUTH_AVAILABLE:
        return jsonify({'success': False, 'error': 'Auth system not available'}), 400
    
    if not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    user = get_current_user()
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    data = request.get_json()
    new_username = data.get('username', '').strip().lower()
    
    if not new_username or len(new_username) < 3:
        return jsonify({'success': False, 'error': 'Username must be at least 3 characters'}), 400
    
    if new_username == user.username:
        return jsonify({'success': False, 'error': 'New username must be different'}), 400
    
    # Check if username already exists
    from user_auth import get_auth_db_connection
    conn, db_type = get_auth_db_connection()
    cursor = conn.cursor()
    
    try:
        # Check if username already exists in users table
        if db_type == 'postgresql':
            cursor.execute('SELECT id FROM users WHERE username = %s AND id != %s', (new_username, user.id))
        else:
            cursor.execute('SELECT id FROM users WHERE username = ? AND id != ?', (new_username, user.id))
        
        if cursor.fetchone():
            return jsonify({'success': False, 'error': 'Username already exists'}), 400
        
        # Check if username is already pending
        # Ensure pending_username_changes table exists first
        if db_type == 'postgresql':
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pending_username_changes (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL,
                    old_username VARCHAR(100) NOT NULL,
                    new_username VARCHAR(100) NOT NULL,
                    status VARCHAR(20) DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    reviewed_by INTEGER,
                    reviewed_at TIMESTAMP
                )
            ''')
            cursor.execute('SELECT id FROM pending_username_changes WHERE new_username = %s AND status = %s', (new_username, 'pending'))
        else:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS pending_username_changes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    old_username TEXT NOT NULL,
                    new_username TEXT NOT NULL,
                    status TEXT DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    reviewed_by INTEGER,
                    reviewed_at TIMESTAMP
                )
            ''')
            cursor.execute('SELECT id FROM pending_username_changes WHERE new_username = ? AND status = ?', (new_username, 'pending'))
        
        if cursor.fetchone():
            return jsonify({'success': False, 'error': 'Username change already pending'}), 400
        
        # Create pending request
        if db_type == 'postgresql':
            cursor.execute('''
                INSERT INTO pending_username_changes (user_id, old_username, new_username, status)
                VALUES (%s, %s, %s, 'pending')
            ''', (user.id, user.username, new_username))
        else:
            cursor.execute('''
                INSERT INTO pending_username_changes (user_id, old_username, new_username, status)
                VALUES (?, ?, ?, 'pending')
            ''', (user.id, user.username, new_username))
        
        conn.commit()
        logger.info(f"‚úÖ Username change requested: user {user.id} ({user.username} -> {new_username})")
        return jsonify({'success': True, 'message': 'Username change request submitted. An admin will review it soon.'})
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå Failed to request username change: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        cursor.close()
        conn.close()


## Discord toggle/status/test routes moved to discord_routes.py

# ============================================================================
# PUSH NOTIFICATION SETTINGS ROUTES
# ============================================================================

@app.route('/api/push/vapid-public-key', methods=['GET'])
def api_get_vapid_public_key():
    """Get the VAPID public key for push notification subscription."""
    if not PUSH_NOTIFICATIONS_ENABLED:
        return jsonify({'error': 'Push notifications not configured'}), 400
    return jsonify({'publicKey': VAPID_PUBLIC_KEY})


@app.route('/api/push/subscribe', methods=['POST'])
def api_push_subscribe():
    """Subscribe to push notifications."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    if not PUSH_NOTIFICATIONS_ENABLED:
        return jsonify({'success': False, 'error': 'Push notifications not configured on server'}), 400
    
    user = get_current_user()
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    data = request.json
    subscription = data.get('subscription')
    
    if not subscription or not subscription.get('endpoint'):
        return jsonify({'success': False, 'error': 'Invalid subscription data'}), 400
    
    import json
    subscription_json = json.dumps(subscription)
    endpoint = subscription.get('endpoint')
    user_agent = request.headers.get('User-Agent', '')[:500]
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Check if subscription already exists
        if is_using_postgres():
            cursor.execute('''
                INSERT INTO push_subscriptions (user_id, subscription_json, endpoint, user_agent, active)
                VALUES (%s, %s, %s, %s, TRUE)
                ON CONFLICT (user_id, endpoint) 
                DO UPDATE SET subscription_json = EXCLUDED.subscription_json, active = TRUE, updated_at = NOW()
            ''', (user.id, subscription_json, endpoint, user_agent))
        else:
            cursor.execute('''
                INSERT OR REPLACE INTO push_subscriptions (user_id, subscription_json, endpoint, user_agent, active)
                VALUES (?, ?, ?, ?, 1)
            ''', (user.id, subscription_json, endpoint, user_agent))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"‚úÖ Push subscription created for user {user.id}")
        return jsonify({'success': True, 'message': 'Push notifications enabled'})
    except Exception as e:
        logger.error(f"‚ùå Failed to create push subscription: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/push/unsubscribe', methods=['POST'])
def api_push_unsubscribe():
    """Unsubscribe from push notifications."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    user = get_current_user()
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    data = request.json
    endpoint = data.get('endpoint')
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if endpoint:
            # Remove specific subscription
            if is_using_postgres():
                cursor.execute('DELETE FROM push_subscriptions WHERE user_id = %s AND endpoint = %s', (user.id, endpoint))
            else:
                cursor.execute('DELETE FROM push_subscriptions WHERE user_id = ? AND endpoint = ?', (user.id, endpoint))
        else:
            # Remove all subscriptions for user
            if is_using_postgres():
                cursor.execute('DELETE FROM push_subscriptions WHERE user_id = %s', (user.id,))
            else:
                cursor.execute('DELETE FROM push_subscriptions WHERE user_id = ?', (user.id,))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"‚úÖ Push subscription removed for user {user.id}")
        return jsonify({'success': True, 'message': 'Push notifications disabled'})
    except Exception as e:
        logger.error(f"‚ùå Failed to remove push subscription: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/push/status', methods=['GET'])
def api_push_status():
    """Get push notification status for current user."""
    status = {
        'configured': PUSH_NOTIFICATIONS_ENABLED,
        'subscribed': False,
        'subscription_count': 0
    }
    
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify(status)
    
    user = get_current_user()
    if not user:
        return jsonify(status)
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('SELECT COUNT(*) FROM push_subscriptions WHERE user_id = %s AND active = TRUE', (user.id,))
        else:
            cursor.execute('SELECT COUNT(*) FROM push_subscriptions WHERE user_id = ? AND active = 1', (user.id,))
        
        row = cursor.fetchone()
        count = row[0] if row else 0
        
        cursor.close()
        conn.close()
        
        status['subscribed'] = count > 0
        status['subscription_count'] = count
        return jsonify(status)
    except Exception as e:
        logger.error(f"Error checking push status: {e}")
        return jsonify(status)


@app.route('/api/push/test', methods=['POST'])
def api_test_push_notification():
    """Send a test push notification to the current user."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    if not PUSH_NOTIFICATIONS_ENABLED:
        return jsonify({'success': False, 'error': 'Push notifications not configured on server'}), 400
    
    user = get_current_user()
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 404
    
    sent = send_push_notification(
        user_id=user.id,
        title='üß™ Test Notification',
        body='This is a test push notification from Just.Trades!',
        url='/settings'
    )
    
    if sent > 0:
        return jsonify({'success': True, 'message': f'Test notification sent to {sent} device(s)!'})
    else:
        return jsonify({'success': False, 'error': 'No push subscriptions found. Enable notifications first.'}), 400


## Discord link/callback routes moved to discord_routes.py

# ============================================================================
# COMMUNITY CHAT - Public chat room for all users
# ============================================================================

# Store for tracking online users (in-memory, resets on server restart)
_chat_online_users = {}  # user_id -> last_heartbeat_time

def ensure_chat_table():
    """Ensure the chat_messages table exists."""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if is_using_postgres():
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS chat_messages (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL,
                    username VARCHAR(100) NOT NULL,
                    display_name VARCHAR(100),
                    is_admin BOOLEAN DEFAULT FALSE,
                    message TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            # Index for faster queries
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_chat_messages_created ON chat_messages(created_at DESC)')
        else:
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS chat_messages (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    username TEXT NOT NULL,
                    display_name TEXT,
                    is_admin INTEGER DEFAULT 0,
                    message TEXT NOT NULL,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_chat_messages_created ON chat_messages(created_at DESC)')
        
        conn.commit()
        logger.info("‚úÖ Chat messages table ready")
        return True
    except Exception as e:
        logger.error(f"‚ùå Error creating chat table: {e}")
        return False
    finally:
        cursor.close()
        conn.close()

# Initialize chat table on startup
ensure_chat_table()


@app.route('/chat')
def chat_page():
    """Community chat page."""
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return redirect(url_for('login'))
    return render_template('chat.html')


@app.route('/api/chat/messages', methods=['GET'])
def get_chat_messages():
    """Get chat messages, optionally after a certain ID."""
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    after_id = request.args.get('after', 0, type=int)
    limit = request.args.get('limit', 100, type=int)
    limit = min(limit, 500)  # Cap at 500 messages
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        placeholder = '%s' if is_using_postgres() else '?'
        
        if after_id > 0:
            cursor.execute(f'''
                SELECT id, user_id, username, display_name, is_admin, message, created_at 
                FROM chat_messages 
                WHERE id > {placeholder}
                ORDER BY id ASC
                LIMIT {placeholder}
            ''', (after_id, limit))
        else:
            # Get last N messages
            cursor.execute(f'''
                SELECT id, user_id, username, display_name, is_admin, message, created_at 
                FROM chat_messages 
                ORDER BY id DESC
                LIMIT {placeholder}
            ''', (limit,))
        
        rows = cursor.fetchall()
        conn.close()
        
        messages = []
        for row in rows:
            if hasattr(row, 'keys'):
                msg = dict(row)
            else:
                msg = {
                    'id': row[0],
                    'user_id': row[1],
                    'username': row[2],
                    'display_name': row[3],
                    'is_admin': bool(row[4]),
                    'message': row[5],
                    'created_at': str(row[6])
                }
            messages.append(msg)
        
        # Reverse if we fetched in DESC order (initial load)
        if after_id == 0:
            messages.reverse()
        
        # Count online users (heartbeat within last 60 seconds)
        now = time.time()
        online_count = sum(1 for ts in _chat_online_users.values() if now - ts < 60)
        
        return jsonify({
            'success': True,
            'messages': messages,
            'online_count': max(online_count, 1)  # At least 1 (current user)
        })
    except Exception as e:
        logger.error(f"Error getting chat messages: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/chat/send', methods=['POST'])
def send_chat_message():
    """Send a chat message."""
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return jsonify({'success': False, 'error': 'Not logged in'}), 401
    
    user = get_current_user()
    if not user:
        return jsonify({'success': False, 'error': 'User not found'}), 401
    
    data = request.get_json()
    message = data.get('message', '').strip() if data else ''
    
    if not message:
        return jsonify({'success': False, 'error': 'Message is required'}), 400
    
    if len(message) > 5000:
        return jsonify({'success': False, 'error': 'Message too long (max 5000 characters)'}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        placeholder = '%s' if is_using_postgres() else '?'
        
        cursor.execute(f'''
            INSERT INTO chat_messages (user_id, username, display_name, is_admin, message)
            VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
        ''', (user.id, user.username, user.display_name, user.is_admin, message))
        
        conn.commit()
        
        # Get the inserted message ID
        if is_using_postgres():
            cursor.execute('SELECT lastval()')
            message_id = cursor.fetchone()[0]
        else:
            message_id = cursor.lastrowid
        
        conn.close()
        
        logger.info(f"üí¨ Chat message from {user.username}: {message[:50]}...")
        
        return jsonify({
            'success': True,
            'message_id': message_id
        })
    except Exception as e:
        logger.error(f"Error sending chat message: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/chat/heartbeat', methods=['POST'])
def chat_heartbeat():
    """Update user's online status."""
    if USER_AUTH_AVAILABLE and not is_logged_in():
        return jsonify({'success': False}), 401
    
    user = get_current_user()
    if user:
        _chat_online_users[user.id] = time.time()
    
    return jsonify({'success': True})


@app.route('/api/user/theme', methods=['GET', 'POST'])
def api_user_theme():
    """Get or update user's theme preference (dark/light mode)."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify({'error': 'Not authenticated'}), 401
    
    try:
        user = get_current_user()
        if not user:
            return jsonify({'error': 'User not found'}), 404
        
        if request.method == 'GET':
            # Return user's saved theme preference
            theme = user.settings.get('theme', 'dark')
            return jsonify({'theme': theme})
        
        elif request.method == 'POST':
            # Update user's theme preference
            data = request.get_json()
            theme = data.get('theme', 'dark')  # 'dark' or 'light'
            
            if theme not in ['dark', 'light']:
                return jsonify({'error': 'Invalid theme. Must be "dark" or "light"'}), 400
            
            from user_auth import update_user_settings
            success = update_user_settings(user.id, {'theme': theme})
            
            if success:
                return jsonify({'success': True, 'theme': theme})
            else:
                return jsonify({'error': 'Failed to update theme'}), 500
            
    except Exception as e:
        logger.error(f"Error with theme API: {e}")
        return jsonify({'error': 'Internal server error'}), 500

@app.route('/affiliate')
def affiliate():
    affiliate_code = ''
    referral_link = ''
    is_affiliate = False
    referral_count = 0
    referred_users = []
    affiliate_name = ''
    if USER_AUTH_AVAILABLE and is_logged_in():
        user = get_current_user()
        if user:
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                ph = '%s' if is_using_postgres() else '?'
                cursor.execute(f"SELECT affiliate_code, name FROM affiliate_applications WHERE email = {ph} AND status = 'approved'", (user.email,))
                row = cursor.fetchone()
                if row:
                    row_dict = dict(row)
                    affiliate_code = row_dict.get('affiliate_code') or ''
                    affiliate_name = row_dict.get('name') or ''
                    if affiliate_code:
                        is_affiliate = True
                        referral_link = f'https://www.justtrades.app/register?ref={affiliate_code}'
                        # Fetch dashboard data
                        cursor.execute(f"SELECT COUNT(*) FROM users WHERE referred_by = {ph}", (affiliate_code,))
                        count_row = cursor.fetchone()
                        referral_count = count_row[0] if count_row else 0
                        cursor.execute(f"SELECT username, display_name, created_at FROM users WHERE referred_by = {ph} ORDER BY created_at DESC", (affiliate_code,))
                        referred_users = []
                        for r in cursor.fetchall():
                            r_dict = dict(r)
                            referred_users.append({
                                'username': r_dict['username'],
                                'display_name': r_dict.get('display_name') or r_dict['username'],
                                'joined': str(r_dict.get('created_at') or '')
                            })
                cursor.close()
                conn.close()
            except Exception as e:
                logger.warning(f"Affiliate page data lookup: {e}")
    return render_template('affiliate.html',
                           affiliate_code=affiliate_code,
                           referral_link=referral_link,
                           is_affiliate=is_affiliate,
                           referral_count=referral_count,
                           referred_users=referred_users,
                           affiliate_name=affiliate_name)


# ============================================================
# AFFILIATE APPLICATION SYSTEM (Restored from backup branch)
# ============================================================

@app.route('/admin/affiliates')
def admin_affiliates_page():
    """Admin affiliate applications dashboard"""
    if USER_AUTH_AVAILABLE:
        user = get_current_user()
        if not user or not user.is_admin:
            return redirect('/login?next=/admin/affiliates')
    return render_template('admin_affiliates.html')


@app.route('/api/affiliate/apply', methods=['POST'])
def submit_affiliate_application():
    """Submit a new affiliate application (public endpoint)"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'No data provided'}), 400

        name = (data.get('name') or '').strip()
        email = (data.get('email') or '').strip()
        audience = (data.get('audience') or '').strip()

        if not name or not email or not audience:
            return jsonify({'error': 'Name, email, and audience description are required'}), 400

        website = (data.get('website') or '').strip() or None
        social = (data.get('social') or '').strip() or None
        experience = (data.get('experience') or '').strip() or None

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(f'SELECT id FROM affiliate_applications WHERE email = {placeholder}', (email,))
        existing = cursor.fetchone()
        if existing:
            conn.close()
            return jsonify({'error': 'An application with this email already exists'}), 409

        cursor.execute(f'''
            INSERT INTO affiliate_applications
            (name, email, website, social_handle, audience_description, trading_experience)
            VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
        ''', (name, email, website, social, audience, experience))

        conn.commit()
        conn.close()

        logger.info(f"New affiliate application from: {name} <{email}>")
        return jsonify({'success': True, 'message': 'Application submitted successfully'}), 201

    except Exception as e:
        logger.error(f"Error submitting affiliate application: {e}")
        return jsonify({'error': 'Failed to submit application'}), 500


@app.route('/api/affiliate/status', methods=['POST'])
def check_affiliate_status():
    """Check affiliate application status by email (public endpoint)"""
    try:
        data = request.get_json()
        email = data.get('email', '').strip().lower() if data else ''

        if not email:
            return jsonify({'error': 'Email is required'}), 400

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(f'''
            SELECT name, email, status, affiliate_code, created_at, reviewed_at
            FROM affiliate_applications
            WHERE LOWER(email) = {placeholder}
            ORDER BY created_at DESC LIMIT 1
        ''', (email,))

        row = cursor.fetchone()
        conn.close()

        if not row:
            return jsonify({'found': False, 'message': 'No application found with this email'}), 404

        if isinstance(row, dict):
            result = {
                'found': True, 'name': row['name'], 'email': row['email'],
                'status': row['status'],
                'affiliate_code': row['affiliate_code'] if row['status'] == 'approved' else None,
                'applied_at': row['created_at'], 'reviewed_at': row['reviewed_at']
            }
        else:
            result = {
                'found': True, 'name': row[0], 'email': row[1], 'status': row[2],
                'affiliate_code': row[3] if row[2] == 'approved' else None,
                'applied_at': row[4], 'reviewed_at': row[5]
            }

        return jsonify(result)

    except Exception as e:
        logger.error(f"Error checking affiliate status: {e}")
        return jsonify({'error': 'Failed to check status'}), 500


@app.route('/api/admin/affiliates', methods=['GET'])
def get_affiliate_applications():
    """Get all affiliate applications (admin only)"""
    try:
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403

        status_filter = request.args.get('status', None)

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        if status_filter:
            cursor.execute(f'''
                SELECT id, name, email, website, social_handle, audience_description,
                       trading_experience, status, affiliate_code, admin_notes, reviewed_at, created_at
                FROM affiliate_applications WHERE status = {placeholder}
                ORDER BY created_at DESC
            ''', (status_filter,))
        else:
            cursor.execute('''
                SELECT id, name, email, website, social_handle, audience_description,
                       trading_experience, status, affiliate_code, admin_notes, reviewed_at, created_at
                FROM affiliate_applications ORDER BY created_at DESC
            ''')

        rows = cursor.fetchall()

        # Get counts by status
        cursor.execute('''
            SELECT status, COUNT(*) as count FROM affiliate_applications GROUP BY status
        ''')
        status_rows = cursor.fetchall()
        conn.close()

        applications = []
        for row in rows:
            if isinstance(row, dict):
                applications.append(row)
            else:
                applications.append({
                    'id': row[0], 'name': row[1], 'email': row[2], 'website': row[3],
                    'social_handle': row[4], 'audience_description': row[5],
                    'trading_experience': row[6], 'status': row[7], 'affiliate_code': row[8],
                    'admin_notes': row[9], 'reviewed_at': row[10], 'created_at': row[11]
                })

        counts = {'pending': 0, 'approved': 0, 'rejected': 0}
        for row in status_rows:
            status = row['status'] if isinstance(row, dict) else row[0]
            count = row['count'] if isinstance(row, dict) else row[1]
            counts[status] = count

        return jsonify({'applications': applications, 'counts': counts})

    except Exception as e:
        logger.error(f"Error fetching affiliate applications: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/affiliates/<int:app_id>/status', methods=['POST'])
def update_affiliate_status(app_id):
    """Update affiliate application status (admin only)"""
    try:
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403

        data = request.get_json()
        new_status = data.get('status')
        admin_notes = data.get('notes', '')
        custom_code = data.get('custom_code', '').strip().upper() if data.get('custom_code') else None

        if new_status not in ['pending', 'approved', 'rejected']:
            return jsonify({'error': 'Invalid status'}), 400

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        affiliate_code = None

        if new_status == 'approved':
            cursor.execute(f'SELECT name, email, affiliate_code FROM affiliate_applications WHERE id = {placeholder}', (app_id,))
            app_row = cursor.fetchone()
            existing_code = app_row['affiliate_code'] if isinstance(app_row, dict) else app_row[2]

            if custom_code:
                cursor.execute(f'SELECT id FROM affiliate_applications WHERE affiliate_code = {placeholder} AND id != {placeholder}', (custom_code, app_id))
                if cursor.fetchone():
                    conn.close()
                    return jsonify({'error': f'Affiliate code "{custom_code}" is already in use'}), 409
                affiliate_code = custom_code
            elif not existing_code:
                import random
                import string
                name = (app_row['name'] if isinstance(app_row, dict) else app_row[0]) or 'USER'
                name_part = ''.join(c for c in name.upper() if c.isalpha())[:3].ljust(3, 'X')
                random_part = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))
                affiliate_code = f"JT-{name_part}{random_part}"

                for _ in range(10):
                    cursor.execute(f'SELECT id FROM affiliate_applications WHERE affiliate_code = {placeholder}', (affiliate_code,))
                    if not cursor.fetchone():
                        break
                    random_part = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))
                    affiliate_code = f"JT-{name_part}{random_part}"

                cursor.execute(f'''
                    UPDATE affiliate_applications
                    SET status = {placeholder}, affiliate_code = {placeholder}, admin_notes = {placeholder},
                        reviewed_by = {placeholder}, reviewed_at = CURRENT_TIMESTAMP
                    WHERE id = {placeholder}
                ''', (new_status, affiliate_code, admin_notes, user.id, app_id))
            else:
                affiliate_code = existing_code
                cursor.execute(f'''
                    UPDATE affiliate_applications
                    SET status = {placeholder}, admin_notes = {placeholder}, reviewed_by = {placeholder},
                        reviewed_at = CURRENT_TIMESTAMP
                    WHERE id = {placeholder}
                ''', (new_status, admin_notes, user.id, app_id))
        else:
            cursor.execute(f'''
                UPDATE affiliate_applications
                SET status = {placeholder}, admin_notes = {placeholder}, reviewed_by = {placeholder},
                    reviewed_at = CURRENT_TIMESTAMP
                WHERE id = {placeholder}
            ''', (new_status, admin_notes, user.id, app_id))

        conn.commit()
        conn.close()

        logger.info(f"Affiliate application {app_id} marked as {new_status} by admin {user.username}" +
                    (f" - Code: {affiliate_code}" if affiliate_code else ""))
        return jsonify({'success': True, 'affiliate_code': affiliate_code})

    except Exception as e:
        logger.error(f"Error updating affiliate status: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/affiliates/<int:app_id>/approve', methods=['POST'])
def approve_affiliate(app_id):
    """Approve an affiliate application (admin only) ‚Äî called by admin_affiliates.html"""
    try:
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403

        data = request.get_json() or {}
        custom_code = (data.get('affiliate_code') or '').strip().upper()
        admin_notes = (data.get('admin_notes') or '').strip()

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        # Generate code if not provided
        if not custom_code:
            import random
            import string
            cursor.execute(f'SELECT name FROM affiliate_applications WHERE id = {placeholder}', (app_id,))
            name_row = cursor.fetchone()
            name = (name_row['name'] if isinstance(name_row, dict) else name_row[0]) if name_row else 'USER'
            name_part = ''.join(c for c in name.upper() if c.isalpha())[:3].ljust(3, 'X')
            random_part = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))
            custom_code = f"JT-{name_part}{random_part}"
            for _ in range(10):
                cursor.execute(f'SELECT id FROM affiliate_applications WHERE affiliate_code = {placeholder}', (custom_code,))
                if not cursor.fetchone():
                    break
                random_part = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))
                custom_code = f"JT-{name_part}{random_part}"
        else:
            # Check code uniqueness
            cursor.execute(f'SELECT id FROM affiliate_applications WHERE affiliate_code = {placeholder} AND id != {placeholder}', (custom_code, app_id))
            if cursor.fetchone():
                conn.close()
                return jsonify({'error': 'This affiliate code is already in use'}), 409

        cursor.execute(f'''
            UPDATE affiliate_applications
            SET status = 'approved', affiliate_code = {placeholder}, admin_notes = {placeholder},
                reviewed_by = {placeholder}, reviewed_at = CURRENT_TIMESTAMP
            WHERE id = {placeholder}
        ''', (custom_code, admin_notes, user.id, app_id))
        conn.commit()
        conn.close()

        logger.info(f"Affiliate application {app_id} approved with code {custom_code} by {user.username}")
        return jsonify({'success': True, 'affiliate_code': custom_code})

    except Exception as e:
        logger.error(f"Error approving affiliate: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/affiliates/<int:app_id>/reject', methods=['POST'])
def reject_affiliate(app_id):
    """Reject an affiliate application (admin only) ‚Äî called by admin_affiliates.html"""
    try:
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403

        data = request.get_json() or {}
        admin_notes = (data.get('admin_notes') or '').strip()

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(f'''
            UPDATE affiliate_applications
            SET status = 'rejected', admin_notes = {placeholder}, reviewed_by = {placeholder},
                reviewed_at = CURRENT_TIMESTAMP
            WHERE id = {placeholder}
        ''', (admin_notes, user.id, app_id))
        conn.commit()
        conn.close()

        logger.info(f"Affiliate application {app_id} rejected by {user.username}")
        return jsonify({'success': True})

    except Exception as e:
        logger.error(f"Error rejecting affiliate: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/affiliates/<int:app_id>/code', methods=['POST'])
def update_affiliate_code(app_id):
    """Update affiliate code for an application (admin only)"""
    try:
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403

        data = request.get_json()
        new_code = data.get('affiliate_code', '').strip().upper() if data else ''

        if not new_code:
            return jsonify({'error': 'Affiliate code is required'}), 400

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(f'SELECT id FROM affiliate_applications WHERE affiliate_code = {placeholder} AND id != {placeholder}', (new_code, app_id))
        if cursor.fetchone():
            conn.close()
            return jsonify({'error': f'Affiliate code "{new_code}" is already in use'}), 409

        cursor.execute(f'UPDATE affiliate_applications SET affiliate_code = {placeholder} WHERE id = {placeholder}', (new_code, app_id))
        conn.commit()
        conn.close()

        logger.info(f"Affiliate code updated for application {app_id} to: {new_code} by admin {user.username}")
        return jsonify({'success': True, 'affiliate_code': new_code})

    except Exception as e:
        logger.error(f"Error updating affiliate code: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/admin/affiliates/count')
def get_affiliate_count():
    """Get count of pending affiliate applications (for nav badge)"""
    try:
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'count': 0})

        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) as cnt FROM affiliate_applications WHERE status = 'pending'")
        result = cursor.fetchone()
        count = result['cnt'] if isinstance(result, dict) else result[0]
        conn.close()

        return jsonify({'count': count})
    except Exception:
        return jsonify({'count': 0})


@app.route('/affiliate/dashboard')
def affiliate_dashboard():
    """Affiliate dashboard page ‚Äî shows stats for the logged-in affiliate."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return redirect(url_for('login'))

    user = get_current_user()
    if not user:
        return redirect(url_for('login'))

    return render_template('affiliate_dashboard.html')


@app.route('/api/affiliate/dashboard', methods=['GET'])
def api_affiliate_dashboard():
    """API for affiliate dashboard data."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify({'error': 'Not logged in'}), 401

    user = get_current_user()
    if not user:
        return jsonify({'error': 'Not logged in'}), 401

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        # Look up the user's affiliate application by email
        cursor.execute(f"SELECT * FROM affiliate_applications WHERE email = {placeholder} AND status = 'approved'", (user.email,))
        row = cursor.fetchone()

        if not row:
            cursor.close()
            conn.close()
            return jsonify({'error': 'No approved affiliate application found for your account'}), 404

        app_data = dict(row)
        code = app_data.get('affiliate_code', '')

        # Get referral count
        cursor.execute(f"SELECT COUNT(*) FROM users WHERE referred_by = {placeholder}", (code,))
        count_row = cursor.fetchone()
        referral_count = count_row[0] if count_row else 0

        # Get referred users list (username + join date only)
        cursor.execute(f"SELECT username, display_name, created_at FROM users WHERE referred_by = {placeholder} ORDER BY created_at DESC", (code,))
        referred_users = []
        for r in cursor.fetchall():
            r_dict = dict(r)
            referred_users.append({
                'username': r_dict['username'],
                'display_name': r_dict.get('display_name', r_dict['username']),
                'joined': str(r_dict.get('created_at', ''))
            })

        cursor.close()
        conn.close()

        return jsonify({
            'affiliate_code': code,
            'referral_link': f"https://www.justtrades.app/register?ref={code}",
            'pricing_link': f"https://www.justtrades.app/pricing?ref={code}",
            'referral_count': referral_count,
            'referred_users': referred_users,
            'name': app_data['name'],
            'status': app_data['status'],
            'whop_link_platform_basic': app_data.get('whop_link_platform_basic') or '',
            'whop_link_platform_premium': app_data.get('whop_link_platform_premium') or '',
            'whop_link_platform_elite': app_data.get('whop_link_platform_elite') or '',
            'whop_link_discord_basic': app_data.get('whop_link_discord_basic') or '',
            'whop_link_discord_premium': app_data.get('whop_link_discord_premium') or '',
        })

    except Exception as e:
        logger.error(f"Affiliate dashboard API error: {e}")
        return jsonify({'error': 'Failed to load dashboard data'}), 500


@app.route('/api/affiliate/whop-links', methods=['POST'])
def api_affiliate_whop_links_save():
    """Save affiliate's custom Whop checkout links."""
    if not USER_AUTH_AVAILABLE or not is_logged_in():
        return jsonify({'error': 'Not logged in'}), 401

    user = get_current_user()
    if not user:
        return jsonify({'error': 'Not logged in'}), 401

    data = request.get_json() or {}

    # Validate links
    WHOP_LINK_FIELDS = [
        'whop_link_platform_basic', 'whop_link_platform_premium', 'whop_link_platform_elite',
        'whop_link_discord_basic', 'whop_link_discord_premium'
    ]
    sanitized = {}
    for field in WHOP_LINK_FIELDS:
        val = (data.get(field) or '').strip()
        if val and not val.startswith('https://whop.com/'):
            return jsonify({'error': f'Invalid link for {field}. Must start with https://whop.com/'}), 400
        sanitized[field] = val if val else None

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        # Verify user has approved affiliate application
        cursor.execute(f"SELECT id FROM affiliate_applications WHERE email = {placeholder} AND status = 'approved'", (user.email,))
        row = cursor.fetchone()
        if not row:
            cursor.close()
            conn.close()
            return jsonify({'error': 'No approved affiliate application found'}), 404

        app_id = dict(row)['id']

        # Update all 5 link columns
        set_clauses = ', '.join(f"{f} = {placeholder}" for f in WHOP_LINK_FIELDS)
        values = [sanitized[f] for f in WHOP_LINK_FIELDS]
        values.append(app_id)
        cursor.execute(f"UPDATE affiliate_applications SET {set_clauses} WHERE id = {placeholder}", values)
        conn.commit()
        cursor.close()
        conn.close()

        return jsonify({'success': True, 'message': 'Whop links saved'})

    except Exception as e:
        logger.error(f"Save whop links error: {e}")
        return jsonify({'error': 'Failed to save links'}), 500


@app.route('/api/affiliate/whop-links-public', methods=['GET'])
def api_affiliate_whop_links_public():
    """Public endpoint ‚Äî returns affiliate's Whop links by ref code."""
    ref_code = request.args.get('ref', '').strip()
    if not ref_code:
        return jsonify({}), 200

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'

        cursor.execute(
            f"SELECT whop_link_platform_basic, whop_link_platform_premium, whop_link_platform_elite, "
            f"whop_link_discord_basic, whop_link_discord_premium "
            f"FROM affiliate_applications WHERE affiliate_code = {placeholder} AND status = 'approved'",
            (ref_code,)
        )
        row = cursor.fetchone()
        cursor.close()
        conn.close()

        if not row:
            return jsonify({}), 200

        row_dict = dict(row)
        # Only return non-null links
        links = {k: v for k, v in row_dict.items() if v}
        return jsonify(links)

    except Exception as e:
        logger.error(f"Public whop links error: {e}")
        return jsonify({}), 200


# API Endpoints for Dashboard Filters
@app.route('/api/dashboard/users', methods=['GET'])
def api_dashboard_users():
    """Get list of users for filter dropdown - ADMIN ONLY"""
    # Security: require admin login
    if USER_AUTH_AVAILABLE:
        if not is_logged_in():
            return jsonify({'error': 'Authentication required', 'users': []}), 401
        user = get_current_user()
        if not user or not getattr(user, 'is_admin', False):
            return jsonify({'error': 'Admin access required', 'users': []}), 403
    try:
        try:
            from app.database import SessionLocal
            from app.models import User

            db = SessionLocal()
            users = db.query(User).order_by(User.username).all()
            db.close()

            return jsonify({
                'users': [{'id': u.id, 'username': u.username} for u in users],
                'current_user_id': getattr(user, 'id', None) if USER_AUTH_AVAILABLE and user else None
            })
        except ImportError:
            # Database modules not available, return empty list
            return jsonify({'error': 'Database not configured', 'users': []}), 200
    except Exception as e:
        logger.error(f"Error fetching users: {e}")
        return jsonify({'error': 'Failed to fetch users', 'users': []}), 500

@app.route('/api/dashboard/strategies', methods=['GET'])
def api_dashboard_strategies():
    """Get list of strategies (recorders) for filter dropdown"""
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get all recorders as strategies with trade counts
        cursor.execute('''
            SELECT 
                r.id, 
                r.name, 
                r.symbol, 
                r.recording_enabled,
                COUNT(rt.id) as trade_count,
                SUM(CASE WHEN rt.status = 'closed' THEN rt.pnl ELSE 0 END) as total_pnl
            FROM recorders r
            LEFT JOIN recorded_trades rt ON r.id = rt.recorder_id
            GROUP BY r.id
            ORDER BY r.name
        ''')
        
        recorders = [dict(row) for row in cursor.fetchall()]
        
        # Get unique symbols from recorded trades
        cursor.execute('''
            SELECT DISTINCT ticker FROM recorded_trades WHERE ticker IS NOT NULL
            UNION
            SELECT DISTINCT symbol FROM recorders WHERE symbol IS NOT NULL AND symbol != ''
            ORDER BY 1
        ''')
        symbols = [row['ticker'] or row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return jsonify({
            'strategies': [{
                'id': r['id'],
                'name': r['name'],
                'symbol': r['symbol'],
                'recording_enabled': bool(r['recording_enabled']),
                'trade_count': r['trade_count'] or 0,
                'total_pnl': r['total_pnl'] or 0
            } for r in recorders],
            'symbols': symbols
        })
    except Exception as e:
        logger.error(f"Error fetching strategies: {e}")
        return jsonify({'error': 'Failed to fetch strategies', 'strategies': [], 'symbols': []}), 500

@app.route('/api/dashboard/chart-data', methods=['GET'])
def api_dashboard_chart_data():
    """Get chart data (profit vs drawdown) from recorded trades"""
    try:
        # Get filter parameters
        strategy_id = request.args.get('strategy_id')  # This is recorder_id
        symbol = request.args.get('symbol')
        timeframe = request.args.get('timeframe', 'month')
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        
        # Build date filter - PostgreSQL vs SQLite syntax
        date_filter = ''
        if is_postgres:
            if timeframe == 'today':
                date_filter = "AND DATE(rt.exit_time) = CURRENT_DATE"
            elif timeframe == 'week':
                date_filter = "AND rt.exit_time >= CURRENT_DATE - INTERVAL '7 days'"
            elif timeframe == 'month':
                date_filter = "AND rt.exit_time >= CURRENT_DATE - INTERVAL '30 days'"
            elif timeframe == '3months':
                date_filter = "AND rt.exit_time >= CURRENT_DATE - INTERVAL '90 days'"
            elif timeframe == '6months':
                date_filter = "AND rt.exit_time >= CURRENT_DATE - INTERVAL '180 days'"
            elif timeframe == 'year':
                date_filter = "AND rt.exit_time >= CURRENT_DATE - INTERVAL '365 days'"
        else:
            if timeframe == 'today':
                date_filter = "AND DATE(rt.exit_time) = DATE('now')"
            elif timeframe == 'week':
                date_filter = "AND rt.exit_time >= DATE('now', '-7 days')"
            elif timeframe == 'month':
                date_filter = "AND rt.exit_time >= DATE('now', '-30 days')"
            elif timeframe == '3months':
                date_filter = "AND rt.exit_time >= DATE('now', '-90 days')"
            elif timeframe == '6months':
                date_filter = "AND rt.exit_time >= DATE('now', '-180 days')"
            elif timeframe == 'year':
                date_filter = "AND rt.exit_time >= DATE('now', '-365 days')"
        
        # Build recorder filter
        recorder_filter = ''
        params = []
        if strategy_id:
            recorder_filter = f'AND rt.recorder_id = {ph}'
            params.append(int(strategy_id))
        
        # Build symbol filter
        symbol_filter = ''
        if symbol:
            symbol_filter = f'AND rt.ticker = {ph}'
            params.append(symbol)
        
        # Get daily aggregate PnL
        cursor.execute(f'''
            SELECT 
                DATE(rt.exit_time) as date,
                SUM(rt.pnl) as daily_pnl,
                SUM(CASE WHEN rt.pnl < 0 THEN ABS(rt.pnl) ELSE 0 END) as daily_loss,
                MAX(ABS(CASE WHEN rt.pnl < 0 THEN rt.pnl ELSE 0 END)) as max_single_loss
            FROM recorded_trades rt
            WHERE rt.status = 'closed' {date_filter} {recorder_filter} {symbol_filter}
            GROUP BY DATE(rt.exit_time)
            ORDER BY DATE(rt.exit_time) ASC
        ''', params if params else ())
        
        # Convert rows to dicts
        columns = [desc[0] for desc in cursor.description]
        daily_data = [dict(zip(columns, row)) for row in cursor.fetchall()]
        conn.close()
        
        # Calculate cumulative profit and drawdown
        labels = []
        cumulative_profit = []
        drawdown_per_day = []
        running_profit = 0
        max_profit = 0
        
        for day in daily_data:
            # Format date label
            try:
                date_obj = datetime.strptime(day['date'], '%Y-%m-%d')
                labels.append(date_obj.strftime('%b %d'))
            except:
                labels.append(day['date'])
            
            running_profit += day['daily_pnl'] or 0
            cumulative_profit.append(round(running_profit, 2))
            
            # Track max profit for drawdown calculation
            if running_profit > max_profit:
                max_profit = running_profit
            
            # Drawdown from peak
            current_drawdown = max_profit - running_profit
            drawdown_per_day.append(round(current_drawdown, 2))
        
        # If no data, return empty arrays
        if not labels:
            return jsonify({
                'labels': [],
                'profit': [],
                'drawdown': []
            })
        
        return jsonify({
            'labels': labels,
            'profit': cumulative_profit,
            'drawdown': drawdown_per_day
        })
    except Exception as e:
        logger.error(f"Error fetching chart data: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to fetch chart data', 'labels': [], 'profit': [], 'drawdown': []}), 500

@app.route('/api/dashboard/trade-history', methods=['GET'])
def api_dashboard_trade_history():
    """Get trade history from recorded_trades - shows ALL trades (open and closed)"""
    try:
        # Get filter parameters
        strategy_id = request.args.get('strategy_id')  # This is recorder_id
        symbol = request.args.get('symbol')
        timeframe = request.args.get('timeframe', 'all')
        status_filter = request.args.get('status', 'all')  # 'all', 'open', 'closed'
        page = int(request.args.get('page', 1))
        per_page = 20
        
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        
        trades = []
        params = []
        where_clauses = ['1=1']  # Always true base

        # Status filter
        if status_filter == 'open':
            where_clauses.append("rt.status = 'open'")
        elif status_filter == 'closed':
            where_clauses.append("rt.status = 'closed'")
        # 'all' shows everything
            
        if strategy_id:
            where_clauses.append(f'rt.recorder_id = {ph}')
            params.append(int(strategy_id))
            
        if symbol:
            where_clauses.append(f'rt.ticker = {ph}')
            params.append(symbol)
            
        # Timeframe filter - PostgreSQL vs SQLite syntax
        if is_postgres:
            if timeframe == 'today':
                where_clauses.append("DATE(COALESCE(rt.exit_time, rt.created_at)) = CURRENT_DATE")
            elif timeframe == 'week':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= CURRENT_DATE - INTERVAL '7 days'")
            elif timeframe == 'month':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= CURRENT_DATE - INTERVAL '30 days'")
            elif timeframe == '3months':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= CURRENT_DATE - INTERVAL '90 days'")
            elif timeframe == '6months':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= CURRENT_DATE - INTERVAL '180 days'")
            elif timeframe == 'year':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= CURRENT_DATE - INTERVAL '365 days'")
        else:
            if timeframe == 'today':
                where_clauses.append("DATE(COALESCE(rt.exit_time, rt.created_at)) = DATE('now')")
            elif timeframe == 'week':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= DATE('now', '-7 days')")
            elif timeframe == 'month':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= DATE('now', '-30 days')")
            elif timeframe == '3months':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= DATE('now', '-90 days')")
            elif timeframe == '6months':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= DATE('now', '-180 days')")
            elif timeframe == 'year':
                where_clauses.append("COALESCE(rt.exit_time, rt.created_at) >= DATE('now', '-365 days')")
        
        # Build WHERE clause (FIXED: moved outside if/else block)
        where_sql = ' AND '.join(where_clauses)
        
        # Get total count
        cursor.execute(f'''
            SELECT COUNT(*) FROM recorded_trades rt WHERE {where_sql}
        ''', params if params else ())
        total_count = cursor.fetchone()[0]
        
        # Get paginated trades with recorder name
        offset = (page - 1) * per_page
        limit_clause = f'LIMIT {ph} OFFSET {ph}'
        
        cursor.execute(f'''
            SELECT 
                rt.id,
                rt.ticker,
                rt.side,
                rt.action,
                rt.entry_price,
                rt.exit_price,
                rt.quantity,
                rt.pnl,
                rt.status,
                rt.tp_price,
                rt.sl_price,
                rt.max_adverse,
                rt.max_favorable,
                rt.created_at,
                rt.exit_time,
                rt.exit_reason,
                r.name as strategy_name
            FROM recorded_trades rt
            LEFT JOIN recorders r ON rt.recorder_id = r.id
            WHERE {where_sql}
            ORDER BY rt.created_at DESC
            {limit_clause}
        ''', (params + [per_page, offset]) if params else [per_page, offset])
        
        columns = [desc[0] for desc in cursor.description]
        for row in cursor.fetchall():
            trade = dict(zip(columns, row))
            
            # Determine win/loss status
            if trade['status'] == 'closed':
                pnl = trade.get('pnl') or 0
                trade_status = 'WIN' if pnl > 0 else ('LOSS' if pnl < 0 else 'FLAT')
            else:
                trade_status = 'OPEN'
            
            trades.append({
                'id': trade.get('id'),
                'open_time': trade.get('created_at'),
                'closed_time': trade.get('exit_time'),
                'strategy': trade.get('strategy_name') or 'N/A',
                'symbol': trade.get('ticker'),
                'side': trade.get('side'),
                'size': trade.get('quantity', 1),
                'entry_price': trade.get('entry_price'),
                'exit_price': trade.get('exit_price'),
                'profit': trade.get('pnl') or 0,
                'tp_price': trade.get('tp_price'),
                'sl_price': trade.get('sl_price'),
                'drawdown': abs(trade.get('max_adverse') or 0),
                'max_favorable': trade.get('max_favorable') or 0,
                'status': trade_status,
                'exit_reason': trade.get('exit_reason')
            })
        
        conn.close()
        
        # Calculate pagination info
        total_pages = (total_count + per_page - 1) // per_page if total_count > 0 else 1
        
        return jsonify({
            'trades': trades,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total_count,
                'total_pages': total_pages,
                'has_next': page < total_pages,
                'has_prev': page > 1
            }
        })
    except Exception as e:
        logger.error(f"Error fetching trade history: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to fetch trade history', 'trades': []}), 500

@app.route('/api/dashboard/pnl-calendar', methods=['GET'])
def api_pnl_calendar():
    """Get P&L data for calendar view from recorded_trades"""
    try:
        start_date = request.args.get('start_date', (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d'))
        end_date = request.args.get('end_date', datetime.now().strftime('%Y-%m-%d'))

        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()

        # Query recorded_trades for daily PnL instead of strategy_pnl_history
        if is_postgres:
            cursor.execute('''
                SELECT DATE(exit_time) as date, SUM(pnl) as daily_pnl, COUNT(*) as trade_count
                FROM recorded_trades
                WHERE status = 'closed' AND DATE(exit_time) BETWEEN %s AND %s
                GROUP BY DATE(exit_time)
                ORDER BY date
            ''', (start_date, end_date))
        else:
            cursor.execute('''
                SELECT DATE(exit_time) as date, SUM(pnl) as daily_pnl, COUNT(*) as trade_count
                FROM recorded_trades
                WHERE status = 'closed' AND DATE(exit_time) BETWEEN ? AND ?
                GROUP BY DATE(exit_time)
                ORDER BY date
            ''', (start_date, end_date))

        data = [
            {
                'date': str(row[0]),
                'pnl': float(row[1]) if row[1] else 0.0,
                'trade_count': int(row[2]) if row[2] else 0
            }
            for row in cursor.fetchall()
        ]
        conn.close()

        return jsonify({'calendar_data': data})
    except Exception as e:
        logger.error(f"Error fetching P&L calendar: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'calendar_data': []})

@app.route('/api/dashboard/pnl-drawdown-chart', methods=['GET'])
def api_pnl_drawdown_chart():
    """Get P&L and drawdown data for chart from recorded_trades"""
    try:
        strategy_id = request.args.get('strategy_id', None)
        limit = int(request.args.get('limit', 1000))
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'

        conn = get_db_connection()
        cursor = conn.cursor()

        # Build query to get closed trades ordered by exit time
        where_clauses = ["status = 'closed'", "exit_time IS NOT NULL"]
        params = []

        if strategy_id:
            where_clauses.append(f'recorder_id = {ph}')
            params.append(int(strategy_id))

        where_sql = ' AND '.join(where_clauses)

        cursor.execute(f'''
            SELECT exit_time, pnl
            FROM recorded_trades
            WHERE {where_sql}
            ORDER BY exit_time ASC
            LIMIT {ph}
        ''', params + [limit])

        rows = cursor.fetchall()
        conn.close()

        # Calculate cumulative PnL and drawdown
        chart_data = []
        cumulative_pnl = 0
        peak_pnl = 0

        for row in rows:
            exit_time = row[0]
            pnl = float(row[1]) if row[1] else 0.0

            cumulative_pnl += pnl
            peak_pnl = max(peak_pnl, cumulative_pnl)
            drawdown = peak_pnl - cumulative_pnl  # Drawdown from peak

            chart_data.append({
                'timestamp': str(exit_time),
                'pnl': cumulative_pnl,
                'drawdown': drawdown
            })

        return jsonify({'chart_data': chart_data})
    except Exception as e:
        logger.error(f"Error fetching P&L chart: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'chart_data': []})

@app.route('/api/dashboard/metrics', methods=['GET'])
def api_dashboard_metrics():
    """Get metric cards data from recorded trades"""
    try:
        # Get filter parameters
        strategy_id = request.args.get('strategy_id')  # This is recorder_id
        symbol = request.args.get('symbol')
        timeframe = request.args.get('timeframe', 'all')
        
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Build filters - use correct placeholder for DB type
        ph = '%s' if is_postgres else '?'
        where_clauses = ["status = 'closed'"]
        params = []
        
        if strategy_id:
            where_clauses.append(f'recorder_id = {ph}')
            params.append(int(strategy_id))
        
        if symbol:
            where_clauses.append(f'ticker = {ph}')
            params.append(symbol)
        
        # Timeframe filter - PostgreSQL compatible
        if timeframe == 'today':
            if is_postgres:
                where_clauses.append("DATE(exit_time) = CURRENT_DATE")
            else:
                where_clauses.append("DATE(exit_time) = DATE('now')")
        elif timeframe == 'week':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '7 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-7 days')")
        elif timeframe == 'month':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '30 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-30 days')")
        elif timeframe == '3months':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '90 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-90 days')")
        elif timeframe == '6months':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '180 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-180 days')")
        elif timeframe == 'year':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '365 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-365 days')")
        
        where_sql = ' AND '.join(where_clauses)
        
        # Get aggregate metrics
        cursor.execute(f'''
            SELECT 
                COUNT(*) as total_trades,
                SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as wins,
                SUM(CASE WHEN pnl < 0 THEN 1 ELSE 0 END) as losses,
                SUM(pnl) as total_pnl,
                SUM(CASE WHEN pnl > 0 THEN pnl ELSE 0 END) as total_wins,
                SUM(CASE WHEN pnl < 0 THEN ABS(pnl) ELSE 0 END) as total_losses,
                MAX(pnl) as max_profit,
                MIN(pnl) as max_loss,
                AVG(CASE WHEN pnl > 0 THEN pnl END) as avg_win,
                AVG(CASE WHEN pnl < 0 THEN ABS(pnl) END) as avg_loss,
                MAX(quantity) as max_quantity,
                AVG(quantity) as avg_quantity,
                MIN(DATE(entry_time)) as first_trade,
                MAX(DATE(exit_time)) as last_trade
            FROM recorded_trades
            WHERE {where_sql}
        ''', params)
        
        row = cursor.fetchone()
        stats = dict(row) if row else {}
        conn.close()
        
        # Calculate derived metrics
        total_trades = stats.get('total_trades') or 0
        wins = stats.get('wins') or 0
        losses = stats.get('losses') or 0
        total_wins_amt = stats.get('total_wins') or 0
        total_losses_amt = stats.get('total_losses') or 0

        win_rate_pct = round((wins / total_trades * 100), 1) if total_trades > 0 else 0
        if total_losses_amt > 0:
            profit_factor = round(total_wins_amt / total_losses_amt, 2)
        elif total_wins_amt > 0:
            profit_factor = 'Infinite'
        else:
            profit_factor = 0
        
        # Calculate time traded
        time_traded = '0D'
        if stats.get('first_trade') and stats.get('last_trade'):
            try:
                first = datetime.strptime(stats['first_trade'], '%Y-%m-%d')
                last = datetime.strptime(stats['last_trade'], '%Y-%m-%d')
                delta = (last - first).days
                months = delta // 30
                days = delta % 30
                if months > 0:
                    time_traded = f"{months}M {days}D"
                else:
                    time_traded = f"{days}D"
            except:
                time_traded = '0D'
        
        return jsonify({
            'metrics': {
                'cumulative_return': {
                    'return': stats.get('total_pnl') or 0,
                    'time_traded': time_traded
                },
                'win_rate': {
                    'wins': wins,
                    'losses': losses,
                    'percentage': win_rate_pct
                },
                'drawdown': {
                    'max': abs(stats.get('max_loss') or 0),
                    'avg': stats.get('avg_loss') or 0,
                    'run': abs(stats.get('max_loss') or 0)
                },
                'total_roi': 0,  # Would need initial capital
                'contracts_held': {
                    'max': stats.get('max_quantity') or 0,
                    'avg': round(stats.get('avg_quantity') or 0)
                },
                'pnl': {
                    'max_profit': stats.get('max_profit') or 0,
                    'avg_profit': stats.get('avg_win') or 0,
                    'max_loss': abs(stats.get('max_loss') or 0),
                    'avg_loss': stats.get('avg_loss') or 0
                },
                'profit_factor': profit_factor
            }
        })
    except Exception as e:
        logger.error(f"Error fetching metrics: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': 'Failed to fetch metrics', 'metrics': {}}), 500

def calculate_time_traded_legacy(positions):
    """Calculate time traded string like '1M 1D' - Legacy version"""
    if not positions:
        return '0D'
    
    dates = [p.entry_timestamp.date() for p in positions if p.entry_timestamp]
    if not dates:
        return '0D'
    
    min_date = min(dates)
    max_date = max(dates)
    delta = max_date - min_date
    
    months = delta.days // 30
    days = delta.days % 30
    
    if months > 0 and days > 0:
        return f'{months}M {days}D'
    elif months > 0:
        return f'{months}M'
    else:
        return f'{days}D'

@app.route('/api/dashboard/summary/', methods=['GET'])
def api_dashboard_summary():
    """Get dashboard summary stats from recorded trades and positions"""
    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        # Get total strategies (recorders)
        cursor.execute('SELECT COUNT(*) FROM recorders')
        total_strategies = cursor.fetchone()[0] or 0

        # Get active positions from recorder_positions
        cursor.execute("SELECT COUNT(*) FROM recorder_positions WHERE status = 'open'")
        active_positions = cursor.fetchone()[0] or 0

        # Get total PnL from closed trades
        cursor.execute("SELECT COALESCE(SUM(pnl), 0) FROM recorded_trades WHERE status = 'closed'")
        total_pnl = cursor.fetchone()[0] or 0

        # Get today's PnL - PostgreSQL vs SQLite syntax
        if is_postgres:
            cursor.execute("""
                SELECT COALESCE(SUM(pnl), 0) FROM recorded_trades
                WHERE status = 'closed' AND DATE(exit_time) = CURRENT_DATE
            """)
        else:
            cursor.execute("""
                SELECT COALESCE(SUM(pnl), 0) FROM recorded_trades
                WHERE status = 'closed' AND DATE(exit_time) = DATE('now')
            """)
        today_pnl = cursor.fetchone()[0] or 0

        conn.close()

        return jsonify({
            'total_strategies': total_strategies,
            'active_positions': active_positions,
            'total_pnl': float(total_pnl),
            'today_pnl': float(today_pnl)
        })
    except Exception as e:
        logger.error(f"Error fetching dashboard summary: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'total_strategies': 0,
            'active_positions': 0,
            'total_pnl': 0,
            'today_pnl': 0,
            'error': str(e)
        }), 500


@app.route('/api/trades/', methods=['GET'])
@api_login_required
def api_trades_list():
    """Alias for /api/dashboard/trade-history for frontend compatibility"""
    return api_dashboard_trade_history()


@app.route('/api/dashboard/calendar-data', methods=['GET'])
def api_dashboard_calendar_data():
    """Get daily PnL data for calendar view from recorded trades"""
    try:
        # Get filter parameters
        strategy_id = request.args.get('strategy_id')  # This is recorder_id
        symbol = request.args.get('symbol')
        timeframe = request.args.get('timeframe', 'month')
        
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Build filters - use correct placeholder for DB type
        ph = '%s' if is_postgres else '?'
        where_clauses = ["status = 'closed'"]
        params = []
        
        if strategy_id:
            where_clauses.append(f'recorder_id = {ph}')
            params.append(int(strategy_id))
        
        if symbol:
            where_clauses.append(f'ticker = {ph}')
            params.append(symbol)
        
        # Timeframe filter - PostgreSQL compatible
        if timeframe == 'today':
            if is_postgres:
                where_clauses.append("DATE(exit_time) = CURRENT_DATE")
            else:
                where_clauses.append("DATE(exit_time) = DATE('now')")
        elif timeframe == 'week':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '7 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-7 days')")
        elif timeframe == 'month':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '30 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-30 days')")
        elif timeframe == '3months':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '90 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-90 days')")
        elif timeframe == '6months':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '180 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-180 days')")
        elif timeframe == 'year':
            if is_postgres:
                where_clauses.append("exit_time >= CURRENT_DATE - INTERVAL '365 days'")
            else:
                where_clauses.append("exit_time >= DATE('now', '-365 days')")
        
        where_sql = ' AND '.join(where_clauses)
        
        # Get daily PnL
        cursor.execute(f'''
            SELECT 
                DATE(exit_time) as date,
                SUM(pnl) as daily_pnl,
                COUNT(*) as trade_count
            FROM recorded_trades
            WHERE {where_sql}
            GROUP BY DATE(exit_time)
            ORDER BY DATE(exit_time) ASC
        ''', params)
        
        rows = cursor.fetchall()
        conn.close()
        
        # Format for frontend (date string -> {pnl, trades})
        calendar_data = {}
        for row in rows:
            date_str = str(row['date'])
            calendar_data[date_str] = {
                'pnl': round(row['daily_pnl'] or 0, 2),
                'trades': row['trade_count'] or 0
            }
        
        return jsonify({'calendar_data': calendar_data})
    except Exception as e:
        logger.error(f"Error fetching calendar data: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'Failed to fetch calendar data: {str(e)}', 'calendar_data': {}}), 500

# ============================================================================
# TradingView Backtest Import API (XLSX + CSV)
# ============================================================================

def _parse_tv_number(val):
    """Strip $, commas, % from a string and return float (or 0.0). Handles native numerics from openpyxl."""
    if val is None:
        return 0.0
    if isinstance(val, (int, float)):
        return float(val)
    val = str(val).strip().replace('$', '').replace(',', '').replace('%', '')
    try:
        return float(val)
    except (ValueError, TypeError):
        return 0.0


def _normalize_header(h):
    """Lowercase + strip whitespace for flexible header matching."""
    return h.strip().lower().replace('.', '').replace('-', '').replace('_', ' ')


# Column mapping: normalised header -> internal key (covers both CSV and XLSX exports)
_TV_HEADER_MAP = {
    'trade #': 'trade_num', 'trade': 'trade_num', 'trade num': 'trade_num',
    'type': 'type',
    'signal': 'signal',
    'date/time': 'date_time', 'datetime': 'date_time', 'date time': 'date_time',
    'date and time': 'date_time', 'date': 'date_time',
    'price': 'price', 'price usd': 'price',
    'contracts': 'contracts', 'qty': 'contracts', 'quantity': 'contracts',
    'position size (qty)': 'contracts', 'position size qty': 'contracts',
    'profit': 'profit', 'profit usd': 'profit',
    'net p&l usd': 'profit', 'net p&l': 'profit', 'net pnl usd': 'profit',
    'cum profit': 'cumulative_profit', 'cumprofit': 'cumulative_profit',
    'cumulative profit': 'cumulative_profit', 'cum profit usd': 'cumulative_profit',
    'cumulative p&l usd': 'cumulative_profit', 'cumulative p&l': 'cumulative_profit',
    'cumulative pnl usd': 'cumulative_profit',
    'run up': 'run_up', 'runup': 'run_up', 'run up usd': 'run_up',
    'favorable excursion usd': 'run_up', 'favorable excursion': 'run_up',
    'drawdown': 'drawdown', 'drawdown usd': 'drawdown', 'draw down': 'drawdown',
    'adverse excursion usd': 'drawdown', 'adverse excursion': 'drawdown',
}


def _parse_xlsx_trades(raw_bytes):
    """Parse 'List of trades' sheet from TradingView XLSX export. Returns (rows, symbol, strategy_name)."""
    import openpyxl
    wb = openpyxl.load_workbook(io.BytesIO(raw_bytes), read_only=True, data_only=True)

    # --- Extract symbol and strategy name from Properties sheet ---
    symbol = None
    strategy_name = None
    if 'Properties' in wb.sheetnames:
        props_ws = wb['Properties']
        for row in props_ws.iter_rows(min_row=1, max_col=2, values_only=True):
            if row[0] is None:
                continue
            key = str(row[0]).strip().lower()
            val = str(row[1]).strip() if row[1] is not None else ''
            if key == 'symbol':
                symbol = val
            elif key in ('script name', 'strategy name'):
                strategy_name = val

    # --- Parse List of trades sheet ---
    trades_sheet = None
    for sn in wb.sheetnames:
        if 'list of trades' in sn.lower():
            trades_sheet = wb[sn]
            break
    if trades_sheet is None:
        wb.close()
        raise ValueError('XLSX has no "List of trades" sheet')

    # Read headers from row 1
    header_row = []
    for cell in trades_sheet[1]:
        header_row.append(str(cell.value).strip() if cell.value is not None else '')

    col_map = {}
    for idx, raw_h in enumerate(header_row):
        norm = _normalize_header(raw_h)
        if norm in _TV_HEADER_MAP:
            col_map[idx] = _TV_HEADER_MAP[norm]

    if not any(v == 'type' for v in col_map.values()):
        wb.close()
        raise ValueError('XLSX "List of trades" sheet missing required "Type" column')

    rows = []
    for row in trades_sheet.iter_rows(min_row=2, values_only=True):
        if row[0] is None:
            continue
        mapped = {}
        for idx, internal_key in col_map.items():
            val = row[idx] if idx < len(row) else None
            # Convert datetime objects to string
            if hasattr(val, 'strftime'):
                val = val.strftime('%Y-%m-%d %H:%M')
            mapped[internal_key] = val
        rows.append(mapped)

    wb.close()
    return rows, symbol, strategy_name


@app.route('/api/backtest/upload', methods=['POST'])
def api_backtest_upload():
    """Upload a TradingView Strategy Tester export (XLSX or CSV) and compute summary metrics."""
    try:
        from app.database import SessionLocal
        from app.models import TVBacktestImport, TVBacktestTrade

        # --- Validate inputs ---
        name = request.form.get('name', '').strip()
        if not name:
            return jsonify({'success': False, 'error': 'Name is required'}), 400

        strategy_id = request.form.get('strategy_id') or None
        if strategy_id:
            strategy_id = int(strategy_id)

        file = request.files.get('file')
        if not file or file.filename == '':
            return jsonify({'success': False, 'error': 'File is required'}), 400

        fname_lower = file.filename.lower()
        is_xlsx = fname_lower.endswith('.xlsx')
        is_csv = fname_lower.endswith('.csv')
        if not is_xlsx and not is_csv:
            return jsonify({'success': False, 'error': 'File must be .xlsx or .csv'}), 400

        # Read into memory (max 10 MB for xlsx)
        max_size = 10 * 1024 * 1024
        raw = file.read(max_size + 1)
        if len(raw) > max_size:
            return jsonify({'success': False, 'error': 'File exceeds 10 MB limit'}), 400

        # --- Parse based on file type ---
        xlsx_symbol = None
        xlsx_strategy_name = None

        if is_xlsx:
            rows, xlsx_symbol, xlsx_strategy_name = _parse_xlsx_trades(raw)
        else:
            text = raw.decode('utf-8-sig', errors='replace')
            reader = csv.DictReader(io.StringIO(text))

            # Map raw headers to internal keys
            if not reader.fieldnames:
                return jsonify({'success': False, 'error': 'CSV has no headers'}), 400

            col_map = {}
            for raw_h in reader.fieldnames:
                norm = _normalize_header(raw_h)
                if norm in _TV_HEADER_MAP:
                    col_map[raw_h] = _TV_HEADER_MAP[norm]

            if 'type' not in col_map.values():
                return jsonify({'success': False, 'error': 'CSV missing required "Type" column'}), 400

            rows = []
            for row in reader:
                mapped = {}
                for raw_h, internal_key in col_map.items():
                    mapped[internal_key] = row.get(raw_h, '')
                rows.append(mapped)

        if not rows:
            return jsonify({'success': False, 'error': 'File contains no data rows'}), 400

        # --- Compute summary metrics from Exit rows ---
        wins, losses = 0, 0
        gross_profit, gross_loss = 0.0, 0.0
        win_profits, loss_profits = [], []
        long_count, short_count = 0, 0
        cum_profits = []
        first_date, last_date = None, None

        for r in rows:
            row_type = str(r.get('type') or '').strip().lower()
            # XLSX uses "exit long"/"exit short"/"entry long"/"entry short"
            # CSV uses plain "exit"/"entry"
            is_exit = row_type.startswith('exit')
            is_entry = row_type.startswith('entry')

            if not is_exit:
                # Track entry for long/short counts
                if is_entry:
                    sig = str(r.get('signal') or '').strip().lower()
                    # XLSX: type="Entry long" or signal="L"; CSV: signal contains "long"/"short"
                    if 'long' in row_type or 'long' in sig or sig == 'l':
                        long_count += 1
                    elif 'short' in row_type or 'short' in sig or sig == 's':
                        short_count += 1
                continue

            profit = _parse_tv_number(r.get('profit'))
            cum_p = _parse_tv_number(r.get('cumulative_profit'))
            cum_profits.append(cum_p)

            dt = str(r.get('date_time') or '').strip()
            if dt:
                if first_date is None:
                    first_date = dt
                last_date = dt

            if profit >= 0:
                wins += 1
                gross_profit += profit
                win_profits.append(profit)
            else:
                losses += 1
                gross_loss += abs(profit)
                loss_profits.append(profit)

        total_trades = wins + losses
        win_rate = (wins / total_trades * 100) if total_trades > 0 else 0.0
        net_pnl = gross_profit - gross_loss
        profit_factor = (gross_profit / gross_loss) if gross_loss > 0 else (999.99 if gross_profit > 0 else 0.0)
        avg_win = (gross_profit / wins) if wins > 0 else 0.0
        avg_loss = (gross_loss / losses) if losses > 0 else 0.0
        largest_win = max(win_profits) if win_profits else 0.0
        largest_loss = min(loss_profits) if loss_profits else 0.0
        avg_trade = (net_pnl / total_trades) if total_trades > 0 else 0.0

        # Max drawdown from cumulative profit peak-to-trough
        max_dd = 0.0
        peak = 0.0
        for cp in cum_profits:
            if cp > peak:
                peak = cp
            dd = peak - cp
            if dd > max_dd:
                max_dd = dd

        # Use symbol/strategy extracted from XLSX Properties sheet (if available)
        symbol = xlsx_symbol
        strategy_name_val = xlsx_strategy_name

        # --- Store in database ---
        db = SessionLocal()
        try:
            imp = TVBacktestImport(
                user_id=None,  # TODO: session-based auth
                strategy_id=strategy_id,
                name=name,
                symbol=symbol,
                strategy_name=strategy_name_val,
                total_trades=total_trades,
                wins=wins,
                losses=losses,
                win_rate=round(win_rate, 2),
                profit_factor=round(profit_factor, 2),
                net_pnl=round(net_pnl, 2),
                gross_profit=round(gross_profit, 2),
                gross_loss=round(gross_loss, 2),
                max_drawdown=round(max_dd, 2),
                avg_win=round(avg_win, 2),
                avg_loss=round(avg_loss, 2),
                largest_win=round(largest_win, 2),
                largest_loss=round(largest_loss, 2),
                avg_trade=round(avg_trade, 2),
                long_trades=long_count,
                short_trades=short_count,
                start_date=first_date,
                end_date=last_date,
                raw_filename=file.filename,
            )
            db.add(imp)
            db.flush()  # get imp.id

            trade_objects = []
            for r in rows:
                trade_objects.append(TVBacktestTrade(
                    import_id=imp.id,
                    trade_num=int(_parse_tv_number(r.get('trade_num'))) if r.get('trade_num') else None,
                    type=str(r.get('type') or '').strip(),
                    signal=str(r.get('signal') or '').strip(),
                    date_time=str(r.get('date_time') or '').strip(),
                    price=_parse_tv_number(r.get('price')),
                    contracts=_parse_tv_number(r.get('contracts')),
                    profit=_parse_tv_number(r.get('profit')),
                    cumulative_profit=_parse_tv_number(r.get('cumulative_profit')),
                    run_up=_parse_tv_number(r.get('run_up')),
                    drawdown=_parse_tv_number(r.get('drawdown')),
                ))
            db.bulk_save_objects(trade_objects)
            db.commit()

            metrics = {
                'import_id': imp.id,
                'total_trades': total_trades,
                'wins': wins,
                'losses': losses,
                'win_rate': round(win_rate, 2),
                'profit_factor': round(profit_factor, 2),
                'net_pnl': round(net_pnl, 2),
                'max_drawdown': round(max_dd, 2),
                'avg_trade': round(avg_trade, 2),
            }

            return jsonify({'success': True, 'import_id': imp.id, 'metrics': metrics})
        except Exception:
            db.rollback()
            raise
        finally:
            db.close()

    except Exception as e:
        logger.error(f"Backtest upload error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/backtest/list', methods=['GET'])
def api_backtest_list():
    """Return all backtest imports with summary metrics."""
    try:
        from app.database import SessionLocal
        from app.models import TVBacktestImport

        strategy_id = request.args.get('strategy_id')

        db = SessionLocal()
        try:
            q = db.query(TVBacktestImport).order_by(TVBacktestImport.created_at.desc())
            if strategy_id:
                q = q.filter(TVBacktestImport.strategy_id == int(strategy_id))
            imports = q.all()

            results = []
            for imp in imports:
                results.append({
                    'id': imp.id,
                    'name': imp.name,
                    'symbol': imp.symbol,
                    'strategy_name': imp.strategy_name,
                    'strategy_id': imp.strategy_id,
                    'total_trades': imp.total_trades,
                    'wins': imp.wins,
                    'losses': imp.losses,
                    'win_rate': imp.win_rate,
                    'profit_factor': imp.profit_factor,
                    'net_pnl': imp.net_pnl,
                    'gross_profit': imp.gross_profit,
                    'gross_loss': imp.gross_loss,
                    'max_drawdown': imp.max_drawdown,
                    'avg_win': imp.avg_win,
                    'avg_loss': imp.avg_loss,
                    'largest_win': imp.largest_win,
                    'largest_loss': imp.largest_loss,
                    'avg_trade': imp.avg_trade,
                    'long_trades': imp.long_trades,
                    'short_trades': imp.short_trades,
                    'start_date': imp.start_date,
                    'end_date': imp.end_date,
                    'raw_filename': imp.raw_filename,
                    'created_at': imp.created_at.isoformat() if imp.created_at else None,
                })

            return jsonify({'success': True, 'imports': results})
        finally:
            db.close()

    except Exception as e:
        logger.error(f"Backtest list error: {e}")
        return jsonify({'success': False, 'error': str(e), 'imports': []}), 500


@app.route('/api/backtest/<int:import_id>', methods=['DELETE'])
def api_backtest_delete(import_id):
    """Delete a backtest import and its trades."""
    try:
        from app.database import SessionLocal
        from app.models import TVBacktestImport, TVBacktestTrade

        db = SessionLocal()
        try:
            imp = db.query(TVBacktestImport).filter(TVBacktestImport.id == import_id).first()
            if not imp:
                return jsonify({'success': False, 'error': 'Import not found'}), 404

            # Delete trades first (SQLite compat), then the import
            db.query(TVBacktestTrade).filter(TVBacktestTrade.import_id == import_id).delete()
            db.delete(imp)
            db.commit()

            return jsonify({'success': True, 'message': f'Backtest "{imp.name}" deleted'})
        except Exception:
            db.rollback()
            raise
        finally:
            db.close()

    except Exception as e:
        logger.error(f"Backtest delete error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/news-feed', methods=['GET'])
def api_news_feed():
    """Get financial news from RSS feeds"""
    try:
        import feedparser
        import urllib.parse
        
        # Try Yahoo Finance RSS (free, no API key needed)
        feeds = [
            'https://feeds.finance.yahoo.com/rss/2.0/headline?s=ES=F,NQ=F,YM=F&region=US&lang=en-US',
            'https://www.financialjuice.com/feed'
        ]
        
        news_items = []
        
        for feed_url in feeds:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:5]:  # Get first 5 items
                    title = entry.get('title', '')[:80]  # Limit length
                    if title:
                        news_items.append({
                            'title': title,
                            'link': entry.get('link', '#')
                        })
            except Exception as e:
                logger.warning(f"Error parsing feed {feed_url}: {e}")
                continue
        
        # If no news, return sample data
        if not news_items:
            news_items = [
                {'title': 'Markets open higher on positive economic data', 'link': '#'},
                {'title': 'Fed signals potential rate adjustments ahead', 'link': '#'},
                {'title': 'Tech stocks rally on strong earnings reports', 'link': '#'},
                {'title': 'Futures trading volume hits record highs', 'link': '#'}
            ]
        
        return jsonify({'news': news_items[:10]})  # Return up to 10 items
    except Exception as e:
        logger.error(f"Error fetching news: {e}")
        # Return sample data on error
        return jsonify({
            'news': [
                {'title': 'Markets open higher on positive economic data', 'link': '#'},
                {'title': 'Fed signals potential rate adjustments ahead', 'link': '#'},
                {'title': 'Tech stocks rally on strong earnings reports', 'link': '#'}
            ]
        })

@app.route('/api/market-data/test', methods=['POST'])
def api_market_data_test():
    """Test market data by getting a quote from TradingView (free, no auth required)"""
    global _market_data_cache

    try:
        data = request.get_json() or {}
        symbol = data.get('symbol', 'MNQ')  # Default to MNQ

        # Extract root symbol (MNQ, MES, ES, NQ, etc)
        root = symbol.upper()
        for suffix in ['1!', '!', 'H5', 'M5', 'U5', 'Z5', 'H4', 'M4', 'U4', 'Z4']:
            root = root.replace(suffix, '')
        root = root.replace('CME_MINI:', '').replace('CME:', '')
        if len(root) > 3:
            root = root[:3]

        # Build TradingView symbol - use correct exchange prefix
        if root in ['MNQ', 'MES', 'M2K', 'MYM', 'ES', 'NQ', 'RTY']:
            tv_symbol = f"CME_MINI:{root}1!"
        elif root in ['YM']:
            tv_symbol = f"CBOT_MINI:{root}1!"
        elif root in ['CL', 'NG', 'HO', 'RB']:
            tv_symbol = f"NYMEX:{root}1!"
        elif root in ['GC', 'SI', 'HG']:
            tv_symbol = f"COMEX:{root}1!"
        else:
            tv_symbol = f"CME:{root}1!"

        # Use TradingView scanner API (free, no authentication required)
        url = "https://scanner.tradingview.com/futures/scan"
        payload = {
            "symbols": {"tickers": [tv_symbol]},
            "columns": ["close", "high", "low", "open", "change"]
        }

        response = requests.post(url, json=payload, timeout=10)

        if response.status_code == 200:
            tv_data = response.json()
            if tv_data.get('data') and len(tv_data['data']) > 0:
                values = tv_data['data'][0].get('d', [])
                if values and values[0]:
                    price = float(values[0])

                    # Cache the price
                    _market_data_cache[root] = {
                        'last': price,
                        'high': values[1] if len(values) > 1 else None,
                        'low': values[2] if len(values) > 2 else None,
                        'open': values[3] if len(values) > 3 else None,
                        'change': values[4] if len(values) > 4 else None,
                        'source': 'TradingView',
                        'updated': time.time()
                    }

                    return jsonify({
                        'success': True,
                        'symbol': root,
                        'tv_symbol': tv_symbol,
                        'price': price,
                        'high': values[1] if len(values) > 1 else None,
                        'low': values[2] if len(values) > 2 else None,
                        'change': values[4] if len(values) > 4 else None,
                        'source': 'TradingView (free API)',
                        'message': f'Successfully retrieved price for {root}: ${price}',
                        'paper_trading_ready': True
                    })

            return jsonify({
                'success': False,
                'error': 'No price data returned from TradingView',
                'tv_symbol': tv_symbol
            }), 400
        else:
            return jsonify({
                'success': False,
                'error': f'TradingView API error: {response.status_code}',
                'response': response.text[:500]
            }), 400

    except Exception as e:
        logger.error(f"Error testing market data: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


def get_price_from_tradingview(symbol: str) -> Optional[float]:
    """Get current price from TradingView scanner API (free, no auth required)"""
    try:
        root = symbol.upper()
        for suffix in ['1!', '!', 'H5', 'M5', 'U5', 'Z5']:
            root = root.replace(suffix, '')
        root = root.replace('CME_MINI:', '').replace('CME:', '')
        if len(root) > 3:
            root = root[:3]

        if root in ['MNQ', 'MES', 'M2K', 'MYM', 'ES', 'NQ', 'RTY']:
            tv_symbol = f"CME_MINI:{root}1!"
        elif root in ['YM']:
            tv_symbol = f"CBOT_MINI:{root}1!"
        elif root in ['CL', 'NG', 'HO', 'RB']:
            tv_symbol = f"NYMEX:{root}1!"
        elif root in ['GC', 'SI', 'HG']:
            tv_symbol = f"COMEX:{root}1!"
        else:
            tv_symbol = f"CME:{root}1!"

        url = "https://scanner.tradingview.com/futures/scan"
        payload = {
            "symbols": {"tickers": [tv_symbol]},
            "columns": ["close"]
        }

        response = requests.post(url, json=payload, timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data.get('data') and len(data['data']) > 0:
                values = data['data'][0].get('d', [])
                if values and values[0]:
                    return float(values[0])
        return None
    except Exception as e:
        logger.debug(f"Error getting TradingView price: {e}")
        return None


@app.route('/api/market-data/status', methods=['GET'])
def api_market_data_status():
    """Check market data connection status and available tokens"""
    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()
        cursor = conn.cursor()

        # Check for accounts with tokens
        if is_postgres:
            cursor.execute('''
                SELECT id, name, environment,
                       CASE WHEN md_access_token IS NOT NULL AND md_access_token != '' THEN true ELSE false END as has_md_token,
                       CASE WHEN tradovate_token IS NOT NULL AND tradovate_token != '' THEN true ELSE false END as has_access_token
                FROM accounts
                WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
                ORDER BY id
            ''')
        else:
            cursor.execute('''
                SELECT id, name, environment,
                       CASE WHEN md_access_token IS NOT NULL AND md_access_token != '' THEN 1 ELSE 0 END as has_md_token,
                       CASE WHEN tradovate_token IS NOT NULL AND tradovate_token != '' THEN 1 ELSE 0 END as has_access_token
                FROM accounts
                WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
                ORDER BY id
            ''')

        rows = cursor.fetchall()
        conn.close()

        accounts_with_tokens = []
        for row in rows:
            if isinstance(row, dict):
                accounts_with_tokens.append({
                    'id': row['id'],
                    'name': row['name'],
                    'environment': row['environment'],
                    'has_md_token': bool(row['has_md_token']),
                    'has_access_token': bool(row['has_access_token'])
                })
            else:
                accounts_with_tokens.append({
                    'id': row[0],
                    'name': row[1],
                    'environment': row[2],
                    'has_md_token': bool(row[3]),
                    'has_access_token': bool(row[4])
                })

        # Check websocket connection status
        ws_connected = _market_data_ws is not None and not getattr(_market_data_ws, 'closed', True)

        return jsonify({
            'success': True,
            'websocket_connected': ws_connected,
            'accounts_with_tokens': accounts_with_tokens,
            'total_accounts': len(accounts_with_tokens),
            'accounts_with_md_token': sum(1 for a in accounts_with_tokens if a['has_md_token']),
            'market_data_cache_symbols': list(_market_data_cache.keys()) if _market_data_cache else [],
            'instructions': 'Market data uses md_access_token if available, otherwise falls back to regular accessToken'
        })
    except Exception as e:
        logger.error(f"Error checking market data status: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/market-data', methods=['GET'])
def api_market_data():
    """Get LIVE market data for futures ticker from TradingView"""
    try:
        # Fetch live futures prices
        live_prices = fetch_live_futures_prices()
        
        market_data = []
        for item in live_prices:
            market_data.append({
                'symbol': item['symbol'],
                'price': item['price_str'],
                'change': item['change_str'],
                'direction': item['direction']
            })
        
        # If no live data available, return sample data as fallback
        if not market_data:
            logger.warning("No live futures data available, using fallback")
            market_data = [
                {'symbol': 'ES', 'price': '$5,950.00', 'change': '+0.00%', 'direction': 'up'},
                {'symbol': 'NQ', 'price': '$20,500.00', 'change': '+0.00%', 'direction': 'up'},
                {'symbol': 'MNQ', 'price': '$20,500.00', 'change': '+0.00%', 'direction': 'up'},
                {'symbol': 'YM', 'price': '$43,500.00', 'change': '+0.00%', 'direction': 'up'},
            ]
        
        return jsonify({'data': market_data})
    except Exception as e:
        logger.error(f"Error fetching market data: {e}")
        return jsonify({'data': []})

# ============================================================================
# TRADINGVIEW REAL-TIME PRICE API (Paper Trading)
# ============================================================================

@app.route('/api/live-prices', methods=['GET'])
def api_live_prices():
    """Get real-time prices from TradingView WebSocket (~300-500ms delay)"""
    if not TV_PRICE_SERVICE_AVAILABLE:
        return jsonify({'success': False, 'error': 'TradingView price service not available'}), 503

    try:
        ticker = get_ticker()
        prices = ticker.get_all_prices()

        # Format response
        formatted = {}
        for symbol, data in prices.items():
            # Convert TradingView symbol to simple format (CME_MINI:NQ1! -> NQ)
            simple_symbol = symbol.split(':')[-1].replace('1!', '').upper()
            formatted[simple_symbol] = {
                'symbol': simple_symbol,
                'full_symbol': symbol,
                'price': data.get('last_price'),
                'bid': data.get('bid'),
                'ask': data.get('ask'),
                'change': data.get('change'),
                'change_percent': data.get('change_percent'),
                'high': data.get('high'),
                'low': data.get('low'),
                'volume': data.get('volume'),
                'timestamp': data.get('timestamp'),
                'age_ms': int((time.time() - data.get('update_time', time.time())) * 1000)
            }

        return jsonify({
            'success': True,
            'prices': formatted,
            'source': 'tradingview_websocket',
            'connected': ticker.connected
        })
    except Exception as e:
        logger.error(f"Error getting live prices: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/live-prices/<symbol>', methods=['GET'])
def api_live_price_symbol(symbol):
    """Get real-time price for specific symbol"""
    if not TV_PRICE_SERVICE_AVAILABLE:
        return jsonify({'success': False, 'error': 'TradingView price service not available'}), 503

    try:
        ticker = get_ticker()

        # Try different symbol formats
        symbol_formats = [
            f"CME_MINI:{symbol.upper()}1!",  # e.g., CME_MINI:NQ1!
            f"CME_MINI:{symbol.upper()}",
            symbol.upper()
        ]

        price_data = None
        for fmt in symbol_formats:
            price_data = ticker.get_price(fmt)
            if price_data:
                break

        if not price_data:
            return jsonify({'success': False, 'error': f'No price data for {symbol}'}), 404

        simple_symbol = symbol.upper()
        return jsonify({
            'success': True,
            'symbol': simple_symbol,
            'price': price_data.get('last_price'),
            'bid': price_data.get('bid'),
            'ask': price_data.get('ask'),
            'change': price_data.get('change'),
            'change_percent': price_data.get('change_percent'),
            'timestamp': price_data.get('timestamp'),
            'age_ms': int((time.time() - price_data.get('update_time', time.time())) * 1000)
        })
    except Exception as e:
        logger.error(f"Error getting price for {symbol}: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/price-stream')
def api_price_stream():
    """SSE endpoint for real-time price streaming to the dashboard.
    Replaces 5-second polling with push-based tick-by-tick updates.
    Frontend connects via EventSource('/api/price-stream')."""
    def generate():
        q = Queue(maxsize=200)
        with _sse_price_lock:
            _sse_price_queues.append(q)
        try:
            # Send initial snapshot of all cached prices
            snapshot = {}
            for sym, sdata in dict(_market_data_cache).items():
                if isinstance(sdata, dict) and 'last' in sdata and ':' not in sym:
                    snapshot[sym] = {
                        'symbol': sym,
                        'price': sdata.get('last'),
                        'bid': sdata.get('bid'),
                        'ask': sdata.get('ask'),
                        'change': sdata.get('ch'),
                        'change_percent': sdata.get('chp'),
                        'source': sdata.get('source'),
                    }
            yield f"data: {json.dumps({'type': 'snapshot', 'prices': snapshot})}\n\n"

            while True:
                try:
                    msg = q.get(timeout=15)
                    yield f"data: {json.dumps(msg)}\n\n"
                except Empty:
                    yield ": heartbeat\n\n"
        except GeneratorExit:
            pass
        finally:
            with _sse_price_lock:
                if q in _sse_price_queues:
                    _sse_price_queues.remove(q)

    return Response(
        generate(),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no',
            'Connection': 'keep-alive',
        }
    )


# ============================================================================
# Paper Trading ‚Äî Dashboard API (reads from legacy paper_trades table)
# ============================================================================

class PaperDBReader:
    """Read-only helper for the paper_trades table (legacy).
    Uses the same database as the main app (get_db_connection / is_using_postgres).
    Open positions = paper_trades WHERE status='open'.
    Closed trades = paper_trades WHERE status='closed'."""

    _tables_ensured = True  # Legacy table created by _record_paper_trade_direct()

    @staticmethod
    def _query(sql, params=(), fetch='all'):
        """Run a read query. Returns list of dicts."""
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        # Replace {ph} placeholders in SQL
        sql = sql.replace('{ph}', ph)
        conn = get_db_connection()
        try:
            cursor = conn.cursor()
            cursor.execute(sql, params)
            rows = cursor.fetchall() if fetch == 'all' else [cursor.fetchone()] if fetch == 'one' else []
            if not rows or rows[0] is None:
                return []
            # Both SQLite (Row) and PostgreSQL (RealDictCursor/DictRow) return dict-like objects
            return [dict(r) for r in rows]
        finally:
            conn.close()

    @staticmethod
    def get_open_positions(recorder_id=None):
        sql = "SELECT pt.*, r.name as recorder_name FROM paper_trades pt LEFT JOIN recorders r ON pt.recorder_id = r.id WHERE pt.status='open'"
        params = []
        if recorder_id is not None:
            sql += " AND pt.recorder_id={ph}"
            params.append(recorder_id)
        sql += " ORDER BY pt.opened_at DESC"
        rows = PaperDBReader._query(sql, tuple(params))
        result = []
        # Per-request price cache: avoid fetching same symbol price multiple times
        _price_cache = {}
        for r in rows:
            symbol = r.get('symbol', '')
            side = r.get('side', 'LONG')
            quantity = r.get('quantity', 0) or 0
            entry_price = r.get('entry_price', 0) or 0

            # Get live price (cache ‚Üí WebSocket cache ‚Üí Yahoo/TradingView API)
            sym_root = extract_symbol_root(symbol)
            if sym_root not in _price_cache:
                live_price = _get_live_price_for_symbol(symbol)
                if not live_price:
                    # Fallback: fetch from Yahoo Finance / TradingView scanner
                    live_price = get_market_price_simple(symbol)
                _price_cache[sym_root] = live_price
            live_price = _price_cache[sym_root]

            unrealized = 0
            if live_price and entry_price and quantity:
                unrealized = _calculate_unrealized_pnl(symbol, side, quantity, entry_price, live_price)

            rec_id = r.get('recorder_id')
            rec_name = r.get('recorder_name') or f'Recorder #{rec_id}'

            result.append({
                'recorder_id': rec_id,
                'recorder_name': rec_name,
                'symbol': symbol,
                'side': side,
                'quantity': quantity,
                'entry_price': entry_price,
                'current_price': round(live_price, 2) if live_price else None,
                'unrealized_pnl': round(unrealized, 2),
                'tp_price': r.get('tp_price'),
                'sl_price': r.get('sl_price'),
                'dca_count': 0,
                'opened_at': str(r.get('opened_at', '')),
            })
        return result

    @staticmethod
    def get_trade_history(recorder_id=None, limit=100):
        sql = "SELECT * FROM paper_trades WHERE status='closed' AND pnl IS NOT NULL"
        params = []
        if recorder_id is not None:
            sql += " AND recorder_id={ph}"
            params.append(recorder_id)
        sql += " ORDER BY closed_at DESC LIMIT {ph}"
        params.append(limit)
        rows = PaperDBReader._query(sql, tuple(params))
        result = []
        for r in rows:
            result.append({
                'id': r.get('id'),
                'recorder_id': r.get('recorder_id'),
                'symbol': r.get('symbol'),
                'side': r.get('side'),
                'quantity': r.get('quantity'),
                'entry_price': r.get('entry_price'),
                'exit_price': r.get('exit_price'),
                'pnl': r.get('pnl'),
                'drawdown': r.get('drawdown'),
                'cumulative_pnl': r.get('cumulative_pnl'),
                'pnl_ticks': None,
                'exit_reason': r.get('exit_reason'),
                'dca_count': 0,
                'max_favorable_excursion': 0,
                'max_adverse_excursion': 0,
                'hold_time_seconds': None,
                'opened_at': str(r.get('opened_at', '')),
                'closed_at': str(r.get('closed_at', '')),
            })
        return result

    @staticmethod
    def get_analytics(recorder_id=None):
        sql = "SELECT pnl, exit_reason, closed_at FROM paper_trades WHERE status='closed' AND pnl IS NOT NULL"
        params = []
        if recorder_id is not None:
            sql += " AND recorder_id={ph}"
            params.append(recorder_id)
        sql += " ORDER BY closed_at ASC"
        rows = PaperDBReader._query(sql, tuple(params))

        if not rows:
            return {
                'total_trades': 0, 'winning_trades': 0, 'losing_trades': 0,
                'win_rate': 0, 'total_pnl': 0, 'gross_profit': 0, 'gross_loss': 0,
                'profit_factor': 0, 'average_win': 0, 'average_loss': 0,
                'largest_win': 0, 'largest_loss': 0, 'max_drawdown': 0,
                'average_trade': 0, 'expectancy': 0, 'avg_hold_time_seconds': 0,
            }

        pnls = [r['pnl'] for r in rows]
        hold_times = []  # Legacy table doesn't track hold time
        winners = [p for p in pnls if p > 0]
        losers = [p for p in pnls if p < 0]

        total_trades = len(pnls)
        winning_trades = len(winners)
        losing_trades = len(losers)
        win_rate = (winning_trades / total_trades * 100) if total_trades else 0

        total_pnl = sum(pnls)
        gross_profit = sum(winners) if winners else 0
        gross_loss = abs(sum(losers)) if losers else 0
        profit_factor = (gross_profit / gross_loss) if gross_loss > 0 else (999.0 if gross_profit > 0 else 0)

        average_win = (gross_profit / winning_trades) if winning_trades else 0
        average_loss = (gross_loss / losing_trades) if losing_trades else 0
        largest_win = max(winners) if winners else 0
        largest_loss = min(losers) if losers else 0
        average_trade = total_pnl / total_trades if total_trades else 0

        loss_rate = losing_trades / total_trades if total_trades else 0
        expectancy = ((win_rate / 100) * average_win) - (loss_rate * average_loss)

        # Max drawdown from equity curve
        peak = 0
        running = 0
        max_dd = 0
        for p in pnls:
            running += p
            if running > peak:
                peak = running
            dd = peak - running
            if dd > max_dd:
                max_dd = dd

        avg_hold = (sum(hold_times) / len(hold_times)) if hold_times else 0

        return {
            'total_trades': total_trades,
            'winning_trades': winning_trades,
            'losing_trades': losing_trades,
            'win_rate': round(win_rate, 2),
            'total_pnl': round(total_pnl, 2),
            'gross_profit': round(gross_profit, 2),
            'gross_loss': round(gross_loss, 2),
            'profit_factor': round(profit_factor, 2),
            'average_win': round(average_win, 2),
            'average_loss': round(average_loss, 2),
            'largest_win': round(largest_win, 2),
            'largest_loss': round(largest_loss, 2),
            'max_drawdown': round(max_dd, 2),
            'average_trade': round(average_trade, 2),
            'expectancy': round(expectancy, 2),
            'avg_hold_time_seconds': round(avg_hold, 0),
        }

    @staticmethod
    def get_equity_curve(recorder_id=None, limit=500):
        sql = "SELECT pnl, closed_at, symbol, side FROM paper_trades WHERE status='closed' AND pnl IS NOT NULL"
        params = []
        if recorder_id is not None:
            sql += " AND recorder_id={ph}"
            params.append(recorder_id)
        sql += " ORDER BY closed_at ASC LIMIT {ph}"
        params.append(limit)
        rows = PaperDBReader._query(sql, tuple(params))

        points = []
        running = 0.0
        for r in rows:
            pnl_val = r['pnl']
            running += pnl_val
            points.append({
                'timestamp': str(r.get('closed_at', '')),
                'pnl': round(pnl_val, 2),
                'cumulative_pnl': round(running, 2),
                'symbol': r.get('symbol'),
                'side': r.get('side'),
            })
        return points

    @staticmethod
    def get_daily_pnl(recorder_id=None, days=30):
        is_postgres = is_using_postgres()
        if is_postgres:
            date_filter = f"closed_at >= CURRENT_DATE - INTERVAL '{days} days'"
        else:
            date_filter = f"closed_at >= DATE('now', '-{days} days')"

        sql = f'''
            SELECT DATE(closed_at) as trade_date,
                   COUNT(*) as trade_count,
                   SUM(pnl) as daily_pnl,
                   SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
                   SUM(CASE WHEN pnl < 0 THEN 1 ELSE 0 END) as losers
            FROM paper_trades
            WHERE status='closed' AND pnl IS NOT NULL AND {date_filter}
        '''
        params = []
        if recorder_id is not None:
            sql += " AND recorder_id={ph}"
            params.append(recorder_id)
        sql += " GROUP BY DATE(closed_at) ORDER BY trade_date DESC"
        rows = PaperDBReader._query(sql, tuple(params))

        result = []
        for r in rows:
            count = r.get('trade_count', 0)
            w = r.get('winners', 0) or 0
            dpnl = r.get('daily_pnl', 0) or 0
            result.append({
                'date': str(r.get('trade_date', '')),
                'trade_count': count,
                'pnl': round(dpnl, 2),
                'winners': w,
                'losers': r.get('losers', 0) or 0,
                'win_rate': round((w / count * 100), 1) if count else 0,
            })
        return result

    @staticmethod
    def get_symbol_stats(recorder_id=None):
        sql = '''
            SELECT symbol,
                   COUNT(*) as trade_count,
                   SUM(pnl) as total_pnl,
                   AVG(pnl) as avg_pnl,
                   SUM(CASE WHEN pnl > 0 THEN 1 ELSE 0 END) as winners,
                   SUM(CASE WHEN pnl < 0 THEN 1 ELSE 0 END) as losers,
                   MAX(pnl) as best_trade,
                   MIN(pnl) as worst_trade
            FROM paper_trades
            WHERE status='closed' AND pnl IS NOT NULL
        '''
        params = []
        if recorder_id is not None:
            sql += " AND recorder_id={ph}"
            params.append(recorder_id)
        sql += " GROUP BY symbol ORDER BY total_pnl DESC"
        rows = PaperDBReader._query(sql, tuple(params))

        result = []
        for r in rows:
            count = r.get('trade_count', 0)
            w = r.get('winners', 0) or 0
            result.append({
                'symbol': r.get('symbol'),
                'trade_count': count,
                'total_pnl': round(r.get('total_pnl', 0) or 0, 2),
                'avg_pnl': round(r.get('avg_pnl', 0) or 0, 2),
                'winners': w,
                'losers': r.get('losers', 0) or 0,
                'win_rate': round((w / count * 100), 1) if count else 0,
                'best_trade': round(r.get('best_trade', 0) or 0, 2),
                'worst_trade': round(r.get('worst_trade', 0) or 0, 2),
            })
        return result

    @staticmethod
    def get_chart_data(recorder_id=None, limit=500):
        """Get PnL chart data: cumulative profit + peak recorded drawdown per day"""
        sql = "SELECT pnl, closed_at, drawdown FROM paper_trades WHERE status='closed' AND pnl IS NOT NULL"
        params = []
        if recorder_id is not None:
            sql += " AND recorder_id={ph}"
            params.append(recorder_id)
        sql += " ORDER BY closed_at ASC LIMIT {ph}"
        params.append(limit)
        rows = PaperDBReader._query(sql, tuple(params))

        if not rows:
            return {'labels': [], 'profit': [], 'drawdown': []}

        # Aggregate trades by day ‚Äî peak recorded DD is max(drawdown) from DB
        from datetime import datetime
        daily = {}  # date_str -> {pnl_sum, peak_dd}
        day_order = []

        for r in rows:
            pnl_val = r.get('pnl', 0) or 0
            trade_dd = abs(r.get('drawdown', 0) or 0)
            closed_at = str(r.get('closed_at', ''))
            date_str = ''
            if closed_at:
                try:
                    dt = datetime.fromisoformat(closed_at.replace('Z', '+00:00'))
                    date_str = dt.strftime('%b %d')
                except Exception:
                    date_str = closed_at[:10]
            if not date_str:
                date_str = f'Day {len(daily) + 1}'

            if date_str not in daily:
                daily[date_str] = {'pnl_sum': 0.0, 'peak_dd': 0.0}
                day_order.append(date_str)

            daily[date_str]['pnl_sum'] += pnl_val
            if trade_dd > daily[date_str]['peak_dd']:
                daily[date_str]['peak_dd'] = trade_dd

        labels = []
        profit = []
        drawdown = []
        running_pnl = 0.0

        for date_str in day_order:
            d = daily[date_str]
            running_pnl += d['pnl_sum']

            labels.append(date_str)
            profit.append(round(running_pnl, 2))
            drawdown.append(round(d['peak_dd'], 2))  # Peak intraday drawdown for the day

        return {'labels': labels, 'profit': profit, 'drawdown': drawdown}

    @staticmethod
    def get_trade_history_paginated(page=1, per_page=20, result_filter=None, recorder_id=None):
        """Get paginated trade history with optional win/loss filter."""
        # Build WHERE clause
        conditions = ["pt.status = 'closed'", "pt.pnl IS NOT NULL"]
        params = []

        if recorder_id is not None:
            conditions.append("pt.recorder_id={ph}")
            params.append(recorder_id)

        if result_filter == 'win':
            conditions.append("pt.pnl >= 0")
        elif result_filter == 'loss':
            conditions.append("pt.pnl < 0")

        where_clause = ' AND '.join(conditions)

        # Count total
        count_sql = f"SELECT COUNT(*) as cnt FROM paper_trades pt WHERE {where_clause}"
        count_rows = PaperDBReader._query(count_sql, tuple(params))
        total = count_rows[0]['cnt'] if count_rows else 0

        total_pages = max(1, (total + per_page - 1) // per_page)
        page = max(1, min(page, total_pages))
        offset = (page - 1) * per_page

        # Fetch trades with recorder name
        sql = f'''
            SELECT pt.*, r.name as recorder_name
            FROM paper_trades pt
            LEFT JOIN recorders r ON pt.recorder_id = r.id
            WHERE {where_clause}
            ORDER BY pt.closed_at DESC
            LIMIT {{ph}} OFFSET {{ph}}
        '''
        rows = PaperDBReader._query(sql, tuple(params) + (per_page, offset))

        trades = []
        for r in rows:
            pnl = r.get('pnl', 0) or 0
            if pnl > 0:
                status = 'WIN'
            elif pnl < 0:
                status = 'LOSS'
            else:
                status = 'FLAT'

            trades.append({
                'id': r.get('id'),
                'recorder_id': r.get('recorder_id'),
                'recorder_name': r.get('recorder_name') or f"Recorder #{r.get('recorder_id')}",
                'symbol': r.get('symbol'),
                'side': r.get('side'),
                'quantity': r.get('quantity'),
                'entry_price': r.get('entry_price'),
                'exit_price': r.get('exit_price'),
                'pnl': pnl,
                'drawdown': r.get('drawdown'),
                'cumulative_pnl': r.get('cumulative_pnl'),
                'opened_at': str(r.get('opened_at', '')) if r.get('opened_at') else None,
                'closed_at': str(r.get('closed_at', '')) if r.get('closed_at') else None,
                'trade_status': r.get('status'),
                'result_status': status,
            })

        return {
            'trades': trades,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total_pages': total_pages,
                'total': total,
                'has_prev': page > 1,
                'has_next': page < total_pages,
            }
        }


# --- Paper Trading v2 API Routes ---

@app.route('/api/paper-trades/positions', methods=['GET'])
def api_paper_positions():
    """Get all open paper trading positions (from paper_positions_v2)"""
    try:
        recorder_id = request.args.get('recorder_id', type=int)
        positions = PaperDBReader.get_open_positions(recorder_id)
        return jsonify({
            'success': True,
            'positions': positions,
            'count': len(positions)
        })
    except Exception as e:
        logger.error(f"Error getting paper positions: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/recorder/<int:recorder_id>', methods=['GET'])
def api_paper_recorder_pnl(recorder_id):
    """Get P&L and positions for a specific recorder (from paper_positions_v2)"""
    try:
        positions = PaperDBReader.get_open_positions(recorder_id)
        total_unrealized = sum(p.get('unrealized_pnl', 0) or 0 for p in positions)
        return jsonify({
            'success': True,
            'recorder_id': recorder_id,
            'unrealized_pnl': round(total_unrealized, 2),
            'positions': positions
        })
    except Exception as e:
        logger.error(f"Error getting recorder {recorder_id} P&L: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/chart-data', methods=['GET'])
def api_paper_chart_data():
    """Get PnL chart data (cumulative profit + drawdown) for Chart.js"""
    try:
        recorder_id = request.args.get('recorder_id', type=int)
        limit = request.args.get('limit', 500, type=int)

        chart_data = PaperDBReader.get_chart_data(recorder_id=recorder_id, limit=limit)

        return jsonify({
            'success': True,
            'labels': chart_data['labels'],
            'profit': chart_data['profit'],
            'drawdown': chart_data['drawdown']
        })
    except Exception as e:
        logger.error(f"Error getting chart data: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/history', methods=['GET'])
def api_paper_trade_history():
    """Get paper trade history ‚Äî supports legacy and paginated modes"""
    try:
        recorder_id = request.args.get('recorder_id', type=int)
        page = request.args.get('page', type=int)

        # Paginated mode: if page param is present
        if page is not None:
            per_page = request.args.get('per_page', 20, type=int)
            result_filter = request.args.get('filter', type=str)  # win, loss, or None
            result = PaperDBReader.get_trade_history_paginated(
                page=page, per_page=per_page,
                result_filter=result_filter, recorder_id=recorder_id
            )
            return jsonify({'success': True, **result})

        # Legacy mode: flat list
        limit = request.args.get('limit', 100, type=int)
        trades = PaperDBReader.get_trade_history(recorder_id=recorder_id, limit=limit)

        return jsonify({
            'success': True,
            'trades': trades,
            'count': len(trades)
        })
    except Exception as e:
        logger.error(f"Error getting trade history: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/open', methods=['POST'])
def api_paper_open_position():
    """Open a paper trading position ‚Äî proxies to paper service on port 5050"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400

        recorder_id = data.get('recorder_id')
        symbol = data.get('symbol')
        side = data.get('side')

        if not all([recorder_id, symbol, side]):
            return jsonify({'success': False, 'error': 'Missing required fields: recorder_id, symbol, side'}), 400

        import requests as http_requests
        resp = http_requests.post('http://127.0.0.1:5050/api/paper/open', json=data, timeout=5)
        return jsonify(resp.json()), resp.status_code
    except Exception as e:
        logger.error(f"Error opening paper position: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/close', methods=['POST'])
def api_paper_close_position():
    """Close a paper trading position ‚Äî proxies to paper service on port 5050"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400

        recorder_id = data.get('recorder_id')
        symbol = data.get('symbol')

        if not all([recorder_id, symbol]):
            return jsonify({'success': False, 'error': 'Missing required fields: recorder_id, symbol'}), 400

        import requests as http_requests
        resp = http_requests.post('http://127.0.0.1:5050/api/paper/close', json=data, timeout=5)
        return jsonify(resp.json()), resp.status_code
    except Exception as e:
        logger.error(f"Error closing paper position: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/service-status', methods=['GET'])
def api_paper_service_status():
    """Get paper trading service status (checks v2 DB + optional service health)"""
    try:
        positions = PaperDBReader.get_open_positions()
        service_up = False
        try:
            import requests as http_requests
            resp = http_requests.get('http://127.0.0.1:5050/api/paper/health', timeout=2)
            service_up = resp.status_code == 200
        except Exception:
            pass

        # Also check TradingView ticker if available
        tv_info = {}
        if TV_PRICE_SERVICE_AVAILABLE:
            try:
                ticker = get_ticker()
                tv_info = {
                    'websocket_connected': ticker.connected,
                    'symbols_tracked': len(ticker.symbols),
                    'prices_cached': len(ticker.prices),
                }
            except Exception:
                pass

        return jsonify({
            'success': True,
            'available': True,
            'paper_service_running': service_up,
            'open_positions': len(positions),
            **tv_info
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'available': False,
            'error': str(e)
        })

@app.route('/api/paper-trades/analytics', methods=['GET'])
def api_paper_analytics():
    """Get comprehensive paper trading analytics (from paper_trades_v2)."""
    try:
        recorder_id = request.args.get('recorder_id', type=int)
        analytics = PaperDBReader.get_analytics(recorder_id)
        return jsonify({
            'success': True,
            'recorder_id': recorder_id,
            'analytics': analytics
        })
    except Exception as e:
        logger.error(f"Error getting paper analytics: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/premium-showcase', methods=['GET'])
def api_premium_showcase():
    """Get all data for the Premium Strategies Showcase.

    Returns combined analytics for all premium recorders using paper_trades_v2
    via PaperDBReader. Includes per-strategy stats, equity curve, recent trades.
    """
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'

        # Get all premium recorders
        true_val = 'TRUE' if is_postgres else '1'
        cursor.execute(f'SELECT id, name, symbol FROM recorders WHERE is_premium = {true_val}')

        rows = cursor.fetchall()
        premium_recorders = []
        for row in rows:
            if isinstance(row, dict) or hasattr(row, 'keys'):
                premium_recorders.append(dict(row))
            else:
                premium_recorders.append({'id': row[0], 'name': row[1], 'symbol': row[2]})
        conn.close()

        if not premium_recorders:
            return jsonify({
                'success': True,
                'message': 'No premium strategies configured',
                'strategies': [],
                'combined_stats': None,
                'recent_trades': [],
                'equity_curve': []
            })

        premium_ids = [r['id'] for r in premium_recorders]

        # Get per-strategy stats using PaperDBReader.get_analytics()
        strategies = []
        for recorder in premium_recorders:
            analytics = PaperDBReader.get_analytics(recorder['id'])
            strategies.append({
                'id': recorder['id'],
                'name': recorder['name'],
                'symbol': recorder.get('symbol', ''),
                'trades': analytics.get('total_trades', 0),
                'win_rate': analytics.get('win_rate', 0),
                'pnl': analytics.get('total_pnl', 0),
            })

        # Get combined stats across all premium recorders
        placeholders = ','.join(['{ph}'] * len(premium_ids))
        combined_sql = f"""SELECT pnl FROM paper_trades
            WHERE status='closed' AND pnl IS NOT NULL AND recorder_id IN ({placeholders})
            ORDER BY closed_at ASC"""
        all_trades = PaperDBReader._query(combined_sql, tuple(premium_ids))

        combined_stats = {'total_trades': 0, 'win_rate': 0, 'profit_factor': 0, 'total_pnl': 0}
        if all_trades:
            pnls = [t['pnl'] for t in all_trades]
            total = len(pnls)
            winners = [p for p in pnls if p > 0]
            losers = [p for p in pnls if p < 0]
            gross_profit = sum(winners) if winners else 0
            gross_loss = abs(sum(losers)) if losers else 0
            combined_stats = {
                'total_trades': total,
                'win_rate': round((len(winners) / total * 100), 1) if total else 0,
                'profit_factor': round(gross_profit / gross_loss, 2) if gross_loss > 0 else ('Infinite' if gross_profit > 0 else 0),
                'total_pnl': round(sum(pnls), 2),
            }

        # Get equity curve for premium recorders
        eq_sql = f"""SELECT pnl, closed_at FROM paper_trades
            WHERE status='closed' AND pnl IS NOT NULL AND recorder_id IN ({placeholders})
            ORDER BY closed_at ASC LIMIT {{ph}}"""
        eq_rows = PaperDBReader._query(eq_sql, tuple(premium_ids) + (50,))
        equity_curve = []
        running = 0.0
        for r in eq_rows:
            running += r['pnl']
            equity_curve.append(round(running, 2))

        # Get recent trades from premium recorders
        rt_sql = f"""SELECT symbol, side, pnl, closed_at, recorder_id FROM paper_trades
            WHERE status='closed' AND pnl IS NOT NULL AND recorder_id IN ({placeholders})
            ORDER BY closed_at DESC LIMIT {{ph}}"""
        rt_rows = PaperDBReader._query(rt_sql, tuple(premium_ids) + (10,))
        recent_trades = []
        for r in rt_rows:
            recent_trades.append({
                'symbol': r.get('symbol', ''),
                'side': r.get('side', ''),
                'pnl': round(r.get('pnl', 0), 2),
            })

        return jsonify({
            'success': True,
            'strategies': strategies,
            'combined_stats': combined_stats,
            'recent_trades': recent_trades,
            'equity_curve': equity_curve
        })

    except Exception as e:
        logger.error(f"Error getting premium showcase: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/recorders/<int:recorder_id>/premium', methods=['POST', 'DELETE'])
@login_required
def api_set_recorder_premium(recorder_id):
    """Set or remove premium status for a recorder. ADMIN ONLY.

    POST: Mark as premium
    DELETE: Remove premium status
    """
    user = get_current_user()
    if not user or not user.is_admin:
        return jsonify({'success': False, 'error': 'Admin access required'}), 403

    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'

        is_premium = request.method == 'POST'
        premium_val = 'TRUE' if is_postgres else '1'
        not_premium_val = 'FALSE' if is_postgres else '0'

        cursor.execute(f'''
            UPDATE recorders
            SET is_premium = {premium_val if is_premium else not_premium_val}
            WHERE id = {ph}
        ''', (recorder_id,))

        conn.commit()
        conn.close()

        return jsonify({
            'success': True,
            'recorder_id': recorder_id,
            'is_premium': is_premium
        })

    except Exception as e:
        logger.error(f"Error setting premium status: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/paper-trades/equity-curve', methods=['GET'])
def api_paper_equity_curve():
    """Get equity curve data for charting (from paper_trades_v2)."""
    try:
        recorder_id = request.args.get('recorder_id', type=int)
        limit = request.args.get('limit', default=500, type=int)
        equity_curve = PaperDBReader.get_equity_curve(recorder_id, limit)
        return jsonify({
            'success': True,
            'recorder_id': recorder_id,
            'equity_curve': equity_curve
        })
    except Exception as e:
        logger.error(f"Error getting equity curve: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/daily-pnl', methods=['GET'])
def api_paper_daily_pnl():
    """Get daily P&L summary (from paper_trades_v2)."""
    try:
        recorder_id = request.args.get('recorder_id', type=int)
        days = request.args.get('days', default=30, type=int)
        daily_pnl = PaperDBReader.get_daily_pnl(recorder_id, days)
        return jsonify({
            'success': True,
            'recorder_id': recorder_id,
            'days': days,
            'daily_pnl': daily_pnl
        })
    except Exception as e:
        logger.error(f"Error getting daily P&L: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/symbol-stats', methods=['GET'])
def api_paper_symbol_stats():
    """Get P&L breakdown by symbol (from paper_trades_v2)."""
    try:
        recorder_id = request.args.get('recorder_id', type=int)
        symbol_stats = PaperDBReader.get_symbol_stats(recorder_id)
        return jsonify({
            'success': True,
            'recorder_id': recorder_id,
            'symbol_stats': symbol_stats
        })
    except Exception as e:
        logger.error(f"Error getting symbol stats: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/paper-trades/test-direct', methods=['POST'])
def api_paper_test_direct():
    """Test the direct paper trading function"""
    try:
        data = request.get_json() or {}
        recorder_id = data.get('recorder_id', 27)
        symbol = data.get('symbol', 'NQ')
        action = data.get('action', 'BUY')
        quantity = data.get('quantity', 1)
        price = data.get('price', 25000)

        result = _record_paper_trade_direct(recorder_id, symbol, action, quantity, price)
        return jsonify({'success': True, 'result': result})
    except Exception as e:
        import traceback
        return jsonify({'success': False, 'error': str(e), 'traceback': traceback.format_exc()}), 500


@app.route('/api/paper-trades/delete', methods=['POST'])
@api_login_required
def api_paper_delete_trade():
    """Delete a specific paper trade by ID (from paper_trades_v2)"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400

        trade_id = data.get('trade_id')
        if not trade_id:
            return jsonify({'success': False, 'error': 'trade_id required'}), 400

        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        conn = get_db_connection()
        cursor = conn.cursor()

        # Get trade info before deleting
        cursor.execute(f'SELECT recorder_id, symbol, side, pnl FROM paper_trades WHERE id = {ph}', (trade_id,))
        trade = cursor.fetchone()

        if not trade:
            conn.close()
            return jsonify({'success': False, 'error': f'Trade {trade_id} not found'}), 404

        # Delete the trade
        cursor.execute(f'DELETE FROM paper_trades WHERE id = {ph}', (trade_id,))
        conn.commit()
        conn.close()

        return jsonify({
            'success': True,
            'deleted_trade_id': trade_id,
            'message': f'Deleted trade {trade_id}'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/paper-trades/reset-all', methods=['POST'])
@api_login_required
def api_paper_reset_all():
    """Reset paper trades. Pass recorder_id to reset a single recorder, or omit for all."""
    try:
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        conn = get_db_connection()
        cursor = conn.cursor()

        # Check for recorder_id filter (query param or JSON body)
        recorder_id = request.args.get('recorder_id', type=int)
        if not recorder_id:
            data = request.get_json(silent=True) or {}
            recorder_id = data.get('recorder_id')

        if recorder_id:
            cursor.execute(f'SELECT COUNT(*) FROM paper_trades WHERE recorder_id = {ph}', (recorder_id,))
            row = cursor.fetchone()
            trades_count = row[0] if isinstance(row, (tuple, list)) else (row.get('count', 0) if isinstance(row, dict) else 0)
            cursor.execute(f'DELETE FROM paper_trades WHERE recorder_id = {ph}', (recorder_id,))
            conn.commit()
            conn.close()
            print(f"RESET: Deleted {trades_count} paper trades for recorder {recorder_id}", flush=True)
            return jsonify({
                'success': True,
                'deleted_trades': trades_count,
                'recorder_id': recorder_id,
                'message': f'Reset complete - deleted {trades_count} paper trades for recorder {recorder_id}'
            })
        else:
            cursor.execute('SELECT COUNT(*) FROM paper_trades')
            row = cursor.fetchone()
            trades_count = row[0] if isinstance(row, (tuple, list)) else (row.get('count', 0) if isinstance(row, dict) else 0)
            cursor.execute('DELETE FROM paper_trades')
            conn.commit()
            conn.close()
            print(f"RESET: Deleted {trades_count} paper trades (all)", flush=True)
            return jsonify({
                'success': True,
                'deleted_trades': trades_count,
                'message': f'Reset complete - deleted {trades_count} paper trades'
            })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/paper-trades/cleanup-stale', methods=['POST'])
@login_required
def api_paper_cleanup_stale():
    """Force-close stale open positions with P&L calculation.
    Pass ?close_all=true to close ALL open positions.
    Default: keeps 1 per recorder/symbol, closes duplicates only."""
    try:
        import os
        from datetime import datetime
        from tv_price_service import FUTURES_SPECS

        close_all = request.args.get('close_all', 'false').lower() == 'true'

        database_url = os.environ.get('DATABASE_URL')
        use_postgres = bool(database_url)
        if use_postgres:
            import psycopg2
            conn = psycopg2.connect(database_url)
            ph = '%s'
        else:
            conn = sqlite3.connect('paper_trades.db')
            ph = '?'
        cursor = conn.cursor()
        now = datetime.now().isoformat()

        # Get all open positions grouped by recorder_id + symbol
        cursor.execute('SELECT id, recorder_id, symbol, side, quantity, entry_price, tp_price, sl_price, opened_at FROM paper_trades WHERE status = %s ORDER BY opened_at DESC' if use_postgres else "SELECT id, recorder_id, symbol, side, quantity, entry_price, tp_price, sl_price, opened_at FROM paper_trades WHERE status = ? ORDER BY opened_at DESC", ('open',))
        all_open = cursor.fetchall()

        from collections import defaultdict
        groups = defaultdict(list)
        for row in all_open:
            key = (row[1], row[2])  # (recorder_id, symbol)
            groups[key].append(row)

        closed_count = 0
        kept_count = 0

        for (rec_id, sym), positions in groups.items():
            # Get live price for P&L calc
            live_price = _get_live_price_for_symbol(sym)
            if not live_price:
                live_price = get_market_price_simple(sym)
            spec = FUTURES_SPECS.get(extract_symbol_root(sym), {'point_value': 1.0})
            point_value = spec['point_value']

            if close_all:
                # Close ALL open positions
                to_close = positions
            else:
                # Keep newest, close duplicates only
                if len(positions) <= 1:
                    kept_count += 1
                    continue
                kept_count += 1
                to_close = positions[1:]

            for pos in to_close:
                pos_id, _, _, pos_side, pos_qty, pos_entry, _, _, _ = pos
                exit_px = live_price if live_price else pos_entry
                if pos_side == 'LONG':
                    pnl = (exit_px - pos_entry) * point_value * pos_qty
                else:
                    pnl = (pos_entry - exit_px) * point_value * pos_qty

                cursor.execute(f'''
                    UPDATE paper_trades SET status = 'closed', exit_price = {ph}, pnl = {ph},
                    exit_reason = {ph}, closed_at = {ph}
                    WHERE id = {ph}
                ''', (exit_px, round(pnl, 2), 'cleanup', now, pos_id))
                closed_count += 1

        conn.commit()
        conn.close()

        mode = 'all positions' if close_all else 'duplicates only'
        logger.info(f"Paper cleanup ({mode}): closed {closed_count}, kept {kept_count}")
        return jsonify({
            'success': True,
            'closed': closed_count,
            'kept': kept_count,
            'message': f'Closed {closed_count} positions ({mode}), kept {kept_count}'
        })
    except Exception as e:
        import traceback
        logger.error(f"Paper cleanup error: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/paper-trades/record', methods=['POST'])
def api_paper_record_trade():
    """
    Record a paper trade (entry or exit) ‚Äî proxies to paper service on port 5050.

    POST body:
    {
        "recorder_id": 1,
        "symbol": "NQ",
        "action": "LONG" | "SHORT" | "CLOSE",
        "quantity": 1,
        "price": 21500.50  (optional - uses live feed if not provided)
    }
    """
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400

        recorder_id = data.get('recorder_id')
        symbol = data.get('symbol', '').upper()
        action = data.get('action', '').upper()

        if not recorder_id or not symbol or not action:
            return jsonify({'success': False, 'error': 'Missing required fields: recorder_id, symbol, action'}), 400

        # Try proxying to paper service
        try:
            import requests as http_requests
            resp = http_requests.post('http://127.0.0.1:5050/api/paper/record', json=data, timeout=5)
            return jsonify(resp.json()), resp.status_code
        except Exception as proxy_err:
            logger.warning(f"Paper service proxy failed: {proxy_err}")

        # Fallback: use old engine if available
        if TV_PRICE_SERVICE_AVAILABLE:
            paper_engine = get_paper_engine()
            ticker = get_ticker()
            quantity = data.get('quantity', 1)
            price = data.get('price')

            if not price:
                tv_symbol = f"CME_MINI:{symbol}1!"
                price_data = ticker.get_price(tv_symbol)
                if price_data:
                    if action in ['LONG', 'BUY']:
                        price = price_data.get('ask') or price_data.get('last_price')
                    elif action in ['SHORT', 'SELL']:
                        price = price_data.get('bid') or price_data.get('last_price')
                    else:
                        price = price_data.get('last_price')

            if action in ['LONG', 'BUY']:
                result = paper_engine.open_position(recorder_id, symbol, 'LONG', quantity, price)
                return jsonify({'success': True, 'action': 'opened', 'position': result})
            elif action in ['SHORT', 'SELL']:
                result = paper_engine.open_position(recorder_id, symbol, 'SHORT', quantity, price)
                return jsonify({'success': True, 'action': 'opened', 'position': result})
            elif action in ['CLOSE', 'EXIT', 'FLAT']:
                result = paper_engine.close_position(recorder_id, symbol, price)
                if result:
                    return jsonify({'success': True, 'action': 'closed', 'trade': result})
                else:
                    return jsonify({'success': False, 'error': 'No open position to close'}), 400
            else:
                return jsonify({'success': False, 'error': f'Unknown action: {action}'}), 400
        else:
            return jsonify({'success': False, 'error': 'Paper trading service not available'}), 503

    except Exception as e:
        logger.error(f"Error recording paper trade: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/futures-specs', methods=['GET'])
def api_futures_specs():
    """Get futures contract specifications."""
    if not TV_PRICE_SERVICE_AVAILABLE:
        return jsonify({'success': False, 'error': 'Price service not available'}), 503

    try:
        from tv_price_service import FUTURES_SPECS, get_futures_spec

        symbol = request.args.get('symbol')
        if symbol:
            spec = get_futures_spec(symbol)
            return jsonify({'success': True, 'symbol': symbol, 'spec': spec})
        else:
            return jsonify({'success': True, 'specs': FUTURES_SPECS})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/stock-heatmap', methods=['GET'])
def api_stock_heatmap():
    """Get stock heatmap data from Finnhub (primary) or Yahoo Finance (fallback)"""
    try:
        # Check if Finnhub API key is set (optional - falls back to Yahoo if not)
        finnhub_api_key = os.environ.get('FINNHUB_API_KEY', None)
        
        # Try Finnhub first if API key is available
        if finnhub_api_key:
            try:
                return get_finnhub_heatmap_data(finnhub_api_key)
            except Exception as e:
                logger.warning(f"Finnhub API failed, falling back to Yahoo Finance: {e}")
        
        # Fallback to Yahoo Finance (current implementation)
        return get_yahoo_heatmap_data()
    except Exception as e:
        logger.error(f"Error fetching heatmap data: {e}")
        return get_sample_heatmap_data()

def get_finnhub_heatmap_data(api_key):
    """Fetch stock data from Finnhub API"""
    symbols_with_cap = [
        {'symbol': 'NVDA', 'market_cap': 3000},
        {'symbol': 'MSFT', 'market_cap': 3200},
        {'symbol': 'AAPL', 'market_cap': 3500},
        {'symbol': 'GOOGL', 'market_cap': 2000},
        {'symbol': 'AMZN', 'market_cap': 1900},
        {'symbol': 'META', 'market_cap': 1300},
        {'symbol': 'TSLA', 'market_cap': 800},
        {'symbol': 'AVGO', 'market_cap': 600},
        {'symbol': 'ORCL', 'market_cap': 500},
        {'symbol': 'AMD', 'market_cap': 300},
        {'symbol': 'NFLX', 'market_cap': 280},
        {'symbol': 'CSCO', 'market_cap': 250},
        {'symbol': 'INTC', 'market_cap': 200},
        {'symbol': 'MU', 'market_cap': 150},
        {'symbol': 'PLTR', 'market_cap': 50},
        {'symbol': 'HOOD', 'market_cap': 20},
    ]
    
    heatmap_data = []
    successful_fetches = 0
    
    for stock_info in symbols_with_cap[:16]:
        symbol = stock_info['symbol']
        market_cap = stock_info['market_cap']
        
        try:
            # Finnhub quote endpoint
            url = f'https://finnhub.io/api/v1/quote?symbol={symbol}&token={api_key}'
            response = requests.get(url, timeout=3)
            
            if response.status_code == 200:
                data = response.json()
                current_price = data.get('c', 0)  # Current price
                previous_close = data.get('pc', current_price)  # Previous close
                
                # Finnhub returns change percentage directly as 'dp' (daily percent change)
                change_pct_raw = data.get('dp', None)
                
                if change_pct_raw is not None:
                    # Finnhub returns percentage directly (e.g., 1.65 for 1.65%)
                    change_pct = change_pct_raw
                elif current_price > 0 and previous_close > 0 and previous_close != current_price:
                    # Calculate from price difference
                    change_pct = ((current_price - previous_close) / previous_close) * 100
                else:
                    change_pct = 0
                
                if current_price > 0:
                    
                    # Get market cap from company profile
                    profile_url = f'https://finnhub.io/api/v1/stock/profile2?symbol={symbol}&token={api_key}'
                    profile_response = requests.get(profile_url, timeout=2)
                    real_market_cap = market_cap  # Default to provided market cap
                    
                    if profile_response.status_code == 200:
                        profile = profile_response.json()
                        if 'marketCapitalization' in profile:
                            finnhub_market_cap = profile['marketCapitalization']
                            # Finnhub returns market cap in raw number, convert to billions
                            if finnhub_market_cap and finnhub_market_cap > 1000:  # Sanity check
                                real_market_cap = finnhub_market_cap / 1_000_000_000  # Convert to billions
                            # If the value seems wrong (too small), use fallback
                            if real_market_cap < 10:  # If less than 10B, it's probably wrong
                                real_market_cap = market_cap  # Use provided fallback
                    
                    heatmap_data.append({
                        'symbol': symbol,
                        'price': round(current_price, 2),
                        'change': round(change_pct, 2),
                        'change_pct': f"{'+' if change_pct >= 0 else ''}{round(change_pct, 2)}%",
                        'market_cap': real_market_cap
                    })
                    successful_fetches += 1
                    logger.info(f"Finnhub: Successfully fetched {symbol}: ${current_price:.2f} ({change_pct:+.2f}%)")
        except Exception as e:
            logger.warning(f"Error fetching {symbol} from Finnhub: {e}")
            continue
    
    logger.info(f"Finnhub API: Successfully fetched {successful_fetches} stocks")
    
    if heatmap_data:
        return jsonify({'stocks': heatmap_data})
    else:
        raise Exception("No data from Finnhub")

def get_yahoo_heatmap_data():
    """Get stock heatmap data from Yahoo Finance (fallback)"""
    try:
        # Most active tech stocks with approximate market cap order (largest first)
        # Market cap data for sizing the treemap
        symbols_with_cap = [
            {'symbol': 'NVDA', 'market_cap': 3000},  # Largest - top left
            {'symbol': 'MSFT', 'market_cap': 3200},
            {'symbol': 'AAPL', 'market_cap': 3500},
            {'symbol': 'GOOGL', 'market_cap': 2000},
            {'symbol': 'AMZN', 'market_cap': 1900},
            {'symbol': 'META', 'market_cap': 1300},
            {'symbol': 'TSLA', 'market_cap': 800},
            {'symbol': 'AVGO', 'market_cap': 600},
            {'symbol': 'ORCL', 'market_cap': 500},
            {'symbol': 'AMD', 'market_cap': 300},
            {'symbol': 'NFLX', 'market_cap': 280},
            {'symbol': 'CSCO', 'market_cap': 250},
            {'symbol': 'INTC', 'market_cap': 200},
            {'symbol': 'MU', 'market_cap': 150},
            {'symbol': 'PLTR', 'market_cap': 50},
            {'symbol': 'HOOD', 'market_cap': 20},
        ]
        
        # Fetch data from Yahoo Finance (using their public API)
        heatmap_data = []
        successful_fetches = 0
        for stock_info in symbols_with_cap[:16]:  # Limit to 16 for treemap layout
            symbol = stock_info['symbol']
            market_cap = stock_info['market_cap']
            try:
                # Yahoo Finance quote endpoint (no API key needed)
                url = f'https://query1.finance.yahoo.com/v8/finance/chart/{symbol}?interval=1d&range=1d'
                response = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
                if response.status_code == 200:
                    data = response.json()
                    if 'chart' in data and 'result' in data['chart'] and len(data['chart']['result']) > 0:
                        result = data['chart']['result'][0]
                        if 'meta' in result:
                            meta = result['meta']
                            current_price = meta.get('regularMarketPrice', 0)
                            # Yahoo Finance chart API uses 'chartPreviousClose', not 'previousClose'
                            previous_close = meta.get('chartPreviousClose') or meta.get('previousClose', 0)
                            
                            # Calculate change percentage from prices
                            if current_price > 0 and previous_close > 0:
                                change_pct = ((current_price - previous_close) / previous_close) * 100
                            else:
                                change_pct = 0
                            
                            # Try to get real market cap from Yahoo Finance
                            real_market_cap = meta.get('marketCap', None)
                            if real_market_cap:
                                # Convert to billions for easier comparison
                                market_cap_billions = real_market_cap / 1_000_000_000
                            else:
                                # Fallback to provided market cap
                                market_cap_billions = market_cap
                            
                            if current_price > 0:
                                heatmap_data.append({
                                    'symbol': symbol,
                                    'price': round(current_price, 2),
                                    'change': round(change_pct, 2),
                                    'change_pct': f"{'+' if change_pct >= 0 else ''}{round(change_pct, 2)}%",
                                    'market_cap': market_cap_billions  # Real market cap in billions
                                })
                                successful_fetches += 1
                                logger.info(f"Successfully fetched {symbol}: ${current_price:.2f} ({change_pct:+.2f}%)")
            except Exception as e:
                logger.warning(f"Error fetching data for {symbol}: {e}")
                continue
        
        logger.info(f"Yahoo Finance API: Successfully fetched {successful_fetches} stocks out of {len(symbols_with_cap[:16])}")
        
        if heatmap_data:
            return jsonify({'stocks': heatmap_data})
        else:
            raise Exception("No data from Yahoo Finance")
    except Exception as e:
        logger.error(f"Error in Yahoo Finance API: {e}")
        raise

def get_sample_heatmap_data():
    """Return sample data as last resort"""
    return jsonify({
        'stocks': [
            {'symbol': 'NVDA', 'price': 189.94, 'change': 1.65, 'change_pct': '+1.65%', 'market_cap': 3000},
            {'symbol': 'MSFT', 'price': 428.50, 'change': 1.29, 'change_pct': '+1.29%', 'market_cap': 3200},
            {'symbol': 'AAPL', 'price': 189.94, 'change': 1.65, 'change_pct': '+1.65%', 'market_cap': 3500},
            {'symbol': 'GOOGL', 'price': 175.20, 'change': -0.16, 'change_pct': '-0.16%', 'market_cap': 2000},
            {'symbol': 'AMZN', 'price': 185.30, 'change': -0.11, 'change_pct': '-0.11%', 'market_cap': 1900},
            {'symbol': 'META', 'price': 512.80, 'change': 0.28, 'change_pct': '+0.28%', 'market_cap': 1300},
            {'symbol': 'TSLA', 'price': 408.83, 'change': 1.70, 'change_pct': '+1.70%', 'market_cap': 800},
            {'symbol': 'AVGO', 'price': 150.20, 'change': 1.20, 'change_pct': '+1.20%', 'market_cap': 600},
            {'symbol': 'ORCL', 'price': 145.30, 'change': 3.87, 'change_pct': '+3.87%', 'market_cap': 500},
            {'symbol': 'AMD', 'price': 185.50, 'change': 1.91, 'change_pct': '+1.91%', 'market_cap': 300},
        ]
    })

@app.route('/webhooks', methods=['POST'])
@admin_or_api_key_required
def create_webhook():
    data = request.get_json()
    url = data.get('url')
    method = data.get('method')
    headers = data.get('headers')
    body = data.get('body')

    if not url or not method:
        return jsonify({'error': 'URL and method are required'}), 400

    conn = get_db_connection()
    cursor = conn.cursor()
    is_postgres = is_using_postgres()
    
    if is_postgres:
        cursor.execute('''
            INSERT INTO webhooks (url, method, headers, body)
            VALUES (%s, %s, %s, %s)
        ''', (url, method, headers, body))
    else:
        cursor.execute('''
            INSERT INTO webhooks (url, method, headers, body)
            VALUES (?, ?, ?, ?)
        ''', (url, method, headers, body))
    conn.commit()
    conn.close()

    return jsonify({'message': 'Webhook created successfully'}), 201

@app.route('/webhooks', methods=['GET'])
@admin_or_api_key_required
def get_webhooks():
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM webhooks')
    webhooks = cursor.fetchall()
    conn.close()

    return jsonify([{
        'id': w[0],
        'url': w[1],
        'method': w[2],
        'headers': w[3],
        'body': w[4],
        'created_at': str(w[5]) if w[5] else None
    } for w in webhooks])

@app.route('/webhooks/<int:id>', methods=['GET'])
@admin_or_api_key_required
def get_webhook(id):
    conn = get_db_connection()
    cursor = conn.cursor()
    is_postgres = is_using_postgres()
    
    if is_postgres:
        cursor.execute('SELECT * FROM webhooks WHERE id = %s', (id,))
    else:
        cursor.execute('SELECT * FROM webhooks WHERE id = ?', (id,))
    webhook = cursor.fetchone()
    conn.close()

    if webhook:
        return jsonify({
            'id': webhook[0],
            'url': webhook[1],
            'method': webhook[2],
            'headers': webhook[3],
            'body': webhook[4],
            'created_at': str(webhook[5]) if webhook[5] else None
        })
    return jsonify({'error': 'Webhook not found'}), 404

@app.route('/webhooks/<int:id>', methods=['DELETE'])
@admin_or_api_key_required
def delete_webhook(id):
    conn = get_db_connection()
    cursor = conn.cursor()
    is_postgres = is_using_postgres()
    
    if is_postgres:
        cursor.execute('DELETE FROM webhooks WHERE id = %s', (id,))
    else:
        cursor.execute('DELETE FROM webhooks WHERE id = ?', (id,))
    conn.commit()
    conn.close()
    return '', 204

# Initialize database on import (for gunicorn)
try:
    init_db()
except Exception as e:
    logger.warning(f"Database initialization warning: {e}")

# Initialize user authentication system
if USER_AUTH_AVAILABLE:
    try:
        init_auth_system()
        # Create initial admin user if no users exist
        create_initial_admin()
        logger.info("‚úÖ User authentication system initialized")
    except Exception as e:
        logger.warning(f"Auth system initialization warning: {e}")

# ============================================================================
# WebSocket Handlers (Real-time updates like Trade Manager)
# ============================================================================

@socketio.on('connect')
def handle_connect():
    """Handle WebSocket connection - requires authentication"""
    if USER_AUTH_AVAILABLE and not is_logged_in():
        logger.warning('Rejected unauthenticated WebSocket connection')
        return False
    logger.info('Client connected to WebSocket')
    emit('status', {
        'connected': True,
        'message': 'Connected to server',
        'timestamp': datetime.now().isoformat()
    })

@socketio.on('disconnect')
def handle_disconnect():
    """Handle WebSocket disconnection"""
    logger.info('Client disconnected from WebSocket')

@socketio.on('subscribe')
def handle_subscribe(data):
    """Subscribe to specific update channels"""
    channels = data.get('channels', [])
    logger.info(f'Client subscribed to: {channels}')
    emit('subscribed', {'channels': channels})

# ============================================================================
# Strategy P&L Recording Functions
# ============================================================================

def record_strategy_pnl(strategy_id, strategy_name, pnl, drawdown=0.0):
    """Record strategy P&L to database (like Trade Manager)"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            cursor.execute('''
                INSERT INTO strategy_pnl_history (strategy_id, strategy_name, pnl, drawdown, timestamp)
                VALUES (%s, %s, %s, %s, %s)
            ''', (strategy_id, strategy_name, pnl, drawdown, datetime.now()))
        else:
            cursor.execute('''
                INSERT INTO strategy_pnl_history (strategy_id, strategy_name, pnl, drawdown, timestamp)
                VALUES (?, ?, ?, ?, ?)
            ''', (strategy_id, strategy_name, pnl, drawdown, datetime.now()))
        conn.commit()
        conn.close()
    except Exception as e:
        logger.error(f"Error recording strategy P&L: {e}")

def calculate_strategy_pnl(strategy_id):
    """Calculate current P&L for a strategy (recorder) from recorded_trades table
    
    FIXED: Now uses recorded_trades table (where webhook signals store trades)
    instead of the old 'trades' table.
    
    strategy_id = recorder_id in our system
    """
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Get realized P&L from closed trades in recorded_trades
        cursor.execute(f'''
            SELECT COALESCE(SUM(pnl), 0.0) as realized_pnl
            FROM recorded_trades
            WHERE recorder_id = {placeholder} AND status = 'closed'
        ''', (strategy_id,))
        result = cursor.fetchone()
        realized_pnl = float(result[0] or 0.0) if result else 0.0
        
        # Get unrealized P&L from open positions in recorder_positions
        # Uses TradingView price data for real-time accuracy
        cursor.execute(f'''
            SELECT COALESCE(SUM(unrealized_pnl), 0.0) as unrealized_pnl
            FROM recorder_positions
            WHERE recorder_id = {placeholder} AND status = 'open'
        ''', (strategy_id,))
        result = cursor.fetchone()
        unrealized_pnl = float(result[0] or 0.0) if result else 0.0
        
        conn.close()
        
        total_pnl = realized_pnl + unrealized_pnl
        logger.debug(f"üìä Strategy {strategy_id} P&L: realized=${realized_pnl:.2f} + unrealized=${unrealized_pnl:.2f} = ${total_pnl:.2f}")
        
        return total_pnl
            
    except Exception as e:
        logger.error(f"Error calculating strategy P&L: {e}")
        return 0.0

def calculate_strategy_drawdown(strategy_id):
    """Calculate current drawdown for a strategy"""
    try:
        # Get historical P&L to calculate drawdown
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        
        if is_postgres:
            cursor.execute('''
                SELECT pnl FROM strategy_pnl_history
                WHERE strategy_id = %s
                ORDER BY timestamp DESC
                LIMIT 100
            ''', (strategy_id,))
        else:
            cursor.execute('''
                SELECT pnl FROM strategy_pnl_history
                WHERE strategy_id = ?
                ORDER BY timestamp DESC
                LIMIT 100
            ''', (strategy_id,))
        
        pnl_history = [float(row[0]) for row in cursor.fetchall() if row[0] is not None]
        conn.close()
        
        if not pnl_history:
            return 0.0
        
        # Calculate drawdown: peak - current
        peak = max(pnl_history)
        current = pnl_history[0] if pnl_history else 0.0
        drawdown = max(0.0, peak - current)
        
        return drawdown
        
    except Exception as e:
        logger.error(f"Error calculating strategy drawdown: {e}")
        return 0.0

# ============================================================================
# Background Threads for Real-Time Updates (Every Second, like Trade Manager)
# ============================================================================

# Global position cache to persist positions across updates
_position_cache = {}

# Market data cache for real-time prices
_market_data_cache = {}

# Market data WebSocket connection
_market_data_ws = None
_market_data_ws_task = None
_market_data_subscribed_symbols = set()

async def connect_tradovate_market_data_websocket():
    """Connect to Tradovate market data WebSocket and subscribe to quotes"""
    global _market_data_cache, _market_data_ws, _market_data_subscribed_symbols
    
    if not WEBSOCKETS_AVAILABLE:
        logger.error("websockets library not available. Cannot connect to market data.")
        return
    
    # Get access token from database - try md_access_token first, then fall back to regular accessToken
    md_token = None
    demo = True  # Default to demo
    account_id = None
    token_source = None

    try:
        conn = get_db_connection()
        is_postgres = is_using_postgres()

        if is_postgres:
            cursor = conn.cursor()
            # First try to find account with md_access_token
            cursor.execute('''
                SELECT id, md_access_token, environment, tradovate_token FROM accounts
                WHERE md_access_token IS NOT NULL AND md_access_token != ''
                LIMIT 1
            ''')
            row = cursor.fetchone()

            if row:
                account_id = row['id'] if isinstance(row, dict) else row[0]
                md_token = row['md_access_token'] if isinstance(row, dict) else row[1]
                env = (row['environment'] if isinstance(row, dict) else row[2]) or 'demo'
                demo = (env == 'demo' or env is None)
                token_source = 'md_access_token'
            else:
                # Fall back to regular accessToken
                cursor.execute('''
                    SELECT id, tradovate_token, environment FROM accounts
                    WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
                    LIMIT 1
                ''')
                row = cursor.fetchone()
                if row:
                    account_id = row['id'] if isinstance(row, dict) else row[0]
                    md_token = row['tradovate_token'] if isinstance(row, dict) else row[1]
                    env = (row['environment'] if isinstance(row, dict) else row[2]) or 'demo'
                    demo = (env == 'demo' or env is None)
                    token_source = 'accessToken (fallback)'
        else:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            # First try to find account with md_access_token
            cursor.execute('''
                SELECT id, md_access_token, environment, tradovate_token FROM accounts
                WHERE md_access_token IS NOT NULL AND md_access_token != ''
                LIMIT 1
            ''')
            row = cursor.fetchone()

            if row:
                account_id = row['id']
                md_token = row['md_access_token']
                env = row['environment'] if row['environment'] else 'demo'
                demo = (env == 'demo' or env is None)
                token_source = 'md_access_token'
            else:
                # Fall back to regular accessToken
                cursor.execute('''
                    SELECT id, tradovate_token, environment FROM accounts
                    WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
                    LIMIT 1
                ''')
                row = cursor.fetchone()
                if row:
                    account_id = row['id']
                    md_token = row['tradovate_token']
                    env = row['environment'] if row['environment'] else 'demo'
                    demo = (env == 'demo' or env is None)
                    token_source = 'accessToken (fallback)'

        # Validate and refresh token if needed
        if account_id:
            valid_token = get_valid_tradovate_token(account_id)
            if valid_token and valid_token != md_token:
                md_token = valid_token
                logger.info(f"Using refreshed token for account {account_id}")
            elif not valid_token:
                logger.warning(f"Account {account_id} has no valid access token - WebSocket may fail")
        conn.close()
    except Exception as e:
        logger.error(f"Error fetching access token for market data: {e}")
        logger.error(traceback.format_exc())
        return

    if not md_token:
        logger.warning("No access token found for market data. WebSocket will not connect.")
        logger.info("To enable market data: Connect a Tradovate account via OAuth on the site.")
        return

    logger.info(f"Using {token_source} from account {account_id} for market data WebSocket")
    
    # WebSocket URL (demo or live)
    ws_url = "wss://demo.tradovateapi.com/v1/websocket" if demo else "wss://live.tradovateapi.com/v1/websocket"
    
    while True:
        try:
            logger.info(f"Connecting to Tradovate market data WebSocket: {ws_url}")
            async with websockets.connect(ws_url) as ws:
                _market_data_ws = ws
                logger.info("‚úÖ Market data WebSocket connected")
                
                # Authorize with md_access_token
                # Format: "authorize\n0\n\n{token}"
                auth_message = f"authorize\n0\n\n{md_token}"
                await ws.send(auth_message)
                
                # Wait for authorization response
                response = await ws.recv()
                logger.info(f"Market data auth response: {response[:200]}")
                
                # Subscribe to quotes for symbols we have positions in
                await subscribe_to_market_data_symbols(ws)
                
                # Listen for market data updates
                async for message in ws:
                    try:
                        # Parse message (format: "frame\n{id}\n\n{json_data}")
                        if message.startswith("frame"):
                            parts = message.split("\n", 3)
                            if len(parts) >= 4:
                                json_data = json.loads(parts[3])
                                await process_market_data_message(json_data)
                        elif message.startswith("["):
                            # Direct JSON array format
                            data = json.loads(message)
                            await process_market_data_message(data)
                    except json.JSONDecodeError as e:
                        logger.debug(f"Could not parse market data message: {e}")
                    except Exception as e:
                        logger.warning(f"Error processing market data message: {e}")
                        
        except websockets.exceptions.ConnectionClosed:
            logger.warning("Market data WebSocket connection closed. Reconnecting in 5 seconds...")
            # Validate tokens before reconnecting
            if account_id:
                get_valid_tradovate_token(account_id)  # Auto-refresh if needed
            await asyncio.sleep(5)
        except Exception as e:
            error_str = str(e)
            # Check for rate limiting (429)
            if '429' in error_str:
                logger.error(f"Market data WebSocket rate limited (429). Backing off for 120 seconds...")
                await asyncio.sleep(120)  # Long backoff for rate limiting
            else:
                logger.error(f"Market data WebSocket error: {e}. Reconnecting in 30 seconds...")
                # Validate tokens before reconnecting
                if account_id:
                    get_valid_tradovate_token(account_id)  # Auto-refresh if needed
                await asyncio.sleep(30)  # Increased from 10 to 30 seconds

async def subscribe_to_market_data_symbols(ws):
    """Subscribe to market data quotes for symbols we have positions in AND open recorder trades"""
    global _position_cache, _market_data_subscribed_symbols
    
    # Get symbols from positions
    symbols_to_subscribe = set()
    for position in _position_cache.values():
        symbol = position.get('symbol', '')
        if symbol:
            # Convert TradingView symbol to Tradovate format if needed
            # MES1! -> MESM1 (front month)
            tradovate_symbol = convert_symbol_for_tradovate_md(symbol)
            symbols_to_subscribe.add(tradovate_symbol)
    
    # Also check database for open positions
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT DISTINCT symbol FROM open_positions WHERE symbol IS NOT NULL")
        for row in cursor.fetchall():
            symbol = row[0]
            if symbol:
                tradovate_symbol = convert_symbol_for_tradovate_md(symbol)
                symbols_to_subscribe.add(tradovate_symbol)
        conn.close()
    except Exception as e:
        logger.warning(f"Error getting symbols from database: {e}")
    
    # Get symbols from open recorder trades (for TP/SL monitoring)
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT DISTINCT ticker FROM recorded_trades 
            WHERE status = 'open' AND ticker IS NOT NULL
        ''')
        for row in cursor.fetchall():
            symbol = row[0]
            if symbol:
                tradovate_symbol = convert_symbol_for_tradovate_md(symbol)
                symbols_to_subscribe.add(tradovate_symbol)
                logger.info(f"Adding recorder trade symbol to market data subscription: {symbol} -> {tradovate_symbol}")
        conn.close()
    except Exception as e:
        logger.warning(f"Error getting recorder trade symbols: {e}")
    
    # Subscribe to each symbol
    for symbol in symbols_to_subscribe:
        if symbol not in _market_data_subscribed_symbols:
            try:
                # Subscribe to quote data
                # Format: "quote/subscribe\n{id}\n\n{json}"
                subscribe_msg = f"quote/subscribe\n1\n\n{json.dumps({'symbol': symbol})}"
                await ws.send(subscribe_msg)
                _market_data_subscribed_symbols.add(symbol)
                logger.info(f"Subscribed to market data for {symbol}")
            except Exception as e:
                logger.warning(f"Error subscribing to {symbol}: {e}")

def convert_symbol_for_tradovate_md(symbol: str) -> str:
    """Convert symbol format for Tradovate market data (MES1! -> MESM1)"""
    # Remove ! and convert month codes
    symbol = symbol.upper().replace('!', '')
    # If it ends with a number, it's already in Tradovate format
    if symbol[-1].isdigit():
        return symbol
    # Otherwise, try to get front month (simplified - you may need contract lookup)
    # For now, just return the symbol as-is
    return symbol

async def process_market_data_message(data):
    """Process incoming market data message and update cache"""
    global _market_data_cache
    
    try:
        symbols_updated = set()
        
        # Handle different message formats
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    symbol = item.get('symbol') or item.get('s')
                    if symbol:
                        # Update cache with latest price under BOTH full symbol and root symbol
                        # This ensures lookups work from both webhook (root) and paper trades (root)
                        if symbol not in _market_data_cache:
                            _market_data_cache[symbol] = {}

                        # Also store under root symbol (e.g., "CME_MINI:MNQ1!" -> "MNQ")
                        root_symbol = extract_symbol_root(symbol)
                        if root_symbol and root_symbol not in _market_data_cache:
                            _market_data_cache[root_symbol] = {}

                        # Extract price data
                        last = item.get('last') or item.get('lastPrice') or item.get('l')
                        bid = item.get('bid') or item.get('b')
                        ask = item.get('ask') or item.get('a')

                        if last:
                            _market_data_cache[symbol]['last'] = float(last)
                            if root_symbol:
                                _market_data_cache[root_symbol]['last'] = float(last)
                            symbols_updated.add(symbol)
                            if root_symbol:
                                symbols_updated.add(root_symbol)
                        if bid:
                            _market_data_cache[symbol]['bid'] = float(bid)
                            if root_symbol:
                                _market_data_cache[root_symbol]['bid'] = float(bid)
                        if ask:
                            _market_data_cache[symbol]['ask'] = float(ask)
                            if root_symbol:
                                _market_data_cache[root_symbol]['ask'] = float(ask)
                        
        elif isinstance(data, dict):
            symbol = data.get('symbol') or data.get('s')
            if symbol:
                if symbol not in _market_data_cache:
                    _market_data_cache[symbol] = {}

                # Also store under root symbol (e.g., "CME_MINI:MNQ1!" -> "MNQ")
                root_symbol = extract_symbol_root(symbol)
                if root_symbol and root_symbol not in _market_data_cache:
                    _market_data_cache[root_symbol] = {}

                last = data.get('last') or data.get('lastPrice') or data.get('l')
                bid = data.get('bid') or data.get('b')
                ask = data.get('ask') or data.get('a')

                if last:
                    _market_data_cache[symbol]['last'] = float(last)
                    if root_symbol:
                        _market_data_cache[root_symbol]['last'] = float(last)
                    symbols_updated.add(symbol)
                    if root_symbol:
                        symbols_updated.add(root_symbol)
                if bid:
                    _market_data_cache[symbol]['bid'] = float(bid)
                    if root_symbol:
                        _market_data_cache[root_symbol]['bid'] = float(bid)
                if ask:
                    _market_data_cache[symbol]['ask'] = float(ask)
                    if root_symbol:
                        _market_data_cache[root_symbol]['ask'] = float(ask)
        
        # Push to SSE subscribers (dashboard real-time)
        for _sse_sym in symbols_updated:
            if ':' not in _sse_sym:
                _broadcast_sse_price(_sse_sym)

        # Update PnL for positions with this symbol
        update_position_pnl()

        # Check TP/SL for open recorder trades
        if symbols_updated:
            check_recorder_trades_tp_sl(symbols_updated)
                
    except Exception as e:
        logger.warning(f"Error processing market data: {e}")

def check_recorder_trades_tp_sl(symbols_updated: set):
    """
    Check open recorder trades for TP/SL hits based on current market prices.
    This is called on every market data update for real-time TP/SL monitoring.
    """
    global _market_data_cache
    
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get all open trades that have TP or SL set
        # SKIP broker_managed_tp_sl=1 trades - broker handles those exits, not us
        cursor.execute('''
            SELECT t.*, r.name as recorder_name
            FROM recorded_trades t
            JOIN recorders r ON t.recorder_id = r.id
            WHERE t.status = 'open'
              AND (t.tp_price IS NOT NULL OR t.sl_price IS NOT NULL)
              AND (t.broker_managed_tp_sl IS NULL OR t.broker_managed_tp_sl = FALSE)
        ''')
        
        open_trades = [dict(row) for row in cursor.fetchall()]
        
        for trade in open_trades:
            ticker = trade['ticker']
            
            # Normalize ticker for market data lookup (MNQ1! -> MNQ, etc.)
            ticker_root = extract_symbol_root(ticker) if ticker else None
            if not ticker_root:
                continue
            
            # Find price in cache (try various symbol formats)
            current_price = None
            for cached_symbol in _market_data_cache.keys():
                if ticker_root in cached_symbol.upper():
                    current_price = _market_data_cache[cached_symbol].get('last')
                    if current_price:
                        break
            
            if not current_price:
                continue
            
            # =====================================================
            # MFE/MAE TRACKING - Update on every price tick
            # =====================================================
            side = trade['side']
            entry_price = trade['entry_price']
            tick_size = get_tick_size(ticker)
            tick_value = get_tick_value(ticker)
            
            # Get current MFE/MAE values
            current_max_favorable = trade.get('max_favorable') or 0
            current_max_adverse = trade.get('max_adverse') or 0
            
            # Calculate current excursion based on trade direction
            if side == 'LONG':
                # For LONG: favorable = price went UP, adverse = price went DOWN
                favorable_excursion = max(0, current_price - entry_price)
                adverse_excursion = max(0, entry_price - current_price)
            else:  # SHORT
                # For SHORT: favorable = price went DOWN, adverse = price went UP
                favorable_excursion = max(0, entry_price - current_price)
                adverse_excursion = max(0, current_price - entry_price)
            
            # Update MFE/MAE if we have new highs/lows
            new_max_favorable = max(current_max_favorable, favorable_excursion)
            new_max_adverse = max(current_max_adverse, adverse_excursion)
            
            # Only update database if values changed (to reduce DB writes)
            if new_max_favorable != current_max_favorable or new_max_adverse != current_max_adverse:
                cursor.execute('''
                    UPDATE recorded_trades 
                    SET max_favorable = ?, max_adverse = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE id = ?
                ''', (new_max_favorable, new_max_adverse, trade['id']))
                conn.commit()
                
                # Log significant drawdown events (when MAE increases by more than 2 ticks)
                if new_max_adverse > current_max_adverse:
                    mae_ticks = new_max_adverse / tick_size if tick_size > 0 else 0
                    mae_dollars = mae_ticks * tick_value * trade['quantity']
                    if mae_ticks >= 2:  # Only log if significant
                        logger.debug(f"üìâ MAE update for trade {trade['id']}: {mae_ticks:.1f} ticks (${mae_dollars:.2f})")
            
            # Check TP/SL
            tp_price = trade.get('tp_price')
            sl_price = trade.get('sl_price')
            
            hit_type = None
            exit_price = None
            
            if side == 'LONG':
                # LONG: TP hit if price >= tp_price, SL hit if price <= sl_price
                if tp_price and current_price >= tp_price:
                    hit_type = 'tp'
                    exit_price = tp_price
                elif sl_price and current_price <= sl_price:
                    hit_type = 'sl'
                    exit_price = sl_price
            else:  # SHORT
                # SHORT: TP hit if price <= tp_price, SL hit if price >= sl_price
                if tp_price and current_price <= tp_price:
                    hit_type = 'tp'
                    exit_price = tp_price
                elif sl_price and current_price >= sl_price:
                    hit_type = 'sl'
                    exit_price = sl_price
            
            if hit_type and exit_price:
                # Calculate PnL (tick_size and tick_value already calculated above for MFE/MAE)
                if side == 'LONG':
                    pnl_ticks = (exit_price - entry_price) / tick_size
                else:
                    pnl_ticks = (entry_price - exit_price) / tick_size
                
                pnl = pnl_ticks * tick_value * trade['quantity']
                
                # Close the trade
                _ph = '%s' if is_using_postgres() else '?'
                cursor.execute(f'''
                    UPDATE recorded_trades
                    SET exit_price = {_ph}, exit_time = CURRENT_TIMESTAMP,
                        pnl = {_ph}, pnl_ticks = {_ph}, status = 'closed',
                        exit_reason = {_ph}, updated_at = CURRENT_TIMESTAMP
                    WHERE id = {_ph}
                ''', (exit_price, pnl, pnl_ticks, hit_type, trade['id']))
                
                conn.commit()
                
                # Also close any open position in recorder_positions (Trade Manager style)
                _ph = '%s' if is_using_postgres() else '?'
                cursor.execute(f'''
                    SELECT id FROM recorder_positions
                    WHERE recorder_id = {_ph} AND ticker = {_ph} AND status = 'open'
                ''', (trade['recorder_id'], ticker))
                open_pos = cursor.fetchone()
                if open_pos:
                    # Close the position with proper PnL calculation
                    cursor.execute(f'SELECT * FROM recorder_positions WHERE id = {_ph}', (open_pos[0],))
                    pos_row = cursor.fetchone()
                    if pos_row:
                        pos_columns = [desc[0] for desc in cursor.description]
                        pos = dict(zip(pos_columns, pos_row))
                        
                        pos_avg_entry = pos['avg_entry_price']
                        pos_total_qty = pos['total_quantity']
                        pos_side = pos['side']
                        
                        if pos_side == 'LONG':
                            pos_pnl_ticks = (exit_price - pos_avg_entry) / tick_size
                        else:
                            pos_pnl_ticks = (pos_avg_entry - exit_price) / tick_size
                        
                        pos_realized_pnl = pos_pnl_ticks * tick_value * pos_total_qty
                        
                        cursor.execute(f'''
                            UPDATE recorder_positions
                            SET status = 'closed',
                                exit_price = {_ph},
                                realized_pnl = {_ph},
                                closed_at = CURRENT_TIMESTAMP,
                                updated_at = CURRENT_TIMESTAMP
                            WHERE id = {_ph}
                        ''', (exit_price, pos_realized_pnl, open_pos[0]))
                        conn.commit()
                        
                        logger.info(f"üìä Position closed on {hit_type.upper()}: {pos_side} {ticker} x{pos_total_qty} | "
                                   f"Avg Entry: {pos_avg_entry} | Exit: {exit_price} | PnL: ${pos_realized_pnl:.2f}")
                
                logger.info(f"üéØ {hit_type.upper()} HIT via market data for '{trade['recorder_name']}': "
                           f"{side} {ticker} | Entry: {entry_price} | Exit: {exit_price} | "
                           f"PnL: ${pnl:.2f} ({pnl_ticks:.1f} ticks)")
                
                # Emit WebSocket event for real-time UI updates
                try:
                    socketio.emit('trade_executed', {
                        'recorder_id': trade['recorder_id'],
                        'recorder_name': trade['recorder_name'],
                        'trade_id': trade['id'],
                        'side': side,
                        'entry_price': entry_price,
                        'exit_price': exit_price,
                        'pnl': pnl,
                        'pnl_ticks': pnl_ticks,
                        'exit_reason': hit_type.upper(),
                        'ticker': ticker,
                        'timestamp': datetime.now().isoformat()
                    })
                    
                    socketio.emit('signal_received', {
                        'recorder_id': trade['recorder_id'],
                        'recorder_name': trade['recorder_name'],
                        'action': f'{hit_type.upper()}_HIT',
                        'ticker': ticker,
                        'price': exit_price,
                        'timestamp': datetime.now().isoformat(),
                        'trade': {
                            'action': 'closed',
                            'trade_id': trade['id'],
                            'side': side,
                            'entry_price': entry_price,
                            'exit_price': exit_price,
                            'pnl': pnl,
                            'exit_reason': hit_type.upper()
                        }
                    })
                except Exception as e:
                    logger.warning(f"Could not emit WebSocket update: {e}")
                
                # Send Discord/Push notification for TP/SL hit
                try:
                    # Get user_id from recorder
                    recorder_user_id = trade.get('user_id')
                    if not recorder_user_id:
                        # Look it up
                        rec_conn = get_db_connection()
                        rec_cursor = rec_conn.cursor()
                        if is_using_postgres():
                            rec_cursor.execute('SELECT user_id FROM recorders WHERE id = %s', (trade['recorder_id'],))
                        else:
                            rec_cursor.execute('SELECT user_id FROM recorders WHERE id = ?', (trade['recorder_id'],))
                        rec_row = rec_cursor.fetchone()
                        rec_cursor.close()
                        rec_conn.close()
                        if rec_row:
                            recorder_user_id = rec_row[0]
                    
                    action_word = 'TP' if hit_type == 'tp' else 'SL'
                    notify_tp_sl_hit(
                        order_type=action_word,
                        symbol=ticker,
                        quantity=trade['quantity'],
                        price=exit_price,
                        pnl=pnl,
                        recorder_name=trade.get('recorder_name'),
                        entry_price=entry_price,
                        side=side,
                        recorder_id=trade['recorder_id']  # Notify ALL users linked to this recorder
                    )
                    logger.info(f"üîî Sent {action_word} notification for recorder {trade['recorder_id']}")
                except Exception as e:
                    logger.warning(f"Could not send TP/SL notification: {e}")
        
        conn.close()
        
    except Exception as e:
        logger.warning(f"Error checking recorder trades TP/SL: {e}")
        import traceback
        logger.debug(traceback.format_exc())


def start_market_data_websocket():
    """Start the market data WebSocket in a background thread"""
    global _market_data_ws_task
    if _market_data_ws_task and hasattr(_market_data_ws_task, 'is_alive') and _market_data_ws_task.is_alive():
        return  # Already running
    
    def run_websocket():
        asyncio.run(connect_tradovate_market_data_websocket())
    
    _market_data_ws_task = threading.Thread(target=run_websocket, daemon=True)
    _market_data_ws_task.start()
    logger.info("Market data WebSocket thread started")


# ============================================================================
# Recorder Trade TP/SL Polling (Fallback when WebSocket not available)
# ============================================================================

_recorder_tp_sl_thread = None

def poll_recorder_trades_tp_sl():
    """
    Polling fallback for TP/SL monitoring when Tradovate WebSocket isn't connected.
    Uses TradingView public API to get prices every 5 seconds.
    """
    global _market_data_cache
    
    logger.info("üîÑ Starting recorder trade TP/SL polling thread (every 5 seconds)")
    
    while True:
        try:
            # Get all open trades with TP/SL set
            conn = get_db_connection()
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT DISTINCT ticker FROM recorded_trades 
                WHERE status = 'open' AND ticker IS NOT NULL
                AND (tp_price IS NOT NULL OR sl_price IS NOT NULL)
            ''')
            
            symbols_needed = [row['ticker'] for row in cursor.fetchall()]
            conn.close()
            
            if not symbols_needed:
                time.sleep(5)
                continue
            
            # Fetch prices for all symbols
            symbols_updated = set()
            for symbol in symbols_needed:
                price = get_cached_price(symbol)
                if price:
                    root = extract_symbol_root(symbol)
                    if root not in _market_data_cache:
                        _market_data_cache[root] = {}
                    _market_data_cache[root]['last'] = price
                    symbols_updated.add(root)
                    logger.debug(f"Polled price for {symbol}: {price}")
            
            # Check TP/SL for open trades
            if symbols_updated:
                check_recorder_trades_tp_sl(symbols_updated)
            
            time.sleep(5)  # Poll every 5 seconds
            
        except Exception as e:
            logger.warning(f"Error in TP/SL polling: {e}")
            time.sleep(10)


def start_recorder_tp_sl_polling():
    """Start the TP/SL polling thread"""
    global _recorder_tp_sl_thread
    
    if _recorder_tp_sl_thread and _recorder_tp_sl_thread.is_alive():
        return
    
    _recorder_tp_sl_thread = threading.Thread(target=poll_recorder_trades_tp_sl, daemon=True)
    _recorder_tp_sl_thread.start()
    logger.info("‚úÖ Recorder TP/SL polling thread started")


# ============================================================================
# Position Drawdown Polling (Trade Manager Style Real-Time Tracking)
# ============================================================================

_position_drawdown_thread = None

def poll_recorder_positions_drawdown():
    """
    Background thread that polls open positions and updates drawdown (worst_unrealized_pnl).
    Runs every 1 second for accurate tracking - same as Trade Manager.
    """
    global _market_data_cache
    
    logger.info("üìä Starting position drawdown tracker (every 1 second)")
    
    while True:
        try:
            conn = get_db_connection()
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # Get all open positions
            cursor.execute('''
                SELECT rp.*, r.name as recorder_name
                FROM recorder_positions rp
                JOIN recorders r ON rp.recorder_id = r.id
                WHERE rp.status = 'open'
            ''')
            
            positions = [dict(row) for row in cursor.fetchall()]
            
            for pos in positions:
                ticker = pos['ticker']
                side = pos['side']
                avg_entry = pos['avg_entry_price']
                total_qty = pos['total_quantity']
                
                # Get current price - try multiple sources
                root = extract_symbol_root(ticker)
                current_price = None

                # 1. Try market data cache (WebSocket prices)
                if root in _market_data_cache:
                    cache_data = _market_data_cache.get(root, {})
                    cache_age = time.time() - cache_data.get('updated', 0)
                    if cache_age < 30:  # Only use if less than 30 seconds old
                        current_price = cache_data.get('last')

                # 2. Always try to get fresh price via API for open positions
                if not current_price:
                    current_price = get_cached_price(ticker)

                if not current_price:
                    logger.debug(f"‚ö†Ô∏è No price available for {ticker} - skipping drawdown update")
                    continue
                
                # Calculate unrealized P&L
                tick_size = get_tick_size(ticker)
                tick_value = get_tick_value(ticker)
                
                if side == 'LONG':
                    pnl_ticks = (current_price - avg_entry) / tick_size
                else:  # SHORT
                    pnl_ticks = (avg_entry - current_price) / tick_size
                
                unrealized_pnl = pnl_ticks * tick_value * total_qty
                
                # Update worst/best unrealized P&L (use .get() for missing columns)
                current_worst = pos.get('worst_unrealized_pnl') or 0
                current_best = pos.get('best_unrealized_pnl') or 0
                
                new_worst = min(current_worst, unrealized_pnl)  # Worst is most negative
                new_best = max(current_best, unrealized_pnl)    # Best is most positive
                
                # Only update database if values changed
                if new_worst != current_worst or new_best != current_best or pos.get('current_price') != current_price:
                    cursor.execute('''
                        UPDATE recorder_positions
                        SET current_price = ?, 
                            unrealized_pnl = ?,
                            worst_unrealized_pnl = ?,
                            best_unrealized_pnl = ?,
                            updated_at = CURRENT_TIMESTAMP
                        WHERE id = ?
                    ''', (current_price, unrealized_pnl, new_worst, new_best, pos['id']))
                    
                    # ALSO update recorded_trades.max_adverse so dashboard shows drawdown
                    # Convert unrealized PnL to excursion values (always positive)
                    # adverse = worst loss ever (use new_worst which tracks the worst)
                    # favorable = best profit ever (use new_best which tracks the best)
                    adverse_excursion = abs(new_worst) if new_worst < 0 else 0
                    favorable_excursion = new_best if new_best > 0 else 0
                    
                    # Use GREATEST() for PostgreSQL, or simple comparison
                    # Cast values to same type to avoid PostgreSQL type mismatch
                    if is_using_postgres():
                        cursor.execute('''
                            UPDATE recorded_trades
                            SET max_adverse = GREATEST(COALESCE(max_adverse, 0)::real, %s::real),
                                max_favorable = GREATEST(COALESCE(max_favorable, 0)::real, %s::real),
                                updated_at = CURRENT_TIMESTAMP
                            WHERE recorder_id = %s AND ticker = %s AND status = 'open'
                        ''', (adverse_excursion, favorable_excursion, pos['recorder_id'], ticker))
                    else:
                        cursor.execute('''
                            UPDATE recorded_trades
                            SET max_adverse = MAX(COALESCE(max_adverse, 0), ?),
                                max_favorable = MAX(COALESCE(max_favorable, 0), ?),
                                updated_at = CURRENT_TIMESTAMP
                            WHERE recorder_id = ? AND ticker = ? AND status = 'open'
                        ''', (adverse_excursion, favorable_excursion, pos['recorder_id'], ticker))
                    
                    conn.commit()
                    
                    # Log significant drawdown changes
                    if new_worst < current_worst:
                        logger.debug(f"üìâ Position drawdown update: {pos['recorder_name']} {side} {ticker} x{total_qty} | Drawdown: ${abs(new_worst):.2f}")
                    
                    # EMIT LIVE P&L UPDATE via WebSocket (Trade Manager style)
                    # This sends real-time P&L to Control Center using TradingView prices
                    try:
                        socketio.emit('live_position_update', {
                            'position_id': pos['id'],
                            'recorder_id': pos['recorder_id'],
                            'recorder_name': pos['recorder_name'],
                            'ticker': ticker,
                            'side': side,
                            'quantity': total_qty,
                            'entry_price': avg_entry,
                            'current_price': current_price,
                            'unrealized_pnl': round(unrealized_pnl, 2),
                            'pnl_ticks': round(pnl_ticks, 2),
                            'drawdown': round(abs(new_worst), 2) if new_worst < 0 else 0,
                            'max_favorable': round(new_best, 2) if new_best > 0 else 0,
                            'pnl_percent': round((unrealized_pnl / (avg_entry * total_qty)) * 100, 2) if avg_entry else 0,
                            'timestamp': time.time()
                        })
                    except Exception as emit_err:
                        logger.debug(f"WebSocket emit error (non-critical): {emit_err}")
            
            # Emit summary of all live positions for Control Center dashboard
            if positions:
                try:
                    live_positions_summary = []
                    for pos in positions:
                        root = extract_symbol_root(pos['ticker'])
                        price = _market_data_cache.get(root, {}).get('last') or get_cached_price(pos['ticker'])
                        if price:
                            tick_size = get_tick_size(pos['ticker'])
                            tick_value = get_tick_value(pos['ticker'])
                            if pos['side'] == 'LONG':
                                pnl_ticks = (price - pos['avg_entry_price']) / tick_size
                            else:
                                pnl_ticks = (pos['avg_entry_price'] - price) / tick_size
                            unrealized = pnl_ticks * tick_value * pos['total_quantity']
                            
                            live_positions_summary.append({
                                'position_id': pos['id'],
                                'recorder_id': pos['recorder_id'],
                                'recorder_name': pos['recorder_name'],
                                'ticker': pos['ticker'],
                                'side': pos['side'],
                                'quantity': pos['total_quantity'],
                                'entry_price': pos['avg_entry_price'],
                                'current_price': price,
                                'unrealized_pnl': round(unrealized, 2),
                                'pnl_ticks': round(pnl_ticks, 2),
                                'drawdown': round(abs(pos.get('worst_unrealized_pnl') or 0), 2),
                                'max_favorable': round(pos.get('best_unrealized_pnl') or 0, 2),
                            })
                    
                    if live_positions_summary:
                        socketio.emit('live_positions_all', {
                            'positions': live_positions_summary,
                            'count': len(live_positions_summary),
                            'timestamp': time.time()
                        })
                except Exception as summary_err:
                    logger.debug(f"Summary emit error: {summary_err}")
            
            conn.close()
            
        except Exception as e:
            logger.warning(f"Error in position drawdown polling: {e}")
        
        time.sleep(1)  # Poll every 1 second for accurate tracking


def start_position_drawdown_polling():
    """Start the position drawdown polling thread"""
    global _position_drawdown_thread
    
    if _position_drawdown_thread and _position_drawdown_thread.is_alive():
        return
    
    _position_drawdown_thread = threading.Thread(target=poll_recorder_positions_drawdown, daemon=True)
    _position_drawdown_thread.start()
    logger.info("‚úÖ Position drawdown polling thread started")


# ============================================================================
# TradingView WebSocket for Real-Time Price Data
# WITH ROBUST AUTO-RECONNECT AND HEALTH MONITORING
# ============================================================================

_tradingview_ws = None
_tradingview_ws_thread = None
_tradingview_subscribed_symbols = set()
_tradingview_last_message_time = 0  # Track last message for health monitoring
_tradingview_reconnect_count = 0
_tradingview_health_thread = None
_tradingview_force_reconnect = False

# Health monitoring settings
TV_STALE_TIMEOUT = 30  # Seconds without data before considering stale
TV_RECONNECT_INTERVAL = 5  # Seconds between reconnect attempts
TV_MAX_RECONNECT_BACKOFF = 60  # Max seconds between reconnect attempts

def get_tradingview_session():
    """Get TradingView session cookies from database"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = DATABASE_URL and DATABASE_URL.startswith('postgres')

        # Don't hardcode id=1 - find first account with TradingView session
        if is_postgres:
            cursor.execute("SELECT tradingview_session FROM accounts WHERE tradingview_session IS NOT NULL AND tradingview_session != '' ORDER BY id LIMIT 1")
        else:
            cursor.execute("SELECT tradingview_session FROM accounts WHERE tradingview_session IS NOT NULL AND tradingview_session != '' ORDER BY id LIMIT 1")
        row = cursor.fetchone()
        conn.close()

        if row and row[0]:
            return json.loads(row[0])
        return None
    except Exception as e:
        logger.error(f"Error getting TradingView session: {e}")
        return None


async def connect_tradingview_websocket():
    """Connect to TradingView WebSocket for real-time quotes with robust reconnection"""
    global _market_data_cache, _tradingview_ws, _tradingview_subscribed_symbols
    global _tradingview_last_message_time, _tradingview_reconnect_count, _tradingview_force_reconnect
    
    if not WEBSOCKETS_AVAILABLE:
        logger.error("websockets library not available for TradingView")
        return
    
    session = get_tradingview_session()
    if not session or not session.get('sessionid'):
        logger.warning("No TradingView session configured. Use /api/tradingview/session to set it up.")
        return

    sessionid = session.get('sessionid')
    sessionid_sign = session.get('sessionid_sign', '')

    # Log session info for debugging (masked)
    logger.info(f"üì° TradingView session: sessionid={sessionid[:10]}...{sessionid[-5:]}, sign={'present' if sessionid_sign else 'MISSING'}")

    # Get JWT auth token for premium real-time data (same approach as tv_price_service.py)
    jwt_token = get_tradingview_auth_token(session)
    if jwt_token:
        logger.info("üöÄ TradingView JWT obtained ‚Äî will use PREMIUM real-time data (~100ms)")
    else:
        jwt_token = "unauthorized_user_token"
        logger.warning("‚ö†Ô∏è TradingView JWT extraction failed ‚Äî falling back to PUBLIC delayed data")

    ws_url = "wss://data.tradingview.com/socket.io/websocket"
    
    while True:
        try:
            # Check for force reconnect flag
            if _tradingview_force_reconnect:
                _tradingview_force_reconnect = False
                logger.info("üîÑ Force reconnect requested by health monitor")
            
            # Calculate backoff time with exponential increase (capped)
            backoff = min(TV_RECONNECT_INTERVAL * (2 ** min(_tradingview_reconnect_count, 4)), TV_MAX_RECONNECT_BACKOFF)
            
            if _tradingview_reconnect_count > 0:
                logger.info(f"üîÑ TradingView reconnect attempt #{_tradingview_reconnect_count} (backoff: {backoff}s)")
            
            logger.info(f"Connecting to TradingView WebSocket...")
            
            # Create connection (auth via JWT in protocol message, not cookies in headers)
            async with websockets.connect(
                ws_url,
                origin='https://www.tradingview.com',
                ping_interval=20,
                ping_timeout=10,
                close_timeout=5
            ) as ws:
                _tradingview_ws = ws
                _tradingview_reconnect_count = 0  # Reset on successful connection
                _tradingview_last_message_time = time.time()
                _tradingview_subscribed_symbols.clear()  # CRITICAL: Clear old subscriptions on reconnect!
                logger.info("‚úÖ TradingView WebSocket CONNECTED!")
                
                # Send JWT auth token ‚Äî premium JWT = real-time data, "unauthorized_user_token" = delayed
                auth_msg = json.dumps({
                    "m": "set_auth_token",
                    "p": [jwt_token]
                })
                await ws.send(f"~m~{len(auth_msg)}~m~{auth_msg}")
                logger.info(f"Sent TradingView auth ({'PREMIUM JWT' if jwt_token != 'unauthorized_user_token' else 'PUBLIC delayed'})")
                
                # Create a quote session
                quote_session = f"qs_{int(time.time())}"
                create_session_msg = json.dumps({
                    "m": "quote_create_session",
                    "p": [quote_session]
                })
                await ws.send(f"~m~{len(create_session_msg)}~m~{create_session_msg}")
                
                # Subscribe to symbols we need
                await subscribe_tradingview_symbols(ws, quote_session)
                
                # Listen for messages
                msg_count = 0
                async for message in ws:
                    # Update last message time for health monitoring
                    _tradingview_last_message_time = time.time()
                    msg_count += 1
                    
                    # Check if health monitor requested reconnect
                    if _tradingview_force_reconnect:
                        logger.info("üîÑ Health monitor requested reconnect, closing connection...")
                        break
                    
                    try:
                        # Handle ping/pong
                        if message.startswith('~h~'):
                            await ws.send(message)
                            continue
                        
                        await process_tradingview_message(message)
                        
                        # Log first few messages
                        if msg_count <= 5:
                            logger.info(f"TradingView msg #{msg_count}: {message[:100]}...")
                            
                    except Exception as e:
                        logger.warning(f"Error processing TradingView message: {e}")
                        
        except websockets.exceptions.ConnectionClosed as e:
            _tradingview_reconnect_count += 1
            logger.warning(f"‚ö†Ô∏è TradingView WebSocket DISCONNECTED: {e}. Reconnecting...")
            await asyncio.sleep(min(backoff, TV_RECONNECT_INTERVAL))
        except Exception as e:
            _tradingview_reconnect_count += 1
            logger.error(f"‚ùå TradingView WebSocket ERROR: {e}. Reconnecting...")
            import traceback
            logger.debug(traceback.format_exc())
            await asyncio.sleep(min(backoff, TV_MAX_RECONNECT_BACKOFF))


async def subscribe_tradingview_symbols(ws, quote_session):
    """Subscribe to symbols for real-time quotes"""
    global _tradingview_subscribed_symbols
    
    # Get symbols from open recorder trades
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT DISTINCT ticker FROM recorded_trades 
            WHERE status = 'open' AND ticker IS NOT NULL
        ''')
        
        symbols = set()
        for row in cursor.fetchall():
            ticker = row[0]
            if ticker:
                # Convert to TradingView format (CME_MINI for all futures on TradingView)
                root = extract_symbol_root(ticker)
                tv_symbol = f"CME_MINI:{root}1!"
                symbols.add(tv_symbol)
        conn.close()

        # Default symbols ‚Äî all use CME_MINI exchange on TradingView
        default_symbols = [
            'CME_MINI:NQ1!', 'CME_MINI:MNQ1!',   # Nasdaq
            'CME_MINI:ES1!', 'CME_MINI:MES1!',   # S&P 500
            'CME_MINI:YM1!', 'CME_MINI:MYM1!',   # Dow
            'CME_MINI:RTY1!', 'CME_MINI:M2K1!',  # Russell
            'COMEX:GC1!', 'COMEX:MGC1!',          # Gold
            'NYMEX:CL1!', 'NYMEX:MCL1!',         # Crude Oil
            'COMEX:SI1!', 'COMEX:SIL1!',          # Silver
        ]
        symbols.update(default_symbols)
        
        for symbol in symbols:
            if symbol not in _tradingview_subscribed_symbols:
                # Add symbol to session - simplified format
                add_msg = json.dumps({
                    "m": "quote_add_symbols",
                    "p": [quote_session, symbol]
                })
                await ws.send(f"~m~{len(add_msg)}~m~{add_msg}")
                _tradingview_subscribed_symbols.add(symbol)
                logger.info(f"üìà Subscribed to TradingView: {symbol}")
                
    except Exception as e:
        logger.warning(f"Error subscribing to TradingView symbols: {e}")


async def process_tradingview_message(message):
    """Process incoming TradingView WebSocket message"""
    global _market_data_cache
    
    try:
        # TradingView messages are formatted as: ~m~{length}~m~{json}
        if not message:
            return
        
        # Log first few messages for debugging
        if len(message) < 500:
            logger.debug(f"TradingView raw msg: {message[:200]}")
        
        # Handle heartbeat
        if message.startswith('~h~'):
            return
        
        if not message.startswith('~m~'):
            return
        
        # Extract JSON part
        parts = message.split('~m~')
        for part in parts:
            if not part or part.isdigit():
                continue
            
            try:
                data = json.loads(part)
                
                # Log message type
                if isinstance(data, dict):
                    msg_type = data.get('m', 'unknown')
                    logger.debug(f"TradingView msg type: {msg_type}")
                
                # Handle quote update messages
                if isinstance(data, dict) and data.get('m') == 'qsd':
                    # Quote session data
                    params = data.get('p', [])
                    logger.debug(f"QSD params: {params}")
                    if len(params) >= 2:
                        symbol_data = params[1]
                        symbol = symbol_data.get('n', '')  # Symbol name
                        values = symbol_data.get('v', {})
                        
                        if symbol and values:
                            # TradingView can send price in different fields
                            last_price = values.get('lp') or values.get('last_price') or values.get('ch')
                            bid = values.get('bid')
                            ask = values.get('ask')
                            
                            # Log what we're getting
                            if 'lp' in values or 'bid' in values or 'ask' in values:
                                logger.info(f"üìä TradingView {symbol}: lp={values.get('lp')}, bid={bid}, ask={ask}")
                            
                            # Use mid price if we have bid/ask but no last
                            if not last_price and bid and ask:
                                last_price = (float(bid) + float(ask)) / 2
                            
                            if last_price:
                                # Extract root symbol (CME_MINI:MNQ1! -> MNQ)
                                root = symbol.split(':')[-1].replace('1!', '').replace('!', '')
                                
                                if root not in _market_data_cache:
                                    _market_data_cache[root] = {}
                                
                                _market_data_cache[root]['last'] = float(last_price)
                                if bid:
                                    _market_data_cache[root]['bid'] = float(bid)
                                if ask:
                                    _market_data_cache[root]['ask'] = float(ask)
                                _market_data_cache[root]['source'] = 'tradingview'
                                _market_data_cache[root]['updated'] = time.time()

                                # Push to SSE subscribers (dashboard real-time)
                                _broadcast_sse_price(root)

                                logger.info(f"üí∞ TradingView price: {root} = {last_price}")
                                
                                # Check TP/SL for recorder trades
                                check_recorder_trades_tp_sl({root})
                                
            except json.JSONDecodeError:
                continue
                
    except Exception as e:
        logger.debug(f"Error processing TradingView message: {e}")


def start_tradingview_websocket():
    """Start TradingView WebSocket in background thread"""
    global _tradingview_ws_thread
    
    if _tradingview_ws_thread and _tradingview_ws_thread.is_alive():
        logger.info("TradingView WebSocket already running")
        return
    
    def run_websocket():
        asyncio.run(connect_tradingview_websocket())
    
    _tradingview_ws_thread = threading.Thread(target=run_websocket, daemon=True, name="TradingView-WS")
    _tradingview_ws_thread.start()
    logger.info("‚úÖ TradingView WebSocket thread started")
    
    # Start health monitor
    start_tradingview_health_monitor()


def tradingview_health_monitor():
    """
    Background thread that monitors TradingView WebSocket health.
    Forces reconnect if connection is stale (no data for TV_STALE_TIMEOUT seconds).
    """
    global _tradingview_last_message_time, _tradingview_force_reconnect, _tradingview_ws_thread, _tradingview_ws

    logger.info("üè• TradingView health monitor STARTED")

    check_interval = 10  # Check every 10 seconds
    stale_warning_logged = False
    force_restart_count = 0

    while True:
        try:
            time.sleep(check_interval)

            # Check if WebSocket thread is alive
            if not _tradingview_ws_thread or not _tradingview_ws_thread.is_alive():
                logger.warning("‚ö†Ô∏è TradingView WebSocket thread DEAD - restarting...")
                start_tradingview_websocket()
                stale_warning_logged = False
                continue

            # Check if data is stale
            if _tradingview_last_message_time > 0:
                seconds_since_last = time.time() - _tradingview_last_message_time

                if seconds_since_last > TV_STALE_TIMEOUT:
                    if not stale_warning_logged:
                        logger.warning(f"‚ö†Ô∏è TradingView data STALE ({seconds_since_last:.0f}s since last message)")
                        stale_warning_logged = True

                    # Force reconnect if very stale (2x timeout = 60 seconds)
                    if seconds_since_last > TV_STALE_TIMEOUT * 2:
                        logger.error(f"üî¥ TradingView connection STALE for {seconds_since_last:.0f}s - FORCING RECONNECT")
                        _tradingview_force_reconnect = True
                        force_restart_count += 1

                        # If connection is truly hung, try to close WebSocket forcibly
                        if _tradingview_ws:
                            try:
                                asyncio.run_coroutine_threadsafe(
                                    _tradingview_ws.close(),
                                    asyncio.get_event_loop()
                                )
                            except:
                                pass  # Ignore errors, we're forcing close

                        # If still stale after 3 force reconnect attempts, kill the thread
                        if force_restart_count >= 3 and seconds_since_last > TV_STALE_TIMEOUT * 4:
                            logger.error(f"üî¥üî¥ TradingView HUNG for {seconds_since_last:.0f}s after {force_restart_count} attempts - KILLING THREAD")
                            # Set thread to None so next check starts fresh
                            _tradingview_ws_thread = None
                            _tradingview_ws = None
                            force_restart_count = 0

                        stale_warning_logged = False
                else:
                    if stale_warning_logged:
                        logger.info(f"‚úÖ TradingView connection RECOVERED - data flowing")
                    stale_warning_logged = False
                    force_restart_count = 0  # Reset counter on successful data

            # Log health status every 5 minutes
            if int(time.time()) % 300 < check_interval:
                if _tradingview_last_message_time > 0:
                    age = time.time() - _tradingview_last_message_time
                    cache_size = len(_market_data_cache)
                    status = "OK" if age < TV_STALE_TIMEOUT else "STALE"
                    logger.info(f"üìä TradingView health: {status} | Last msg: {age:.1f}s ago | Symbols cached: {cache_size}")

        except Exception as e:
            logger.error(f"Error in TradingView health monitor: {e}")


def start_tradingview_health_monitor():
    """Start the TradingView health monitor thread"""
    global _tradingview_health_thread
    
    if _tradingview_health_thread and _tradingview_health_thread.is_alive():
        return
    
    _tradingview_health_thread = threading.Thread(
        target=tradingview_health_monitor, 
        daemon=True, 
        name="TradingView-Health"
    )
    _tradingview_health_thread.start()
    logger.info("‚úÖ TradingView health monitor thread started")


def get_tradingview_connection_status():
    """Get current TradingView WebSocket connection status"""
    global _tradingview_ws_thread, _tradingview_last_message_time, _tradingview_reconnect_count
    
    is_connected = _tradingview_ws_thread and _tradingview_ws_thread.is_alive()
    last_msg_age = time.time() - _tradingview_last_message_time if _tradingview_last_message_time > 0 else -1
    
    return {
        'connected': is_connected,
        'last_message_age_seconds': round(last_msg_age, 1) if last_msg_age >= 0 else None,
        'is_stale': last_msg_age > TV_STALE_TIMEOUT if last_msg_age >= 0 else True,
        'reconnect_count': _tradingview_reconnect_count,
        'cached_symbols': list(_market_data_cache.keys()),
        'status': 'healthy' if is_connected and last_msg_age >= 0 and last_msg_age < TV_STALE_TIMEOUT else 'degraded'
    }


def update_position_pnl():
    """Update PnL for all cached positions based on current market prices"""
    global _position_cache, _market_data_cache
    
    for cache_key, position in _position_cache.items():
        symbol = position.get('symbol', '')
        if not symbol:
            continue
        
        # Get current price from market data cache
        current_price = _market_data_cache.get(symbol, {}).get('last', 0.0)
        if current_price == 0:
            # Try to get from bid/ask
            market_data = _market_data_cache.get(symbol, {})
            current_price = market_data.get('bid', 0.0) or market_data.get('ask', 0.0)
        
        if current_price > 0 and position.get('avg_price', 0) > 0:
            # Calculate PnL: (current_price - avg_price) * quantity * contract_multiplier
            contract_multiplier = get_contract_multiplier(symbol)
            quantity = position.get('net_quantity', 0)
            avg_price = position.get('avg_price', 0)
            
            # PnL = (current - entry) * quantity * multiplier
            # For long: (current - entry) * qty * mult
            # For short: (entry - current) * qty * mult = (current - entry) * (-qty) * mult
            pnl = (current_price - avg_price) * quantity * contract_multiplier
            
            position['last_price'] = current_price
            position['unrealized_pnl'] = pnl
            
            # Update in database
            try:
                conn = get_db_connection()
                cursor = conn.cursor()
                cursor.execute('''
                    UPDATE open_positions 
                    SET last_price = ?, unrealized_pnl = ?, updated_at = ?
                    WHERE symbol = ? AND subaccount_id = ?
                ''', (current_price, pnl, datetime.now().isoformat(), symbol, position.get('subaccount_id')))
                conn.commit()
                conn.close()
            except Exception as e:
                logger.warning(f"Error updating position PnL in database: {e}")
            
            logger.debug(f"Updated PnL for {symbol}: price={current_price}, avg={avg_price}, qty={quantity}, mult={contract_multiplier}, PnL={pnl}")

# ============================================================================
# Custom OCO Monitor - Tracks TP/SL pairs and cancels the other when one fills
# ============================================================================

# Store paired orders: {tp_order_id: sl_order_id, sl_order_id: tp_order_id}
_oco_pairs = {}
# Store order details for monitoring: {order_id: {account_id, symbol, type: 'tp'|'sl', partner_id}}
_oco_order_details = {}
_oco_lock = threading.Lock()

def register_oco_pair(tp_order_id: int, sl_order_id: int, account_id: int, symbol: str):
    """Register a TP/SL pair for OCO monitoring"""
    global _oco_pairs, _oco_order_details
    
    with _oco_lock:
        # Store the pairing both ways for easy lookup
        _oco_pairs[tp_order_id] = sl_order_id
        _oco_pairs[sl_order_id] = tp_order_id
        
        # Store details for each order
        _oco_order_details[tp_order_id] = {
            'account_id': account_id,
            'symbol': symbol,
            'type': 'tp',
            'partner_id': sl_order_id,
            'created_at': time.time()
        }
        _oco_order_details[sl_order_id] = {
            'account_id': account_id,
            'symbol': symbol,
            'type': 'sl',
            'partner_id': tp_order_id,
            'created_at': time.time()
        }
        
        logger.info(f"üîó OCO pair registered: TP={tp_order_id} <-> SL={sl_order_id} for {symbol}")

def unregister_oco_pair(order_id: int):
    """Remove an OCO pair from monitoring (called when one side fills/cancels)"""
    global _oco_pairs, _oco_order_details
    
    with _oco_lock:
        if order_id in _oco_pairs:
            partner_id = _oco_pairs.pop(order_id, None)
            if partner_id and partner_id in _oco_pairs:
                _oco_pairs.pop(partner_id, None)
            
            # Remove details
            _oco_order_details.pop(order_id, None)
            if partner_id:
                _oco_order_details.pop(partner_id, None)

def monitor_oco_orders():
    """
    Background thread that monitors OCO order pairs across ALL accounts.
    When one order fills, it cancels the partner order on the SAME account.
    """
    logger.info("üîÑ OCO Monitor started - watching for TP/SL fills...")
    
    while True:
        try:
            # Only process if we have pairs to monitor
            with _oco_lock:
                if not _oco_pairs:
                    time.sleep(1)
                    continue
                
                # Get a copy of current pairs
                pairs_to_check = dict(_oco_pairs)
                details_copy = dict(_oco_order_details)
            
            if not pairs_to_check:
                time.sleep(1)
                continue
            
            # Get ALL accounts with tokens (for multi-account support)
            conn = get_db_connection()
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, tradovate_token, environment, tradovate_accounts, subaccounts
                FROM accounts 
                WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
            ''')
            all_accounts = cursor.fetchall()
            conn.close()
            
            if not all_accounts:
                time.sleep(2)
                continue
            
            # Group OCO pairs by account_id for efficient processing
            account_orders = {}  # {account_id: [order_ids]}
            for order_id, details in details_copy.items():
                acc_id = details.get('account_id')
                if acc_id:
                    if acc_id not in account_orders:
                        account_orders[acc_id] = []
                    account_orders[acc_id].append(order_id)
            
            # Build a map of account_id -> account info (token, env)
            # Account IDs in our system are the tradovate subaccount IDs (like 26029294)
            account_info_map = {}
            for acc in all_accounts:
                # Get valid token (auto-refreshes if needed)
                account_record_id = acc['id']
                token = get_valid_tradovate_token(account_record_id)
                if not token:
                    logger.warning(f"No valid token for account record {account_record_id} - skipping")
                    continue
                
                env = acc['environment'] or 'demo'
                
                # Try tradovate_accounts field (JSON format)
                tradovate_accounts_str = acc['tradovate_accounts'] or ''
                if tradovate_accounts_str:
                    try:
                        import json
                        tradovate_accounts = json.loads(tradovate_accounts_str)
                        for ta in tradovate_accounts:
                            acc_id = ta.get('id') or ta.get('accountId')
                            if acc_id:
                                account_info_map[int(acc_id)] = {'token': token, 'env': env}
                    except:
                        pass
                
                # Also try subaccounts field (comma-separated or JSON)
                subaccounts_str = acc['subaccounts'] or ''
                if subaccounts_str:
                    try:
                        import json
                        subaccounts = json.loads(subaccounts_str)
                        for sa in subaccounts:
                            if isinstance(sa, dict):
                                acc_id = sa.get('id') or sa.get('accountId')
                            else:
                                acc_id = sa
                            if acc_id:
                                account_info_map[int(acc_id)] = {'token': token, 'env': env}
                    except:
                        # Try comma-separated
                        for tid in subaccounts_str.split(','):
                            tid = tid.strip()
                            if tid:
                                try:
                                    account_info_map[int(tid)] = {'token': token, 'env': env}
                                except ValueError:
                                    pass
            
            # Process each account's orders
            orders_to_remove = []
            partners_to_cancel = []  # [(partner_id, filled_id, symbol, account_id)]
            
            for account_id, order_ids in account_orders.items():
                # Get token for this account
                acc_info = account_info_map.get(account_id)
                if not acc_info:
                    # Try to find any token (fallback for accounts not in our map)
                    if all_accounts:
                        acc_info = {
                            'token': all_accounts[0]['tradovate_token'],
                            'env': all_accounts[0]['environment'] or 'demo'
                        }
                    else:
                        continue
                
                token = acc_info['token']
                env = acc_info['env']
                base_url = 'https://demo.tradovateapi.com/v1' if env == 'demo' else 'https://live.tradovateapi.com/v1'
                headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
                
                # Get orders for this account
                try:
                    response = requests.get(f'{base_url}/order/list', headers=headers, timeout=5)
                    if response.status_code != 200:
                        continue
                    
                    orders = response.json()
                    order_status_map = {o.get('id'): o.get('ordStatus', '') for o in orders}
                except Exception as e:
                    logger.debug(f"OCO monitor fetch error for account {account_id}: {e}")
                    continue
                
                # Check each order for this account
                for order_id in order_ids:
                    if order_id not in pairs_to_check:
                        continue
                    
                    partner_id = pairs_to_check.get(order_id)
                    details = details_copy.get(order_id, {})
                    status = order_status_map.get(order_id, '')
                    
                    # If order is filled, we need to cancel the partner
                    if status.lower() == 'filled':
                        order_type = details.get('type', 'unknown')
                        symbol = details.get('symbol', 'unknown')
                        
                        logger.info(f"üéØ OCO: {order_type.upper()} order {order_id} FILLED for {symbol} (account {account_id})")
                        logger.info(f"üéØ OCO: Cancelling partner order {partner_id}...")
                        
                        partners_to_cancel.append((partner_id, order_id, symbol, account_id, token, env))
                        orders_to_remove.append(order_id)
                    
                    # If order is cancelled/rejected, just remove from monitoring
                    elif status.lower() in ['canceled', 'cancelled', 'rejected', 'expired']:
                        orders_to_remove.append(order_id)
            
            # Cancel partner orders (using the correct token for each account)
            for partner_id, filled_id, symbol, account_id, token, env in partners_to_cancel:
                try:
                    base_url = 'https://demo.tradovateapi.com/v1' if env == 'demo' else 'https://live.tradovateapi.com/v1'
                    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
                    
                    cancel_response = requests.post(
                        f'{base_url}/order/cancelorder',
                        json={'orderId': partner_id, 'isAutomated': True},
                        headers=headers,
                        timeout=5
                    )
                    if cancel_response.status_code == 200:
                        result = cancel_response.json()
                        if result.get('errorText'):
                            logger.warning(f"‚ö†Ô∏è OCO: Cancel returned error for {partner_id}: {result.get('errorText')}")
                        else:
                            logger.info(f"‚úÖ OCO: Successfully cancelled partner order {partner_id} for {symbol} (account {account_id})")
                        
                        # Emit event to frontend
                        socketio.emit('oco_triggered', {
                            'filled_order': filled_id,
                            'cancelled_order': partner_id,
                            'symbol': symbol,
                            'account_id': account_id,
                            'message': f'OCO triggered: cancelled partner order for {symbol}'
                        })
                    else:
                        logger.warning(f"‚ö†Ô∏è OCO: Failed to cancel partner order {partner_id}: {cancel_response.text[:200]}")
                except Exception as e:
                    logger.error(f"‚ùå OCO: Error cancelling partner order {partner_id}: {e}")
            
            # Remove processed orders from monitoring
            for order_id in orders_to_remove:
                unregister_oco_pair(order_id)
            
            time.sleep(1)  # Check every second
            
        except Exception as e:
            logger.error(f"OCO Monitor error: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            time.sleep(2)

# Start OCO monitor thread
oco_monitor_thread = threading.Thread(target=monitor_oco_orders, daemon=True)
oco_monitor_thread.start()
logger.info("üîÑ OCO Monitor thread started")

def cleanup_orphaned_orders():
    """
    On startup, scan for working orders that don't have matching positions.
    This cleans up orphaned TP/SL orders from previous sessions.
    """
    time.sleep(10)  # Wait for server to fully start
    logger.info("üßπ Scanning for orphaned orders...")
    
    try:
        # Get all accounts with tokens
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id, tradovate_token, tradovate_accounts
            FROM accounts 
            WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
        ''')
        accounts = cursor.fetchall()
        conn.close()
        
        if not accounts:
            logger.info("üßπ No accounts to scan for orphaned orders")
            return
        
        for account in accounts:
            # Get valid token (auto-refreshes if needed)
            account_record_id = account['id']
            token = get_valid_tradovate_token(account_record_id)
            if not token:
                logger.warning(f"No valid token for account record {account_record_id} - skipping")
                continue
            
            tradovate_accounts = []
            if account['tradovate_accounts']:
                try:
                    tradovate_accounts = json.loads(account['tradovate_accounts'])
                except:
                    pass
            
            for ta in tradovate_accounts:
                acc_id = ta.get('id')
                # CRITICAL FIX: Use environment as source of truth for demo vs live
                env = (ta.get('environment') or 'demo').lower()
                is_demo = env != 'live'
                acc_name = ta.get('name', str(acc_id))
                base_url = 'https://demo.tradovateapi.com/v1' if is_demo else 'https://live.tradovateapi.com/v1'
                headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
                
                try:
                    # Get positions
                    pos_response = requests.get(f'{base_url}/position/list', headers=headers, timeout=5)
                    positions = pos_response.json() if pos_response.status_code == 200 else []
                    
                    # Build set of contract IDs with open positions
                    open_contracts = set()
                    for pos in positions:
                        if pos.get('netPos', 0) != 0:
                            open_contracts.add(pos.get('contractId'))
                    
                    # Get orders
                    order_response = requests.get(f'{base_url}/order/list', headers=headers, timeout=5)
                    orders = order_response.json() if order_response.status_code == 200 else []
                    
                    orphaned = []
                    for order in orders:
                        order_status = order.get('ordStatus', '').lower()
                        if order_status in ['working', 'pending', 'accepted']:
                            contract_id = order.get('contractId')
                            # If no open position for this contract, it's orphaned
                            if contract_id not in open_contracts:
                                orphaned.append(order)
                    
                    if orphaned:
                        logger.warning(f"üßπ Found {len(orphaned)} orphaned orders for {acc_name}")
                        for order in orphaned:
                            order_id = order.get('id')
                            try:
                                cancel_response = requests.post(
                                    f'{base_url}/order/cancelorder',
                                    json={'orderId': order_id, 'isAutomated': True},
                                    headers=headers,
                                    timeout=5
                                )
                                if cancel_response.status_code == 200:
                                    logger.info(f"üßπ Cancelled orphaned order {order_id}")
                                else:
                                    logger.warning(f"üßπ Failed to cancel orphaned order {order_id}: {cancel_response.text[:100]}")
                            except Exception as e:
                                logger.warning(f"üßπ Error cancelling orphaned order {order_id}: {e}")
                    else:
                        logger.info(f"üßπ No orphaned orders for {acc_name}")
                        
                except Exception as e:
                    logger.warning(f"üßπ Error scanning {acc_name} for orphaned orders: {e}")
                    
    except Exception as e:
        logger.error(f"üßπ Error in orphaned orders cleanup: {e}")

# Start orphaned orders cleanup in background
cleanup_thread = threading.Thread(target=cleanup_orphaned_orders, daemon=True)
cleanup_thread.start()

# ============================================================================
# Break-Even Monitor - Moves SL to entry price when position goes profitable
# ============================================================================

# Store break-even monitors: {key: {account_id, symbol, entry_price, is_long, activation_ticks, ...}}
_break_even_monitors = {}
_break_even_lock = threading.Lock()

def register_break_even_monitor(account_id: int, symbol: str, entry_price: float, is_long: bool,
                                 activation_ticks: int, tick_size: float, sl_order_id: int,
                                 quantity: int, account_spec: str):
    """Register a position for break-even monitoring"""
    global _break_even_monitors
    
    key = f"{account_id}:{symbol}"
    
    with _break_even_lock:
        _break_even_monitors[key] = {
            'account_id': account_id,
            'symbol': symbol,
            'entry_price': entry_price,
            'is_long': is_long,
            'activation_ticks': activation_ticks,
            'tick_size': tick_size,
            'sl_order_id': sl_order_id,
            'quantity': quantity,
            'account_spec': account_spec,
            'triggered': False,
            'created_at': time.time()
        }
        
        activation_price = entry_price + (tick_size * activation_ticks) if is_long else entry_price - (tick_size * activation_ticks)
        logger.info(f"üìä Break-even monitor registered: {symbol} on account {account_id}")
        logger.info(f"   Entry: {entry_price}, Activation: {activation_price} ({activation_ticks} ticks)")

def unregister_break_even_monitor(key: str):
    """Remove a break-even monitor"""
    global _break_even_monitors
    
    with _break_even_lock:
        if key in _break_even_monitors:
            _break_even_monitors.pop(key)

# ============================================================================
# Signal Blocking - Instant in-memory position tracking for broker-managed exits
# ============================================================================
# When signal_blocking is enabled on a recorder, BUY/SELL signals are instantly
# blocked while a position is tracked. Close/flat/exit signals pass through.
# Reconciliation (every 60s) clears the block when broker reports flat.

_signal_blocking_positions = {}  # key: "recorder_id:symbol_root" -> {side, set_at}
_signal_blocking_lock = threading.Lock()

def set_signal_blocking_position(recorder_id, symbol_root, side):
    """Track a position for signal blocking"""
    key = f"{recorder_id}:{symbol_root}"
    with _signal_blocking_lock:
        _signal_blocking_positions[key] = {
            'side': side,
            'set_at': time.time()
        }
    logger.info(f"üõë SIGNAL BLOCKING SET: {key} ‚Üí {side}")

def clear_signal_blocking_position(recorder_id, symbol_root):
    """Clear signal blocking when position is closed"""
    key = f"{recorder_id}:{symbol_root}"
    with _signal_blocking_lock:
        if key in _signal_blocking_positions:
            _signal_blocking_positions.pop(key)
            logger.info(f"‚úÖ SIGNAL BLOCKING CLEARED: {key}")

def check_signal_blocking(recorder_id, symbol_root):
    """Check if a position is being blocked. Returns entry or None.
    Auto-clears entries older than 5 minutes (safety valve for stale entries)."""
    key = f"{recorder_id}:{symbol_root}"
    with _signal_blocking_lock:
        entry = _signal_blocking_positions.get(key)
        if entry:
            age = time.time() - entry['set_at']
            if age > 300:  # 5-minute staleness safety valve
                _signal_blocking_positions.pop(key)
                logger.warning(f"‚è∞ SIGNAL BLOCKING EXPIRED: {key} (age={age:.0f}s > 300s) ‚Äî auto-cleared")
                return None
            return entry
    return None

def monitor_break_even():
    """
    Background thread that monitors positions for break-even activation.
    When price reaches activation_ticks profit, cancels old SL and places new SL at entry.
    """
    logger.info("üìä Break-Even Monitor started")
    
    while True:
        try:
            with _break_even_lock:
                if not _break_even_monitors:
                    time.sleep(2)
                    continue
                
                monitors_copy = dict(_break_even_monitors)
            
            if not monitors_copy:
                time.sleep(2)
                continue
            
            # Get tokens from database
            conn = get_db_connection()
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT id, tradovate_token, environment, tradovate_accounts, subaccounts
                FROM accounts 
                WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
            ''')
            all_accounts = cursor.fetchall()
            conn.close()
            
            if not all_accounts:
                time.sleep(2)
                continue
            
            # Build account info map
            account_info_map = {}
            for acc in all_accounts:
                # Get valid token (auto-refreshes if needed)
                account_record_id = acc['id']
                token = get_valid_tradovate_token(account_record_id)
                if not token:
                    logger.warning(f"No valid token for account record {account_record_id} - skipping")
                    continue
                
                env = acc['environment'] or 'demo'
                
                tradovate_accounts_str = acc['tradovate_accounts'] or ''
                if tradovate_accounts_str:
                    try:
                        tradovate_accounts = json.loads(tradovate_accounts_str)
                        for ta in tradovate_accounts:
                            acc_id = ta.get('id') or ta.get('accountId')
                            # CRITICAL FIX: Use environment as source of truth (already have env from acc)
                            # Prioritize account-level environment over stale ta.is_demo
                            ta_env = (ta.get('environment') or env or 'demo').lower()
                            if acc_id:
                                account_info_map[int(acc_id)] = {'token': token, 'env': ta_env}
                    except:
                        pass
            
            # Check each monitored position
            monitors_to_remove = []
            
            for key, monitor in monitors_copy.items():
                if monitor.get('triggered'):
                    continue
                
                account_id = monitor['account_id']
                symbol = monitor['symbol']
                entry_price = monitor['entry_price']
                is_long = monitor['is_long']
                activation_ticks = monitor['activation_ticks']
                tick_size = monitor['tick_size']
                sl_order_id = monitor['sl_order_id']
                quantity = monitor['quantity']
                account_spec = monitor['account_spec']
                
                acc_info = account_info_map.get(account_id)
                if not acc_info:
                    continue
                
                token = acc_info['token']
                env = acc_info['env']
                base_url = 'https://demo.tradovateapi.com/v1' if env == 'demo' else 'https://live.tradovateapi.com/v1'
                headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
                
                try:
                    # Get current positions
                    pos_response = requests.get(f'{base_url}/position/list', headers=headers, timeout=5)
                    if pos_response.status_code != 200:
                        continue
                    
                    positions = pos_response.json()
                    
                    # Find matching position
                    position = None
                    for p in positions:
                        if p.get('accountId') == account_id:
                            pos_symbol = p.get('contractId')  # Need to resolve
                            # For now, check by netPos direction matching
                            net_pos = p.get('netPos', 0)
                            if (is_long and net_pos > 0) or (not is_long and net_pos < 0):
                                position = p
                                break
                    
                    if not position:
                        # Position closed, remove monitor
                        monitors_to_remove.append(key)
                        continue
                    
                    # Get current MARKET price (NOT position.netPrice which is entry price!)
                    # Use market data cache for real-time prices
                    symbol_root = extract_symbol_root(symbol)
                    current_price = None
                    
                    # Try market data cache first (real-time WebSocket prices)
                    if symbol_root in _market_data_cache:
                        current_price = _market_data_cache[symbol_root].get('last')
                    
                    # Try exact symbol match
                    if not current_price and symbol in _market_data_cache:
                        current_price = _market_data_cache[symbol].get('last')
                    
                    # Fallback to cached price function
                    if not current_price:
                        current_price = get_cached_price(symbol)
                    
                    # Last resort: try TradingView API
                    if not current_price:
                        current_price = get_market_price_simple(symbol)
                    
                    if not current_price:
                        logger.debug(f"Break-even monitor: No market price for {symbol}, skipping check")
                        continue
                    
                    # Calculate profit in ticks
                    if is_long:
                        profit_ticks = (current_price - entry_price) / tick_size
                    else:
                        profit_ticks = (entry_price - current_price) / tick_size
                    
                    # Log monitoring progress (every ~10 seconds)
                    monitor_age = time.time() - monitor.get('created_at', time.time())
                    if int(monitor_age) % 10 < 2:  # Log roughly every 10 seconds
                        logger.debug(f"üìä Break-even monitor: {symbol} | Entry: {entry_price:.2f} | Current: {current_price:.2f} | Profit: {profit_ticks:.1f}/{activation_ticks} ticks")
                    
                    # Check if activation threshold reached
                    if profit_ticks >= activation_ticks:
                        logger.info(f"üéØ Break-even triggered for {symbol}! Profit: {profit_ticks:.1f} ticks >= {activation_ticks} ticks")
                        
                        # Cancel old SL order
                        if sl_order_id:
                            cancel_response = requests.post(
                                f'{base_url}/order/cancelorder',
                                json={'orderId': sl_order_id, 'isAutomated': True},
                                headers=headers,
                                timeout=5
                            )
                            if cancel_response.status_code == 200:
                                logger.info(f"‚úÖ Cancelled old SL order {sl_order_id}")
                        
                        # Place new SL at entry price (break-even)
                        exit_side = 'Sell' if is_long else 'Buy'
                        new_sl_data = {
                            "accountSpec": account_spec,
                            "orderType": "Stop",
                            "action": exit_side,
                            "symbol": symbol,
                            "orderQty": int(quantity),
                            "stopPrice": float(entry_price),
                            "timeInForce": "GTC",
                            "isAutomated": True
                        }
                        
                        sl_response = requests.post(
                            f'{base_url}/order/placeorder',
                            json=new_sl_data,
                            headers=headers,
                            timeout=5
                        )
                        
                        if sl_response.status_code == 200:
                            result = sl_response.json()
                            new_sl_id = result.get('orderId')
                            logger.info(f"‚úÖ Break-even SL placed at {entry_price}, Order ID: {new_sl_id}")
                            
                            # Emit to frontend
                            socketio.emit('break_even_triggered', {
                                'symbol': symbol,
                                'account_id': account_id,
                                'entry_price': entry_price,
                                'new_sl_order_id': new_sl_id,
                                'message': f'Break-even activated for {symbol}'
                            })
                        else:
                            logger.warning(f"‚ö†Ô∏è Failed to place break-even SL: {sl_response.text[:200]}")
                        
                        # Mark as triggered
                        with _break_even_lock:
                            if key in _break_even_monitors:
                                _break_even_monitors[key]['triggered'] = True
                        
                        monitors_to_remove.append(key)
                
                except Exception as e:
                    logger.debug(f"Break-even monitor error for {key}: {e}")
                    continue
            
            # Remove processed monitors
            for key in monitors_to_remove:
                unregister_break_even_monitor(key)
            
            time.sleep(2)  # Check every 2 seconds
            
        except Exception as e:
            logger.error(f"Break-Even Monitor error: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            time.sleep(3)

# Start break-even monitor thread
break_even_thread = threading.Thread(target=monitor_break_even, daemon=True)
break_even_thread.start()
logger.info("üìä Break-Even Monitor thread started")

# ============================================================================
# ProjectX Break-Even Safety-Net Monitor (No native BE support)
# ============================================================================
# ProjectX bracket API only accepts ticks + type ‚Äî no break-even.
# This daemon polls price via get_cached_price (0 broker API calls),
# and on trigger: cancel old SL + place new SL at entry (3 broker calls, once).
_projectx_break_even_monitors = {}
_projectx_break_even_lock = threading.Lock()

def register_projectx_break_even_monitor(account_id: int, contract_id: str, symbol_root: str,
                                          entry_price: float, is_long: bool, activation_ticks: int,
                                          offset_ticks: int, tick_size: float, quantity: int,
                                          session_token: str, base_url: str, prop_firm: str,
                                          username: str, api_key: str):
    """Register a ProjectX position for break-even monitoring"""
    key = f"px:{account_id}:{contract_id}"
    with _projectx_break_even_lock:
        _projectx_break_even_monitors[key] = {
            'account_id': account_id,
            'contract_id': contract_id,
            'symbol_root': symbol_root,
            'entry_price': entry_price,
            'is_long': is_long,
            'activation_ticks': activation_ticks,
            'offset_ticks': offset_ticks,
            'tick_size': tick_size,
            'quantity': quantity,
            'session_token': session_token,
            'base_url': base_url,
            'prop_firm': prop_firm,
            'username': username,
            'api_key': api_key,
            'triggered': False,
            'created_at': time.time()
        }
    activation_price = entry_price + (tick_size * activation_ticks) if is_long else entry_price - (tick_size * activation_ticks)
    logger.info(f"üìä ProjectX break-even monitor registered: {symbol_root} on account {account_id}")
    logger.info(f"   Entry: {entry_price}, Activation: {activation_price} ({activation_ticks} ticks)")

def unregister_projectx_break_even_monitor(key: str):
    """Remove a ProjectX break-even monitor"""
    with _projectx_break_even_lock:
        _projectx_break_even_monitors.pop(key, None)

def monitor_projectx_break_even():
    """
    Background thread that monitors ProjectX positions for break-even activation.
    Uses get_cached_price() for price (0 broker API calls per cycle).
    On trigger: searchOpen ‚Üí cancel SL ‚Üí place new SL at entry (3 calls, once).
    """
    logger.info("üìä ProjectX Break-Even Monitor started")

    while True:
        try:
            with _projectx_break_even_lock:
                if not _projectx_break_even_monitors:
                    time.sleep(2)
                    continue
                monitors_copy = dict(_projectx_break_even_monitors)

            monitors_to_remove = []

            for key, monitor in monitors_copy.items():
                if monitor.get('triggered'):
                    continue

                symbol_root = monitor['symbol_root']
                entry_price = monitor['entry_price']
                is_long = monitor['is_long']
                activation_ticks = monitor['activation_ticks']
                offset_ticks = monitor['offset_ticks']
                tick_size = monitor['tick_size']
                quantity = monitor['quantity']
                contract_id = monitor['contract_id']
                account_id = monitor['account_id']
                session_token = monitor['session_token']
                base_url = monitor['base_url']

                # Get current price (Yahoo/TradingView ‚Äî 0 broker API calls)
                current_price = get_cached_price(symbol_root)
                if not current_price:
                    continue

                # Calculate profit in ticks
                if is_long:
                    profit_ticks = (current_price - entry_price) / tick_size
                else:
                    profit_ticks = (entry_price - current_price) / tick_size

                # Debug logging every ~10 seconds
                monitor_age = time.time() - monitor.get('created_at', time.time())
                if int(monitor_age) % 10 < 2:
                    logger.debug(f"üìä ProjectX BE monitor: {symbol_root} | Entry: {entry_price:.2f} | Current: {current_price:.2f} | Profit: {profit_ticks:.1f}/{activation_ticks} ticks")

                if profit_ticks < activation_ticks:
                    continue

                # === TRIGGERED ===
                logger.info(f"üéØ ProjectX break-even triggered for {symbol_root}! Profit: {profit_ticks:.1f} ticks >= {activation_ticks} ticks")
                headers = {'Authorization': f'Bearer {session_token}', 'Content-Type': 'application/json'}

                try:
                    # Step 1: Find open SL orders for this contract
                    search_resp = requests.post(
                        f'{base_url}/Order/searchOpen',
                        json={'accountId': account_id},
                        headers=headers, timeout=5
                    )

                    if search_resp.status_code == 401:
                        # Re-authenticate
                        logger.info(f"üîë ProjectX BE monitor: re-authenticating for account {account_id}")
                        auth_resp = requests.post(
                            f'{base_url}/Auth/loginKey',
                            json={'userName': monitor['username'], 'apiKey': monitor['api_key']},
                            headers={'Content-Type': 'application/json'}, timeout=5
                        )
                        if auth_resp.status_code == 200:
                            new_token = auth_resp.json().get('token')
                            if new_token:
                                monitor['session_token'] = new_token
                                session_token = new_token
                                with _projectx_break_even_lock:
                                    if key in _projectx_break_even_monitors:
                                        _projectx_break_even_monitors[key]['session_token'] = new_token
                                headers = {'Authorization': f'Bearer {new_token}', 'Content-Type': 'application/json'}
                                search_resp = requests.post(
                                    f'{base_url}/Order/searchOpen',
                                    json={'accountId': account_id},
                                    headers=headers, timeout=5
                                )

                    if search_resp.status_code != 200:
                        logger.warning(f"‚ö†Ô∏è ProjectX BE: searchOpen failed ({search_resp.status_code})")
                        continue

                    open_orders = search_resp.json() if isinstance(search_resp.json(), list) else search_resp.json().get('orders', [])

                    # Step 2: Cancel matching SL orders (type 4=StopMarket, 5=StopLimit)
                    sl_cancelled = 0
                    for order in open_orders:
                        order_contract = str(order.get('contractId', ''))
                        order_type = order.get('type', 0)
                        if order_contract == str(contract_id) and order_type in (4, 5):
                            cancel_resp = requests.post(
                                f'{base_url}/Order/cancel',
                                json={'orderId': order.get('id')},
                                headers=headers, timeout=5
                            )
                            if cancel_resp.status_code == 200:
                                sl_cancelled += 1
                                logger.info(f"‚úÖ ProjectX BE: Cancelled SL order {order.get('id')}")
                            else:
                                logger.warning(f"‚ö†Ô∏è ProjectX BE: Cancel SL {order.get('id')} failed ({cancel_resp.status_code})")

                    # Step 3: Place new SL at break-even price
                    if is_long:
                        stop_price = entry_price + (offset_ticks * tick_size)
                    else:
                        stop_price = entry_price - (offset_ticks * tick_size)
                    stop_price = round(round(stop_price / tick_size) * tick_size, 10)  # Rule 2: tick rounding

                    exit_side = "Sell" if is_long else "Buy"
                    new_sl_data = {
                        'accountId': account_id,
                        'contractId': contract_id,
                        'type': 4,  # StopMarket
                        'side': exit_side,
                        'size': int(quantity),
                        'stopPrice': float(stop_price),
                        'timeInForce': 'GTC'
                    }

                    sl_resp = requests.post(
                        f'{base_url}/Order/place',
                        json=new_sl_data,
                        headers=headers, timeout=5
                    )

                    if sl_resp.status_code == 200:
                        new_sl_id = sl_resp.json().get('id') or sl_resp.json().get('orderId')
                        logger.info(f"‚úÖ ProjectX break-even SL placed at {stop_price}, Order ID: {new_sl_id}")
                        socketio.emit('break_even_triggered', {
                            'symbol': symbol_root,
                            'account_id': account_id,
                            'entry_price': entry_price,
                            'new_sl_order_id': new_sl_id,
                            'message': f'ProjectX break-even activated for {symbol_root}'
                        })
                    else:
                        logger.warning(f"‚ö†Ô∏è ProjectX BE: Place SL failed ({sl_resp.status_code}): {sl_resp.text[:200]}")

                    # Mark triggered and queue for removal
                    with _projectx_break_even_lock:
                        if key in _projectx_break_even_monitors:
                            _projectx_break_even_monitors[key]['triggered'] = True
                    monitors_to_remove.append(key)

                except Exception as trigger_err:
                    logger.warning(f"‚ö†Ô∏è ProjectX BE trigger error for {key}: {trigger_err}")
                    continue

            # Remove processed monitors
            for key in monitors_to_remove:
                unregister_projectx_break_even_monitor(key)

            time.sleep(2)

        except Exception as e:
            logger.error(f"ProjectX Break-Even Monitor error: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            time.sleep(3)

# Start ProjectX break-even monitor thread
_projectx_be_thread = threading.Thread(target=monitor_projectx_break_even, daemon=True)
_projectx_be_thread.start()
logger.info("üìä ProjectX Break-Even Monitor thread started")

# ============================================================================
# ProjectX Trailing Stop Emulator (Cancel/Replace)
# ============================================================================
# ProjectX type=5 trailing stop is BROKEN on buy/long side (inverts direction).
# This daemon emulates trailing stops by polling price via get_cached_price
# (0 broker API calls per cycle) and on SL move: cancel old SL + place new SL.
_projectx_trailing_monitors = {}
_projectx_trailing_lock = threading.Lock()

def register_projectx_trailing_monitor(account_id, contract_id, symbol_root,
                                         entry_price, is_long, activation_ticks,
                                         offset_ticks, frequency_ticks, tick_size,
                                         quantity, session_token, base_url,
                                         prop_firm, username, api_key):
    """Register a ProjectX position for trailing stop monitoring"""
    key = f"px_trail:{account_id}:{contract_id}"
    with _projectx_trailing_lock:
        _projectx_trailing_monitors[key] = {
            'account_id': account_id,
            'contract_id': contract_id,
            'symbol_root': symbol_root,
            'entry_price': entry_price,
            'is_long': is_long,
            'activation_ticks': activation_ticks,
            'offset_ticks': offset_ticks,
            'frequency_ticks': frequency_ticks,
            'tick_size': tick_size,
            'quantity': quantity,
            'session_token': session_token,
            'base_url': base_url,
            'prop_firm': prop_firm,
            'username': username,
            'api_key': api_key,
            'best_price': entry_price,
            'trail_active': activation_ticks == 0,
            'last_sl_price': None,
            'created_at': time.time()
        }
    logger.info(f"üìä ProjectX trailing monitor registered: {symbol_root} on account {account_id}")
    logger.info(f"   Entry: {entry_price}, Offset: {offset_ticks}t, Trigger: {activation_ticks}t, Freq: {frequency_ticks}t")

def unregister_projectx_trailing_monitor(key):
    with _projectx_trailing_lock:
        _projectx_trailing_monitors.pop(key, None)

def monitor_projectx_trailing():
    """
    Background thread emulating trailing stops for ProjectX.
    Polls get_cached_price (0 broker API calls per cycle).
    On SL move: searchOpen -> cancel type 4/5 -> place type 4 (3 calls).
    """
    logger.info("üìä ProjectX Trailing Stop Emulator started")

    while True:
        try:
            with _projectx_trailing_lock:
                if not _projectx_trailing_monitors:
                    time.sleep(2)
                    continue
                monitors_copy = dict(_projectx_trailing_monitors)

            monitors_to_remove = []

            for key, mon in monitors_copy.items():
                symbol_root = mon['symbol_root']
                entry_price = mon['entry_price']
                is_long = mon['is_long']
                tick_size = mon['tick_size']
                offset_ticks = mon['offset_ticks']
                activation_ticks = mon['activation_ticks']
                frequency_ticks = mon['frequency_ticks']
                contract_id = mon['contract_id']
                account_id = mon['account_id']
                session_token = mon['session_token']
                base_url = mon['base_url']

                # 24h staleness cleanup
                if time.time() - mon['created_at'] > 86400:
                    logger.info(f"üßπ ProjectX trailing monitor expired (24h): {key}")
                    monitors_to_remove.append(key)
                    continue

                # Get current price (TradingView cache ‚Äî 0 broker API calls)
                current_price = get_cached_price(symbol_root)
                if not current_price:
                    continue

                # Update best_price (most favorable for our side)
                best_price = mon['best_price']
                if is_long:
                    if current_price > best_price:
                        best_price = current_price
                        with _projectx_trailing_lock:
                            if key in _projectx_trailing_monitors:
                                _projectx_trailing_monitors[key]['best_price'] = best_price
                else:
                    if current_price < best_price:
                        best_price = current_price
                        with _projectx_trailing_lock:
                            if key in _projectx_trailing_monitors:
                                _projectx_trailing_monitors[key]['best_price'] = best_price

                # Check activation
                if not mon['trail_active']:
                    if is_long:
                        profit_ticks = (best_price - entry_price) / tick_size
                    else:
                        profit_ticks = (entry_price - best_price) / tick_size
                    if profit_ticks >= activation_ticks:
                        with _projectx_trailing_lock:
                            if key in _projectx_trailing_monitors:
                                _projectx_trailing_monitors[key]['trail_active'] = True
                        mon['trail_active'] = True
                        logger.info(f"üéØ ProjectX trailing ACTIVATED: {symbol_root} ({profit_ticks:.1f} >= {activation_ticks} ticks)")
                    else:
                        continue

                # Calculate new SL price
                sl_distance = offset_ticks * tick_size
                if is_long:
                    new_sl = best_price - sl_distance
                else:
                    new_sl = best_price + sl_distance
                new_sl = round(round(new_sl / tick_size) * tick_size, 10)

                # Only move SL in favorable direction
                last_sl = mon['last_sl_price']
                should_update = False
                if last_sl is None:
                    should_update = True
                elif is_long and new_sl > last_sl:
                    if frequency_ticks == 0 or (new_sl - last_sl) / tick_size >= frequency_ticks:
                        should_update = True
                elif not is_long and new_sl < last_sl:
                    if frequency_ticks == 0 or (last_sl - new_sl) / tick_size >= frequency_ticks:
                        should_update = True

                if not should_update:
                    continue

                # === CANCEL/REPLACE SL ===
                headers = {'Authorization': f'Bearer {session_token}', 'Content-Type': 'application/json'}

                # Step 1: searchOpen
                search_resp = requests.post(f'{base_url}/Order/searchOpen',
                    json={'accountId': account_id}, headers=headers, timeout=5)

                if search_resp.status_code == 401:
                    auth_resp = requests.post(f'{base_url}/Auth/loginKey',
                        json={'userName': mon['username'], 'apiKey': mon['api_key']},
                        headers={'Content-Type': 'application/json'}, timeout=5)
                    if auth_resp.status_code == 200:
                        new_token = auth_resp.json().get('token')
                        if new_token:
                            session_token = new_token
                            with _projectx_trailing_lock:
                                if key in _projectx_trailing_monitors:
                                    _projectx_trailing_monitors[key]['session_token'] = new_token
                            headers = {'Authorization': f'Bearer {new_token}', 'Content-Type': 'application/json'}
                            search_resp = requests.post(f'{base_url}/Order/searchOpen',
                                json={'accountId': account_id}, headers=headers, timeout=5)

                if search_resp.status_code != 200:
                    logger.warning(f"‚ö†Ô∏è ProjectX trail: searchOpen failed ({search_resp.status_code})")
                    continue

                open_orders = search_resp.json() if isinstance(search_resp.json(), list) else search_resp.json().get('orders', [])

                # Step 2: Cancel existing SL orders for this contract
                sl_cancelled = 0
                for order in open_orders:
                    if str(order.get('contractId', '')) == str(contract_id) and order.get('type', 0) in (4, 5):
                        cancel_resp = requests.post(f'{base_url}/Order/cancel',
                            json={'orderId': order.get('id')}, headers=headers, timeout=5)
                        if cancel_resp.status_code == 200:
                            sl_cancelled += 1

                # If no SL orders found after first placement, position likely closed
                if sl_cancelled == 0 and last_sl is not None:
                    logger.info(f"üìä ProjectX trail: No SL orders found for {symbol_root} ‚Äî position likely closed")
                    monitors_to_remove.append(key)
                    continue

                # Step 3: Place new SL
                exit_side = 1 if is_long else 0
                new_sl_data = {
                    'accountId': account_id,
                    'contractId': contract_id,
                    'type': 4,
                    'side': exit_side,
                    'size': int(mon['quantity']),
                    'stopPrice': float(new_sl)
                }
                sl_resp = requests.post(f'{base_url}/Order/place',
                    json=new_sl_data, headers=headers, timeout=5)

                if sl_resp.status_code == 200:
                    with _projectx_trailing_lock:
                        if key in _projectx_trailing_monitors:
                            _projectx_trailing_monitors[key]['last_sl_price'] = new_sl
                    logger.info(f"üìä ProjectX trail SL moved: {symbol_root} -> {new_sl} (best={best_price}, offset={offset_ticks}t)")
                else:
                    logger.warning(f"‚ö†Ô∏è ProjectX trail: Place SL failed ({sl_resp.status_code})")

            # Cleanup
            for key in monitors_to_remove:
                unregister_projectx_trailing_monitor(key)

            time.sleep(2)

        except Exception as e:
            logger.error(f"ProjectX Trailing Stop Emulator error: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            time.sleep(3)

# Start ProjectX trailing stop emulator thread
_projectx_trail_thread = threading.Thread(target=monitor_projectx_trailing, daemon=True)
_projectx_trail_thread.start()
logger.info("üìä ProjectX Trailing Stop Emulator thread started")

# Redis break-even request listener (receives requests from trading engine process)
def _break_even_redis_listener():
    """Poll Redis for break-even registration requests from the trading engine."""
    import redis as _redis
    _redis_url = os.environ.get('REDIS_URL')
    if not _redis_url:
        return
    try:
        r = _redis.from_url(_redis_url, decode_responses=True)
        r.ping()
    except Exception:
        return
    logger.info("üìä Break-even Redis listener started (receives requests from trading engine)")
    while True:
        try:
            result = r.blpop('jt:break_even_requests', timeout=5)
            if result:
                data = json.loads(result[1])
                register_break_even_monitor(
                    account_id=int(data['account_id']),
                    symbol=data['symbol'],
                    entry_price=float(data['entry_price']),
                    is_long=data['is_long'],
                    activation_ticks=int(data['activation_ticks']),
                    tick_size=float(data['tick_size']),
                    sl_order_id=data.get('sl_order_id'),
                    quantity=int(data['quantity']),
                    account_spec=data['account_spec']
                )
                logger.info(f"üìä Break-even monitor registered via Redis: {data['symbol']} on account {data['account_id']}")
        except Exception as e:
            logger.debug(f"Break-even Redis listener error: {e}")
            time.sleep(2)

if _EXTERNAL_ENGINE:
    _be_redis_listener_thread = threading.Thread(target=_break_even_redis_listener, daemon=True, name="BE-Redis-Listener")
    _be_redis_listener_thread.start()

# ============================================================================
# Tradovate PnL Fetching (Direct from API - No Market Data Required!)
# ============================================================================

# Cache for Tradovate PnL data
# RESET on every server start to avoid stale data
_tradovate_pnl_cache = {
    'last_fetch': 0,
    'data': {},
    'positions': [],
    'account_count': 10,  # Start slower (2 second interval) to avoid rate limits
    'rate_limited_until': 0  # Timestamp when rate limit expires
}

# Track last refresh attempt per account to avoid hammering API
_last_refresh_attempt = {}

def get_valid_tradovate_token(account_id: int) -> str | None:
    """
    Get a valid Tradovate access token for an account.
    Automatically refreshes if token is expired or expiring soon (within 15 minutes).
    Returns the access token string, or None if unavailable.
    """
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute('''
            SELECT tradovate_token, tradovate_refresh_token, token_expires_at, name
            FROM accounts WHERE id = ?
        ''', (account_id,))
        account = cursor.fetchone()
        conn.close()
        
        if not account or not account['tradovate_token']:
            logger.debug(f"No access token found for account {account_id}")
            return None
        
        access_token = account['tradovate_token']
        refresh_token = account['tradovate_refresh_token']
        expires_at_str = account['token_expires_at']
        
        # Check if token needs refresh
        needs_refresh = False
        if not expires_at_str:
            # No expiration time stored - refresh to be safe
            logger.debug(f"Account {account_id} has no expiration time - refreshing token")
            needs_refresh = True
        else:
            try:
                from datetime import datetime as dt_class, timedelta
                import datetime as dt_module
                
                # Handle both string and datetime objects (PostgreSQL returns datetime directly)
                # Use multiple checks because isinstance can fail with some DB drivers
                if isinstance(expires_at_str, (dt_class, dt_module.datetime)):
                    expires_at = expires_at_str
                elif hasattr(expires_at_str, 'strftime'):
                    # Has datetime-like methods - treat as datetime
                    expires_at = expires_at_str
                elif isinstance(expires_at_str, str):
                    expires_at = dt_class.strptime(expires_at_str, '%Y-%m-%d %H:%M:%S')
                else:
                    # Unknown type - try to convert to string first
                    expires_at = dt_class.strptime(str(expires_at_str), '%Y-%m-%d %H:%M:%S')
                
                # Make both naive for comparison (strip timezone if present)
                if hasattr(expires_at, 'tzinfo') and expires_at.tzinfo is not None:
                    expires_at = expires_at.replace(tzinfo=None)
                now = dt_class.now()
                
                # Refresh if expired or expiring within 15 minutes
                time_until_expiry = (expires_at - now).total_seconds()
                if time_until_expiry <= 0:
                    logger.info(f"Token for account {account_id} is expired - refreshing")
                    needs_refresh = True
                elif time_until_expiry < 15 * 60:  # Less than 15 minutes
                    logger.info(f"Token for account {account_id} expires in {int(time_until_expiry/60)} minutes - refreshing proactively")
                    needs_refresh = True
            except Exception as e:
                logger.warning(f"Could not parse expiration time for account {account_id}: {e} - refreshing to be safe")
                needs_refresh = True
        
        # Refresh if needed
        if needs_refresh and refresh_token:
            logger.info(f"Refreshing token for account {account_id}")
            if try_refresh_tradovate_token(account_id):
                # Get the new token
                conn = get_db_connection()
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                _ph = '%s' if is_using_postgres() else '?'
                cursor.execute(f'SELECT tradovate_token FROM accounts WHERE id = {_ph}', (account_id,))
                new_account = cursor.fetchone()
                conn.close()
                if new_account and new_account['tradovate_token']:
                    return new_account['tradovate_token']
                else:
                    logger.warning(f"Token refresh succeeded but new token not found for account {account_id}")
            else:
                logger.warning(f"Failed to refresh token for account {account_id}")
                # Return existing token anyway - might still work
                return access_token
        
        return access_token
        
    except Exception as e:
        logger.error(f"Error getting valid token for account {account_id}: {e}")
        return None

def try_refresh_tradovate_token(account_id: int) -> bool:
    """
    Try to refresh the Tradovate access token.
    
    CRITICAL FIX (Dec 18, 2025): Tradovate doesn't use traditional refresh tokens!
    Instead, you renew the ACCESS TOKEN using Authorization header.
    See: https://community.tradovate.com/t/token-expiry/5276
    
    Uses environment-specific endpoint based on account settings.
    Includes rate limit protection to avoid 429 errors.
    """
    global _last_refresh_attempt

    # Rate limit protection: don't try more than once per 30 seconds per account
    last_attempt = _last_refresh_attempt.get(account_id, 0)
    if time.time() - last_attempt < 30:
        logger.debug(f"Skipping refresh for account {account_id} - tried recently")
        return False
    _last_refresh_attempt[account_id] = time.time()

    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute('''
            SELECT tradovate_token, tradovate_refresh_token, environment, name
            FROM accounts WHERE id = ?
        ''', (account_id,))
        account = cursor.fetchone()

        if not account or not account['tradovate_token']:
            conn.close()
            logger.warning(f"No access token found for account {account_id}")
            return False

        current_token = account['tradovate_token']
        account_name = account['name'] or f'Account {account_id}'
        env = account['environment'] or 'demo'

        # Use environment-specific endpoint (demo tokens only work on demo, live on live)
        if env == 'live':
            token_endpoints = [
                'https://live.tradovateapi.com/v1/auth/renewAccessToken'
            ]
        else:
            token_endpoints = [
                'https://demo.tradovateapi.com/v1/auth/renewAccessToken'
            ]

        for token_url in token_endpoints:
            try:
                # CRITICAL: Use Authorization header with current access token!
                # NOT json body with refreshToken (that's the old broken way)
                response = requests.post(
                    token_url,
                    headers={
                        'Authorization': f'Bearer {current_token}',
                        'Content-Type': 'application/json'
                    },
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = response.json()
                    new_access_token = data.get('accessToken') or data.get('access_token')
                    # Keep the existing refresh token (Tradovate renewal doesn't return a new one)
                    existing_refresh_token = account['tradovate_refresh_token']
                    
                    if new_access_token:
                        # Calculate actual expiration time from Tradovate response
                        expires_at = None
                        if 'expirationTime' in data:
                            try:
                                from datetime import datetime
                                expires_at = datetime.fromisoformat(data['expirationTime'].replace('Z', '+00:00'))
                                expires_at = expires_at.strftime('%Y-%m-%d %H:%M:%S')
                            except Exception as e:
                                logger.warning(f"Could not parse expirationTime in refresh response: {e}")
                        elif 'expiresIn' in data:
                            try:
                                from datetime import datetime, timedelta
                                expires_in_seconds = int(data['expiresIn'])
                                expires_at = (datetime.now() + timedelta(seconds=expires_in_seconds)).strftime('%Y-%m-%d %H:%M:%S')
                            except Exception as e:
                                logger.warning(f"Could not parse expiresIn in refresh response: {e}")
                        
                        # Fallback: Tradovate access tokens typically expire in 90 minutes
                        if not expires_at:
                            from datetime import datetime, timedelta
                            expires_at = (datetime.now() + timedelta(minutes=85)).strftime('%Y-%m-%d %H:%M:%S')
                        
                        cursor.execute('''
                            UPDATE accounts 
                            SET tradovate_token = ?, 
                                tradovate_refresh_token = ?,
                                token_expires_at = ?
                            WHERE id = ?
                        ''', (new_access_token, existing_refresh_token, expires_at, account_id))
                        conn.commit()
                        conn.close()
                        logger.info(f"‚úÖ Successfully refreshed token for '{account_name}' via {token_url.split('/')[2]}")
                        
                        # UPDATE SCALABILITY WS MANAGER with new token
                        # This ensures WebSocket connections use the fresh token
                        if SCALABILITY_MODULE_AVAILABLE:
                            try:
                                from scalability import get_ws_manager
                                ws_manager = get_ws_manager()
                                if ws_manager:
                                    # Get all subaccount IDs for this account
                                    conn2 = get_db_connection()
                                    conn2.row_factory = sqlite3.Row
                                    cursor2 = conn2.cursor()
                                    cursor2.execute('SELECT tradovate_accounts FROM accounts WHERE id = ?', (account_id,))
                                    row = cursor2.fetchone()
                                    conn2.close()
                                    
                                    if row and row['tradovate_accounts']:
                                        import json
                                        subaccounts = json.loads(row['tradovate_accounts'])
                                        for sub in subaccounts:
                                            sub_id = sub.get('id') or sub.get('accountId')
                                            if sub_id:
                                                ws_manager.update_token(sub_id, new_access_token)
                                                logger.info(f"üì° Updated WS Manager token for subaccount {sub_id}")
                            except Exception as ws_err:
                                logger.debug(f"Could not update WS Manager token: {ws_err}")
                        
                        return True
                elif response.status_code == 429:
                    logger.warning(f"‚ö†Ô∏è Rate limited (429) at {token_url.split('/')[2]}, trying next...")
                    time.sleep(1)  # Brief pause before trying next endpoint
                    continue
                else:
                    logger.debug(f"Refresh failed at {token_url}: {response.status_code}")
                    continue
                    
            except requests.exceptions.RequestException as e:
                logger.debug(f"Request error at {token_url}: {e}")
                continue
        
        # CHECK: Does this account have credentials that will work for trading?
        # API Access during trades uses username/password (bypasses refresh token issues)
        try:
            cursor = conn.cursor()
            _ph = '%s' if is_using_postgres() else '?'
            cursor.execute(f'SELECT username, password FROM accounts WHERE id = {_ph}', (account_id,))
            creds_row = cursor.fetchone()
            username = None
            password = None
            if creds_row:
                username = creds_row['username'] if isinstance(creds_row, dict) else creds_row[0]
                password = creds_row['password'] if isinstance(creds_row, dict) else creds_row[1]
            
            if username and password:
                # Account has credentials - trades will work via API Access
                conn.close()
                logger.info(f"‚ö†Ô∏è [{account_name}] Refresh token expired, but has credentials for API Access - trades will still work")
                # Don't mark as needing reauth - trading will work
                clear_account_reauth(account_id)
                return True  # Consider it "refreshed" since trading will work
        except Exception as e:
            logger.debug(f"Error checking credentials: {e}")
        
        conn.close()
        logger.error(f"‚ùå CRITICAL: No valid auth for '{account_name}' (ID: {account_id}) - no refresh token AND no credentials")
        logger.error(f"‚ùå This account will NOT be able to execute trades!")
        logger.error(f"‚ùå User must re-authenticate via OAuth to restore connection")
        # Mark account as needing re-auth (only if no credentials)
        mark_account_needs_reauth(account_id)
        return False
    except Exception as e:
        logger.error(f"Error refreshing Tradovate token: {e}")
        return False

def fetch_tradovate_pnl_sync():
    """
    Fetch real-time PnL directly from Tradovate's cashBalance API.
    This is the CORRECT way to get PnL - Tradovate calculates it for us!
    No market data subscription required.
    """
    global _tradovate_pnl_cache
    
    current_time = time.time()
    
    # Dynamic throttling based on account count to avoid rate limits
    # Tradovate allows ~120 requests/min, each account needs 2 calls (cashBalance + positions)
    # 
    # SCALING TABLE:
    # Accounts | Calls/update | Safe interval | Updates/min
    # ---------|--------------|---------------|------------
    #    1     |      2       |    0.5s       |    120
    #    2     |      4       |    0.5s       |    120  
    #    5     |     10       |    1.0s       |     60
    #   10     |     20       |    2.0s       |     30
    #   20     |     40       |    4.0s       |     15
    #   50     |    100       |   10.0s       |      6
    #
    # Formula: interval = max(0.5, num_accounts * 0.2) to stay under 120 req/min
    # Check if we're in a rate limit cooldown
    rate_limited_until = _tradovate_pnl_cache.get('rate_limited_until', 0)
    if current_time < rate_limited_until:
        # Still in cooldown, return cached data
        return _tradovate_pnl_cache.get('data', {}), _tradovate_pnl_cache.get('positions', [])
    
    # ========================================================================
    # SCALABILITY: Use WebSocket data if fresh (skip REST calls entirely)
    # This saves REST API calls when WebSocket is providing real-time data
    # ========================================================================
    if SCALABILITY_MODULE_AVAILABLE and SCALABILITY_FEATURES.get('ws_state_manager_enabled'):
        try:
            from scalability import get_state_cache
            cache = get_state_cache()
            if cache:
                cache_stats = cache.get_stats()
                accounts_tracked = cache_stats.get('accounts_tracked', 0)
                
                # Only use cache if we have accounts being tracked via WebSocket
                if accounts_tracked > 0:
                    all_snapshot = cache.get_all_accounts_snapshot()
                    
                    # Check if ALL tracked accounts have fresh data (< 5 seconds old)
                    all_fresh = True
                    fresh_pnl_data = {}
                    fresh_positions = []
                    
                    for acc_id, snapshot in all_snapshot.items():
                        pnl = snapshot.get('pnl') or {}
                        updated_at = pnl.get('_updated_at', 0)
                        
                        # If any account has stale data (> 5s), fall back to REST
                        if current_time - updated_at > 5:
                            all_fresh = False
                            break
                        
                        # Build legacy format from cache
                        fresh_pnl_data[acc_id] = {
                            'account_name': pnl.get('account_name', f'Account-{acc_id}'),
                            'is_demo': pnl.get('is_demo', True),
                            'user_id': pnl.get('user_id'),
                            'open_pnl': pnl.get('openPnL', 0) or pnl.get('open_pnl', 0),
                            'realized_pnl': pnl.get('realizedPnL', 0) or pnl.get('realized_pnl', 0),
                            'net_liq': pnl.get('netLiq', 0) or pnl.get('net_liq', 0),
                            'total_cash_value': pnl.get('totalCashValue', 0) or pnl.get('total_cash_value', 0),
                            'total_pnl': pnl.get('totalPnL', 0) or pnl.get('total_pnl', 0),
                        }
                        
                        # Build positions from cache
                        for pos in snapshot.get('positions', []):
                            net_qty = pos.get('netPos', 0) or pos.get('net_quantity', 0)
                            if net_qty != 0:
                                fresh_positions.append({
                                    'account_id': acc_id,
                                    'account_name': pnl.get('account_name', f'Account-{acc_id}'),
                                    'is_demo': pnl.get('is_demo', True),
                                    'user_id': pnl.get('user_id'),
                                    'contract_id': pos.get('contractId', pos.get('contract_id')),
                                    'symbol': pos.get('symbol', pos.get('contractName', 'Unknown')),
                                    'net_quantity': net_qty,
                                    'avg_price': pos.get('avgPrice', pos.get('avg_price', 0)),
                                })
                    
                    if all_fresh and fresh_pnl_data:
                        # WebSocket data is fresh - skip REST calls entirely!
                        logger.debug(f"üì° Using WebSocket cache data ({accounts_tracked} accounts, skipping REST)")
                        return fresh_pnl_data, fresh_positions
                    
        except Exception as e:
            # If cache check fails, fall through to REST polling
            logger.debug(f"Scalability cache check failed (using REST): {e}")
    
    num_accounts = _tradovate_pnl_cache.get('account_count', 2)
    if not isinstance(num_accounts, int) or num_accounts < 1:
        num_accounts = 2
    # Increased minimum interval to reduce rate limiting (trades are priority, PnL is secondary)
    # Formula: max(3.0, num_accounts * 0.6) - very conservative to stay under 120 req/min
    min_interval = max(3.0, num_accounts * 0.6)  # Scale up as accounts increase (very conservative - trades priority)
    
    if current_time - _tradovate_pnl_cache['last_fetch'] < min_interval:
        return _tradovate_pnl_cache['data'], _tradovate_pnl_cache['positions']
    
    try:
        # Get ALL connected accounts from database (multi-account support for copy trading)
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id, name, tradovate_token, tradovate_accounts, environment, user_id
            FROM accounts 
            WHERE tradovate_token IS NOT NULL AND tradovate_token != ''
        ''')
        all_linked_accounts = cursor.fetchall()
        conn.close()
        
        if not all_linked_accounts:
            return {}, []
        
        all_pnl_data = {}
        all_positions = []
        total_subaccounts = 0
        
        # Process each linked account (user may have multiple Tradovate logins)
        for account in all_linked_accounts:
            account_id = account['id']
            account_user_id = account['user_id']  # Track which user owns this account
            # Get valid token (auto-refreshes if needed)
            token = get_valid_tradovate_token(account_id)
            if not token:
                logger.warning(f"No valid token available for account {account_id} - skipping")
                continue
            
            env = account['environment'] or 'demo'
            user_account_name = account['name'] if account['name'] else f"Account {account_id}"  # User's custom name
            
            headers = {
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json'
            }
            
            # Parse tradovate_accounts to get subaccount IDs
            tradovate_accounts = []
            try:
                if account['tradovate_accounts']:
                    tradovate_accounts = json.loads(account['tradovate_accounts'])
            except:
                pass
            
            total_subaccounts += len(tradovate_accounts)
            
            # Fetch PnL for each subaccount under this linked account
            for ta in tradovate_accounts:
                acc_id = ta.get('id')
                subaccount_name = ta.get('name', str(acc_id))  # Tradovate's subaccount name
                # CRITICAL FIX: Use environment as source of truth for demo vs live
                env = (ta.get('environment') or 'demo').lower()
                is_demo = env != 'live'
                # Display as "UserName - SubaccountName" (like account dropdown)
                acc_name = f"{user_account_name} - {subaccount_name}"
                
                # Use correct base URL for demo vs live accounts
                acc_base_url = 'https://demo.tradovateapi.com/v1' if is_demo else 'https://live.tradovateapi.com/v1'
                
                try:
                    # Get cash balance snapshot (includes openPnL!)
                    response = requests.get(
                        f'{acc_base_url}/cashBalance/getCashBalanceSnapshot?accountId={acc_id}',
                        headers=headers,
                        timeout=5
                    )
                    
                    if response.status_code == 200:
                        snap = response.json()
                        all_pnl_data[acc_id] = {
                            'account_id': acc_id,
                            'account_name': acc_name,
                            'is_demo': is_demo,
                            'user_id': account_user_id,  # Track owner for user isolation
                            'total_cash_value': snap.get('totalCashValue', 0),
                            'net_liq': snap.get('netLiq', 0),
                            'open_pnl': snap.get('openPnL', 0),  # Unrealized PnL!
                            'realized_pnl': snap.get('realizedPnL', 0),
                            'total_pnl': snap.get('totalPnL', 0),
                            'week_realized_pnl': snap.get('weekRealizedPnL', 0),
                            'initial_margin': snap.get('initialMargin', 0),
                            'maintenance_margin': snap.get('maintenanceMargin', 0)
                        }
                        logger.debug(f"Fetched PnL for {acc_name}: openPnL=${snap.get('openPnL', 0):.2f}, realizedPnL=${snap.get('realizedPnL', 0):.2f}")
                    elif response.status_code == 429:
                        # Rate limited - enter cooldown for 60 seconds
                        cooldown_seconds = 60
                        _tradovate_pnl_cache['rate_limited_until'] = current_time + cooldown_seconds
                        logger.warning(f"Rate limited by Tradovate (429)! Entering {cooldown_seconds}s cooldown")
                        return _tradovate_pnl_cache.get('data', {}), _tradovate_pnl_cache.get('positions', [])
                    elif response.status_code == 401:
                        logger.warning(f"‚ö†Ô∏è Token expired for account {acc_id} (401) - token validation should have caught this")
                        # Token should have been refreshed proactively, but if we still get 401, try one more refresh
                        refreshed = try_refresh_tradovate_token(account_id)
                        if refreshed:
                            logger.info(f"‚úÖ Token refreshed for account {account_id}, will retry on next cycle")
                        else:
                            logger.error(f"‚ùå CRITICAL: Failed to refresh token for account {account_id} - account connection broken!")
                            logger.error(f"‚ùå User must re-authenticate via OAuth to restore connection")
                        continue
                    else:
                        logger.debug(f"Cash balance API returned {response.status_code} for {acc_id}: {response.text[:100]}")
                    
                    # Get positions for this account
                    pos_response = requests.get(
                        f'{acc_base_url}/position/list',
                        headers=headers,
                        timeout=5
                    )
                    
                    if pos_response.status_code == 200:
                        positions = pos_response.json()
                        for pos in positions:
                            if pos.get('netPos', 0) != 0:  # Only open positions
                                # Get contract name
                                contract_id = pos.get('contractId')
                                contract_name = get_contract_name_cached(contract_id, acc_base_url, headers)
                                
                                all_positions.append({
                                    'account_id': acc_id,
                                    'account_name': acc_name,
                                    'is_demo': is_demo,
                                    'user_id': account_user_id,  # Track owner for user isolation
                                    'contract_id': contract_id,
                                    'symbol': contract_name,
                                    'net_quantity': pos.get('netPos', 0),
                                    'bought': pos.get('bought', 0),
                                    'bought_value': pos.get('boughtValue', 0),
                                    'sold': pos.get('sold', 0),
                                    'sold_value': pos.get('soldValue', 0),
                                    # Calculate avg price from bought/sold values
                                    'avg_price': calculate_avg_price(pos),
                                    'timestamp': pos.get('timestamp')
                                })
                    elif pos_response.status_code == 401:
                        logger.warning(f"‚ö†Ô∏è Token expired when fetching positions for account {acc_id} - attempting auto-refresh")
                        refreshed = try_refresh_tradovate_token(account['id'])
                        if refreshed:
                            logger.info(f"‚úÖ Token refreshed for account {account['id']}, positions will retry on next cycle")
                        else:
                            logger.error(f"‚ùå CRITICAL: Failed to refresh token for account {account['id']} - position fetch failed!")
                                
                except requests.exceptions.RequestException as e:
                    logger.warning(f"Error fetching PnL/positions for account {acc_id}: {e}")
                    continue
        
        # Update cache (including total subaccount count for dynamic throttling)
        _tradovate_pnl_cache['last_fetch'] = current_time
        _tradovate_pnl_cache['data'] = all_pnl_data
        _tradovate_pnl_cache['positions'] = all_positions
        _tradovate_pnl_cache['account_count'] = total_subaccounts  # Total across all linked accounts
        
        if total_subaccounts > 5:
            logger.info(f"Monitoring {total_subaccounts} subaccounts, update interval: {max(0.5, total_subaccounts * 0.2):.1f}s")
        
        # Debug: Log what we're returning
        if all_pnl_data:
            logger.info(f"üìä Returning PnL data for {len(all_pnl_data)} accounts, {len(all_positions)} positions")
        
        return all_pnl_data, all_positions
        
    except Exception as e:
        logger.error(f"Error fetching Tradovate PnL: {e}")
        import traceback
        logger.debug(f"Traceback: {traceback.format_exc()}")
        return _tradovate_pnl_cache.get('data', {}), _tradovate_pnl_cache.get('positions', [])

# Cache for contract names
_contract_name_cache = {}

def get_contract_name_cached(contract_id, base_url, headers):
    """Get contract name from ID, with caching"""
    global _contract_name_cache
    
    if contract_id in _contract_name_cache:
        return _contract_name_cache[contract_id]
    
    try:
        response = requests.get(
            f'{base_url}/contract/item?id={contract_id}',
            headers=headers,
            timeout=5
        )
        if response.status_code == 200:
            contract = response.json()
            name = contract.get('name', str(contract_id))
            _contract_name_cache[contract_id] = name
            return name
    except:
        pass
    
    return str(contract_id)

def calculate_avg_price(position):
    """Get average entry price from position data.
    
    CRITICAL: Use netPrice from broker - this IS the correct average entry price.
    Don't calculate from boughtValue/bought - that gives wrong values for DCA positions.
    """
    # netPrice IS the broker's calculated average entry price - use it directly!
    net_price = position.get('netPrice')
    if net_price:
        return net_price
    
    # Fallback only if netPrice not available (shouldn't happen)
    net_pos = position.get('netPos', 0)
    if net_pos == 0:
        return 0
    
    if net_pos > 0:
        bought = position.get('bought', 0)
        bought_value = position.get('boughtValue', 0)
        if bought > 0:
            return bought_value / bought
    else:
        sold = position.get('sold', 0)
        sold_value = position.get('soldValue', 0)
        if sold > 0:
            return sold_value / sold
    
    return 0

# ============================================================================
# PROACTIVE TOKEN REFRESH - Prevents session expiration throughout the day
# ============================================================================
# This thread runs every 5 minutes and refreshes any Tradovate tokens that
# will expire within 30 minutes. This keeps connections alive INDEFINITELY.
# ============================================================================

def proactive_token_refresh():
    """
    Background thread that proactively refreshes Tradovate tokens BEFORE they expire.
    
    CRITICAL FIX (Dec 18, 2025): 
    - Changed from 30 min to 5 min intervals (tokens can expire in 90 min!)
    - Changed from 2 hour to 30 min threshold (refresh well before expiry)
    
    This keeps sessions alive INDEFINITELY without manual re-login.
    """
    logger.info("üîê Proactive token refresh thread started (checks every 5 minutes)")
    
    # Wait 60 seconds before first check to let server fully start
    time.sleep(60)
    
    while True:
        try:
            conn = get_db_connection()
            is_postgres = is_using_postgres()
            if not is_postgres:
                conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # Find accounts with tokens that expire within 30 minutes
            # Also refresh tokens where we don't know the expiration (NULL)
            # More aggressive refresh keeps connections alive INDEFINITELY
            if is_postgres:
                cursor.execute('''
                    SELECT id, name, tradovate_token, tradovate_refresh_token, 
                           token_expires_at, environment
                    FROM accounts 
                    WHERE tradovate_token IS NOT NULL 
                      AND tradovate_refresh_token IS NOT NULL
                      AND (
                          token_expires_at IS NULL 
                          OR token_expires_at < NOW() + INTERVAL '30 minutes'
                      )
                ''')
            else:
                cursor.execute('''
                    SELECT id, name, tradovate_token, tradovate_refresh_token, 
                           token_expires_at, environment
                    FROM accounts 
                    WHERE tradovate_token IS NOT NULL 
                      AND tradovate_refresh_token IS NOT NULL
                  AND (
                      token_expires_at IS NULL 
                      OR token_expires_at < datetime('now', '+30 minutes')
                  )
            ''')
            accounts_to_refresh = cursor.fetchall()
            conn.close()
            
            if accounts_to_refresh:
                logger.info(f"üîê Found {len(accounts_to_refresh)} account(s) with tokens expiring soon")
                
                for account in accounts_to_refresh:
                    account_id = account['id']
                    account_name = account['name']
                    expires_at = account['token_expires_at']
                    
                    if expires_at:
                        logger.info(f"üîÑ Proactively refreshing token for '{account_name}' (expires: {expires_at})")
                    else:
                        logger.info(f"üîÑ Proactively refreshing token for '{account_name}' (expiration unknown)")
                    
                    # Use existing refresh function
                    success = try_refresh_tradovate_token(account_id)
                    
                    if success:
                        logger.info(f"‚úÖ Proactively refreshed token for '{account_name}' - good for 24 more hours")
                    else:
                        logger.warning(f"‚ö†Ô∏è Failed to refresh token for '{account_name}' - may need to re-authenticate via OAuth")
                    
                    # Small delay between accounts to avoid rate limiting
                    time.sleep(2)
            else:
                logger.info("üîê All Tradovate tokens are fresh (not expiring within 30 minutes)")
                
        except Exception as e:
            logger.error(f"Error in proactive token refresh: {e}")

        # Check every 5 minutes (more aggressive to keep tokens fresh)
        time.sleep(5 * 60)

def emit_realtime_updates():
    """Emit real-time updates (throttled to avoid rate limits)"""
    global _position_cache
    while True:
        try:
            # ============================================================
            # FETCH REAL-TIME PnL DIRECTLY FROM TRADOVATE
            # This is the correct approach - no market data needed!
            # Note: fetch_tradovate_pnl_sync() has built-in throttling
            # ============================================================
            
            total_pnl = 0.0
            today_pnl = 0.0
            open_pnl = 0.0
            active_positions = 0
            positions_list = []
            
            # Fetch PnL from Tradovate's cashBalance API (throttled internally)
            pnl_data, tradovate_positions = fetch_tradovate_pnl_sync()
            
            if pnl_data:
                # Sum up PnL from all accounts
                for acc_id, acc_data in pnl_data.items():
                    open_pnl += acc_data.get('open_pnl', 0)
                    today_pnl += acc_data.get('realized_pnl', 0)
                    total_pnl += acc_data.get('total_pnl', 0)
                
                logger.debug(f"Tradovate PnL: open=${open_pnl:.2f}, realized=${today_pnl:.2f}, total=${total_pnl:.2f}")
            
            if tradovate_positions:
                active_positions = len(tradovate_positions)
                positions_list = [{
                    'symbol': pos.get('symbol', 'Unknown'),
                    'net_quantity': pos.get('net_quantity', 0),
                    'avg_price': pos.get('avg_price', 0),
                    'account_id': pos.get('account_id'),
                    'account_name': pos.get('account_name'),
                    'is_demo': pos.get('is_demo', True),
                    # Note: unrealized_pnl per position requires market data
                    # But we have total open_pnl from cashBalance
                } for pos in tradovate_positions]
            
            # Also include any synthetic positions from manual trades
            if _position_cache:
                for cache_key, pos in _position_cache.items():
                    # Check if this position is already in tradovate_positions
                    exists = any(
                        p.get('symbol') == pos.get('symbol') and 
                        p.get('account_id') == pos.get('account_id')
                        for p in positions_list
                    )
                    if not exists and pos.get('net_quantity', 0) != 0:
                        positions_list.append(pos)
                        active_positions = len(positions_list)
            
            # Emit P&L updates with REAL data from Tradovate
            socketio.emit('pnl_update', {
                'total_pnl': total_pnl,
                'open_pnl': open_pnl,  # Unrealized PnL
                'today_pnl': today_pnl,  # Realized PnL today
                'active_positions': active_positions,
                'timestamp': datetime.now().isoformat()
            })
            
            # Emit position updates
            socketio.emit('position_update', {
                'positions': positions_list,
                'count': active_positions,
                'pnl_data': pnl_data,  # Include full PnL data per account
                'timestamp': datetime.now().isoformat()
            })
            
            # Note: Position and PnL fetching is now handled by fetch_tradovate_pnl_sync() above
            # The new implementation uses Tradovate's cashBalance API which provides 
            # real-time PnL without needing market data subscription
            
        except Exception as e:
            logger.error(f"Error emitting real-time updates: {e}")
            import traceback
            logger.debug(f"Traceback: {traceback.format_exc()}")
        # Sleep 5 seconds - trades are priority, PnL updates are secondary
        # fetch_tradovate_pnl_sync() has its own throttling, so this just reduces check frequency
        time.sleep(5)  # Check every 5 seconds (trades are priority, PnL is secondary)

def record_strategy_pnl_continuously():
    """Record P&L for all active strategies every second (like Trade Manager)"""
    while True:
        try:
            strategies = []
            
            # Try SQLAlchemy models first
            try:
                from app.database import SessionLocal
                from app.models import Strategy
                
                db = SessionLocal()
                active_strategies = db.query(Strategy).filter(Strategy.active == True).all()
                strategies = [(s.id, s.name) for s in active_strategies]
                db.close()
                
            except (ImportError, Exception):
                # Fallback to unified database connection
                try:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    if is_using_postgres():
                        cursor.execute('''
                            SELECT id, name FROM strategies WHERE enabled = true
                        ''')
                    else:
                        cursor.execute('''
                            SELECT id, name FROM strategies WHERE enabled = 1
                        ''')
                    strategies = cursor.fetchall()
                    conn.close()
                except Exception as e:
                    # If strategies table doesn't exist yet, that's okay
                    if 'no such table' not in str(e).lower() and 'does not exist' not in str(e).lower():
                        logger.debug(f"Strategies table not found: {e}")
                    strategies = []
            
            # Record P&L for each strategy
            for strategy_id, strategy_name in strategies:
                try:
                    # Calculate current P&L for strategy
                    pnl = calculate_strategy_pnl(strategy_id)
                    drawdown = calculate_strategy_drawdown(strategy_id)
                    
                    # Record to database
                    record_strategy_pnl(strategy_id, strategy_name, pnl, drawdown)
                    
                    # Emit real-time update
                    socketio.emit('strategy_pnl_update', {
                        'strategy_id': strategy_id,
                        'strategy_name': strategy_name,
                        'pnl': pnl,
                        'drawdown': drawdown,
                        'timestamp': datetime.now().isoformat()
                    })
                except Exception as e:
                    logger.error(f"Error processing strategy {strategy_id}: {e}")
                
        except Exception as e:
            logger.error(f"Error recording strategy P&L: {e}")
        time.sleep(1)  # Every second

# Start background threads
update_thread = threading.Thread(target=emit_realtime_updates, daemon=True)
update_thread.start()

pnl_recording_thread = threading.Thread(target=record_strategy_pnl_continuously, daemon=True)
pnl_recording_thread.start()

# Start proactive token refresh thread (keeps sessions alive throughout the day)
token_refresh_thread = threading.Thread(target=proactive_token_refresh, daemon=True)
token_refresh_thread.start()
logger.info("‚úÖ Proactive token refresh thread started (refreshes tokens before expiration)")

# ============================================================================
# DAILY P&L SUMMARY - Discord notifications at market close
# ============================================================================

_daily_summary_sent_today = False

def daily_pnl_summary_worker():
    """
    Background worker that sends daily P&L summaries to Discord users.
    Runs once per day around market close (4:15 PM ET).
    """
    global _daily_summary_sent_today
    
    logger.info("üìä Daily P&L Summary worker started")
    
    while True:
        try:
            from datetime import datetime
            import pytz
            
            # Get current time in Eastern
            eastern = pytz.timezone('America/New_York')
            now = datetime.now(eastern)
            
            # Reset flag at midnight
            if now.hour == 0 and now.minute < 5:
                _daily_summary_sent_today = False
            
            # Send summaries at 4:15 PM ET (after market close)
            if now.hour == 16 and 15 <= now.minute < 20 and not _daily_summary_sent_today:
                if DISCORD_NOTIFICATIONS_ENABLED:
                    logger.info("üìä Sending daily P&L summaries to Discord users...")
                    send_daily_summaries()
                    _daily_summary_sent_today = True
            
            time.sleep(60)  # Check every minute
        except Exception as e:
            logger.error(f"Daily P&L summary worker error: {e}")
            time.sleep(60)


def send_daily_summaries():
    """Calculate and send daily P&L summaries to all Discord-enabled users."""
    try:
        users = get_discord_enabled_users()
        if not users:
            return
        
        from datetime import datetime, timedelta
        today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        for user_data in users:
            user_id = user_data['user_id']
            
            try:
                # Get today's closed trades for this user
                conn = get_db_connection()
                cursor = conn.cursor()
                
                if is_using_postgres():
                    cursor.execute('''
                        SELECT rt.pnl, rt.closed_at
                        FROM recorded_trades rt
                        JOIN recorders r ON rt.recorder_id = r.id
                        WHERE r.user_id = %s 
                        AND rt.status = 'closed'
                        AND rt.closed_at >= %s
                    ''', (user_id, today_start.isoformat()))
                else:
                    cursor.execute('''
                        SELECT rt.pnl, rt.closed_at
                        FROM recorded_trades rt
                        JOIN recorders r ON rt.recorder_id = r.id
                        WHERE r.user_id = ? 
                        AND rt.status = 'closed'
                        AND rt.closed_at >= ?
                    ''', (user_id, today_start.isoformat()))
                
                rows = cursor.fetchall()
                conn.close()
                
                if not rows:
                    continue  # No trades today, skip
                
                # Calculate stats
                pnls = []
                for row in rows:
                    pnl = row[0] if isinstance(row, tuple) else row.get('pnl')
                    if pnl is not None:
                        pnls.append(float(pnl))
                
                if not pnls:
                    continue
                
                total_trades = len(pnls)
                winners = sum(1 for p in pnls if p > 0)
                losers = sum(1 for p in pnls if p < 0)
                total_pnl = sum(pnls)
                best_trade = max(pnls) if pnls else None
                worst_trade = min(pnls) if pnls else None
                
                # Send summary
                notify_daily_summary(
                    user_id=user_id,
                    total_trades=total_trades,
                    winners=winners,
                    losers=losers,
                    total_pnl=total_pnl,
                    best_trade=best_trade,
                    worst_trade=worst_trade
                )
                
            except Exception as user_err:
                logger.warning(f"Error sending summary to user {user_id}: {user_err}")
                
    except Exception as e:
        logger.error(f"Error in send_daily_summaries: {e}")


# Start daily P&L summary worker
if DISCORD_NOTIFICATIONS_ENABLED:
    daily_summary_thread = threading.Thread(target=daily_pnl_summary_worker, daemon=True)
    daily_summary_thread.start()
    logger.info("üìä Daily P&L Summary worker started (sends at 4:15 PM ET)")

# ============================================================================
# Auto-Flat After Cutoff - Automatically closes positions after trading window
# ============================================================================

def auto_flat_after_cutoff_worker():
    """
    Background task that checks if any recorders have positions that should be 
    closed because we're outside the trading window and auto_flat_after_cutoff is enabled.
    """
    from datetime import datetime, timedelta
    
    logger.info("üîÑ Auto-Flat After Cutoff worker started")
    
    def parse_time(time_str):
        """Parse time string like '8:45 AM' or '13:45' to datetime.time"""
        if not time_str:
            return None
        time_str = time_str.strip()
        try:
            if 'AM' in time_str.upper() or 'PM' in time_str.upper():
                return datetime.strptime(time_str.upper(), '%I:%M %p').time()
            return datetime.strptime(time_str, '%H:%M').time()
        except:
            return None
    
    def is_time_past_cutoff(current_time, stop_str):
        """Check if current time is past the cutoff"""
        stop = parse_time(stop_str)
        if not stop:
            return False
        # Check if we're within 1 minute after the cutoff (to avoid repeated closes)
        current = current_time.time()
        stop_dt = datetime.combine(datetime.today(), stop)
        current_dt = datetime.combine(datetime.today(), current)
        # Close if we're between stop and stop + 2 minutes
        return stop_dt <= current_dt <= stop_dt + timedelta(minutes=2)
    
    while True:
        try:
            # Get current time (local time - assumes server is in correct timezone)
            now = datetime.now()
            
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Find recorders with auto_flat_after_cutoff enabled and time filters set
            if is_using_postgres():
                cursor.execute('''
                    SELECT r.id, r.name, r.time_filter_1_stop, r.time_filter_2_stop, r.auto_flat_after_cutoff
                    FROM recorders r
                    WHERE r.auto_flat_after_cutoff = true
                    AND (r.time_filter_1_stop IS NOT NULL OR r.time_filter_2_stop IS NOT NULL)
                ''')
            else:
                cursor.execute('''
                    SELECT r.id, r.name, r.time_filter_1_stop, r.time_filter_2_stop, r.auto_flat_after_cutoff
                    FROM recorders r
                    WHERE r.auto_flat_after_cutoff = 1
                    AND (r.time_filter_1_stop IS NOT NULL OR r.time_filter_2_stop IS NOT NULL)
                ''')
            recorders = cursor.fetchall()
            
            for rec in recorders:
                rec = dict(rec)
                recorder_id = rec['id']
                recorder_name = rec['name']
                time_filter_1_stop = rec.get('time_filter_1_stop')
                time_filter_2_stop = rec.get('time_filter_2_stop')
                
                # Check if we just hit cutoff time
                at_cutoff_1 = is_time_past_cutoff(now, time_filter_1_stop) if time_filter_1_stop else False
                at_cutoff_2 = is_time_past_cutoff(now, time_filter_2_stop) if time_filter_2_stop else False
                
                if at_cutoff_1 or at_cutoff_2:
                    cutoff_time = time_filter_1_stop if at_cutoff_1 else time_filter_2_stop
                    logger.info(f"üïê [{recorder_name}] At cutoff time ({cutoff_time}) - checking for open positions")
                    
                    # Check for open positions
                    placeholder = '%s' if is_using_postgres() else '?'
                    cursor.execute(f'''
                        SELECT id, ticker, side, quantity FROM recorded_trades
                        WHERE recorder_id = {placeholder} AND status = 'open'
                    ''', (recorder_id,))
                    open_trades = cursor.fetchall()
                    
                    if open_trades:
                        logger.info(f"üîÑ [{recorder_name}] Found {len(open_trades)} open trades - AUTO-FLATTENING")
                        
                        # Get trader info for closing
                        enabled_val = 'true' if is_using_postgres() else '1'
                        cursor.execute(f'''
                                SELECT t.*, a.tradovate_token, a.username, a.password, a.id as account_id
                                FROM traders t
                                JOIN accounts a ON t.account_id = a.id
                            WHERE t.recorder_id = {placeholder} AND t.enabled = {enabled_val}
                                LIMIT 1
                            ''', (recorder_id,))
                        trader_row = cursor.fetchone()
                        
                        if trader_row:
                            from recorder_service import close_all_positions_for_recorder
                            try:
                                close_result = close_all_positions_for_recorder(recorder_id)
                                logger.info(f"‚úÖ [{recorder_name}] Auto-flat result: {close_result}")
                            except Exception as e:
                                logger.error(f"‚ùå [{recorder_name}] Auto-flat failed: {e}")
                        else:
                            logger.warning(f"‚ö†Ô∏è [{recorder_name}] No trader linked - cannot auto-flat")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"‚ùå Auto-flat worker error: {e}")
            import traceback
            traceback.print_exc()
        
        time.sleep(60)  # Check every minute

# Start auto-flat after cutoff worker
auto_flat_thread = threading.Thread(target=auto_flat_after_cutoff_worker, daemon=True)
auto_flat_thread.start()
logger.info("‚úÖ Auto-Flat After Cutoff worker started")

# Start Tradovate market data WebSocket
if WEBSOCKETS_AVAILABLE:
    start_market_data_websocket()
    logger.info("‚úÖ Market data WebSocket thread started")
else:
    logger.warning("websockets library not installed. Market data WebSocket will not work. Install with: pip install websockets")

# ============================================================================
# RECORDER THREADS DISABLED - Now handled by Trading Engine (port 8083)
# ============================================================================
# The following threads have been moved to recorder_service.py (Trading Engine):
# - TP/SL monitoring (poll_recorder_trades_tp_sl)
# - Position drawdown tracking (poll_recorder_positions_drawdown)
# - TradingView WebSocket for recorders
#
# DO NOT RE-ENABLE THESE - they would duplicate the Trading Engine's work
# and cause race conditions with the shared database.
# ============================================================================

# Enable recorder TP/SL monitoring for drawdown tracking (Trade Manager style)
start_recorder_tp_sl_polling()
logger.info("‚úÖ Recorder TP/SL monitoring ENABLED - tracking drawdown on recorded_trades")

# Enable position drawdown tracking for unrealized P&L (Trade Manager style)
start_position_drawdown_polling()
logger.info("‚úÖ Position drawdown tracking ENABLED - using TradingView prices for unrealized P&L")

# Auto-start TradingView WebSocket if session is configured (Trade Manager style real-time prices)
try:
    tv_session = get_tradingview_session()
    if tv_session and tv_session.get('sessionid'):
        start_tradingview_websocket()
        logger.info("‚úÖ TradingView WebSocket AUTO-STARTED for real-time prices")
    else:
        logger.info("‚ÑπÔ∏è TradingView session not configured - set via /api/tradingview/session")
except Exception as e:
    logger.warning(f"Could not auto-start TradingView WebSocket: {e}")

# Configure logging for production
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# ============================================================================
# THREAD HEALTH WATCHDOG - Ensures threads never die overnight
# ============================================================================

# Registry of critical threads that must stay alive
_critical_threads = {}

def register_critical_thread(name: str, thread: threading.Thread, restart_func):
    """Register a thread for health monitoring"""
    _critical_threads[name] = {
        'thread': thread,
        'restart_func': restart_func,
        'restarts': 0,
        'last_restart': None
    }

def thread_health_watchdog():
    """
    CRITICAL: Watchdog that monitors all critical threads and restarts dead ones.
    This ensures the system NEVER stops overnight.
    
    Runs every 60 seconds and:
    1. Checks if each critical thread is alive
    2. Restarts dead threads
    3. Logs all restarts for debugging
    """
    logger.info("üêï Thread Health Watchdog started - monitoring all critical threads")
    
    while True:
        try:
            time.sleep(60)  # Check every 60 seconds
            
            dead_threads = []
            for name, info in _critical_threads.items():
                thread = info['thread']
                if thread is None or not thread.is_alive():
                    dead_threads.append(name)
            
            if dead_threads:
                logger.warning(f"üö® WATCHDOG: Found {len(dead_threads)} dead threads: {dead_threads}")
                
                for name in dead_threads:
                    info = _critical_threads[name]
                    restart_func = info['restart_func']
                    
                    if restart_func:
                        try:
                            logger.info(f"üîÑ WATCHDOG: Restarting thread '{name}'...")
                            new_thread = restart_func()
                            if new_thread:
                                _critical_threads[name]['thread'] = new_thread
                                _critical_threads[name]['restarts'] += 1
                                _critical_threads[name]['last_restart'] = datetime.now().isoformat()
                                logger.info(f"‚úÖ WATCHDOG: Thread '{name}' restarted successfully (total restarts: {info['restarts'] + 1})")
                            else:
                                logger.error(f"‚ùå WATCHDOG: Failed to restart thread '{name}' - restart function returned None")
                        except Exception as e:
                            logger.error(f"‚ùå WATCHDOG: Error restarting thread '{name}': {e}")
                            import traceback
                            traceback.print_exc()
                    else:
                        logger.warning(f"‚ö†Ô∏è WATCHDOG: No restart function for thread '{name}'")
            else:
                # Log healthy status every 10 minutes (every 10th check)
                pass  # Silent when healthy to reduce log noise
                
        except Exception as e:
            logger.error(f"‚ùå WATCHDOG ERROR: {e}")
            import traceback
            traceback.print_exc()

def restart_tradingview_websocket():
    """Restart TradingView WebSocket thread"""
    global _tradingview_ws_thread
    try:
        start_tradingview_websocket()
        return _tradingview_ws_thread
    except Exception as e:
        logger.error(f"Failed to restart TradingView WebSocket: {e}")
        return None

def restart_tradingview_health_monitor():
    """Restart TradingView health monitor thread"""
    global _tradingview_health_thread
    try:
        start_tradingview_health_monitor()
        return _tradingview_health_thread
    except Exception as e:
        logger.error(f"Failed to restart TradingView health monitor: {e}")
        return None

def restart_tp_sl_polling():
    """Restart TP/SL polling thread"""
    global _recorder_tp_sl_thread
    try:
        start_recorder_tp_sl_polling()
        return _recorder_tp_sl_thread
    except Exception as e:
        logger.error(f"Failed to restart TP/SL polling: {e}")
        return None

def restart_position_drawdown_polling():
    """Restart position drawdown polling thread"""
    global _position_drawdown_thread
    try:
        start_position_drawdown_polling()
        return _position_drawdown_thread
    except Exception as e:
        logger.error(f"Failed to restart position drawdown polling: {e}")
        return None

@app.route('/api/traders/<int:trader_id>/debug', methods=['GET'])
def api_trader_debug(trader_id):
    """Debug endpoint to check trader settings including multipliers"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        cursor.execute(f'''
            SELECT id, recorder_id, enabled, enabled_accounts, initial_position_size, add_position_size,
                   multiplier, account_id, subaccount_id, subaccount_name, user_id
            FROM traders WHERE id = {placeholder}
        ''', (trader_id,))
        trader = cursor.fetchone()
        conn.close()

        if not trader:
            return jsonify({'success': False, 'error': 'Trader not found'}), 404

        trader_dict = dict(trader) if hasattr(trader, 'keys') else {
            'id': trader[0],
            'recorder_id': trader[1],
            'enabled': trader[2],
            'enabled_accounts': trader[3],
            'initial_position_size': trader[4] if len(trader) > 4 else None,
            'add_position_size': trader[5] if len(trader) > 5 else None,
            'multiplier': trader[6] if len(trader) > 6 else None,
            'account_id': trader[7] if len(trader) > 7 else None,
            'subaccount_id': trader[8] if len(trader) > 8 else None,
            'subaccount_name': trader[9] if len(trader) > 9 else None,
            'user_id': trader[10] if len(trader) > 10 else None
        }
        
        # Parse enabled_accounts
        enabled_accounts_raw = trader_dict.get('enabled_accounts')
        enabled_accounts_parsed = None
        if enabled_accounts_raw:
            try:
                enabled_accounts_parsed = json.loads(enabled_accounts_raw) if isinstance(enabled_accounts_raw, str) else enabled_accounts_raw
            except:
                pass
        
        return jsonify({
            'success': True,
            'trader_id': trader_id,
            'enabled': trader_dict.get('enabled'),
            'initial_position_size': trader_dict.get('initial_position_size'),
            'add_position_size': trader_dict.get('add_position_size'),
            'multiplier': trader_dict.get('multiplier'),
            'account_id': trader_dict.get('account_id'),
            'subaccount_id': trader_dict.get('subaccount_id'),
            'subaccount_name': trader_dict.get('subaccount_name'),
            'user_id': trader_dict.get('user_id'),
            'enabled_accounts_raw': str(enabled_accounts_raw)[:500] if enabled_accounts_raw else None,
            'enabled_accounts_parsed': enabled_accounts_parsed,
            'multipliers': [
                {
                    'account_name': acct.get('account_name', 'Unknown'),
                    'subaccount_id': acct.get('subaccount_id'),
                    'multiplier': acct.get('multiplier', 'NOT SET'),
                    'max_contracts': acct.get('max_contracts', 0),
                    'custom_ticker': acct.get('custom_ticker', '')
                }
                for acct in (enabled_accounts_parsed or [])
            ] if enabled_accounts_parsed else []
        })
    except Exception as e:
        logger.error(f"Error in trader debug: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/signal-pipeline', methods=['GET'])
def api_signal_pipeline():
    """View signal pipeline - see exactly where each signal is in the process.

    This tracks signals through all 9 steps:
    1. RECEIVED - Webhook endpoint received signal
    2. LOGGED_RAW - Logged to raw webhooks
    3. QUEUED - Queued for fast processing
    4. WORKER_PICKED - Fast worker picked from queue
    5. PROCESSING - Started processing (dedup, validation, filters)
    6. BROKER_QUEUED - Queued for broker execution
    7. BROKER_WORKER_PICKED - Broker worker picked task
    8. CALLING_BROKER - Calling Tradovate/broker API
    9. TRADE_SUCCESS/TRADE_FAILED - Final result
    """
    try:
        limit = int(request.args.get('limit', 50))
        show_pending = request.args.get('pending', 'false').lower() == 'true'

        if show_pending:
            # Show only signals stuck in pending state
            signals = get_pending_signals()
            return jsonify({
                'success': True,
                'mode': 'pending_only',
                'pending_count': len(signals),
                'signals': signals,
                'note': 'These signals have been pending for >30 seconds and may be stuck'
            })
        else:
            # Show recent signals with full pipeline
            signals = get_signal_pipeline(limit)

            # Calculate stats
            completed = sum(1 for s in signals.values() if s['status'] == 'complete')
            failed = sum(1 for s in signals.values() if s['status'] == 'failed')
            pending = sum(1 for s in signals.values() if s['status'] == 'pending')

            return jsonify({
                'success': True,
                'total': len(signals),
                'stats': {
                    'completed': completed,
                    'failed': failed,
                    'pending': pending,
                    'success_rate': f"{(completed/(completed+failed)*100):.1f}%" if (completed+failed) > 0 else "N/A"
                },
                'signals': signals
            })
    except Exception as e:
        logger.error(f"Error in signal pipeline: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/broker-execution/status', methods=['GET'])
@admin_or_api_key_required
def api_broker_execution_status():
    """Diagnostic endpoint to check broker execution status"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Check enabled traders
        cursor.execute(f'''
            SELECT COUNT(*) as total,
                   COUNT(CASE WHEN enabled = {'true' if is_postgres else '1'} THEN 1 END) as enabled_count
            FROM traders
        ''')
        trader_stats = dict(cursor.fetchone()) if cursor.rowcount > 0 else {'total': 0, 'enabled_count': 0}
        
        # Check traders with enabled_accounts
        cursor.execute(f'''
            SELECT id, recorder_id, enabled, enabled_accounts
            FROM traders
            WHERE enabled = {'true' if is_postgres else '1'}
        ''')
        enabled_traders = []
        for row in cursor.fetchall():
            trader = dict(row)
            enabled_accounts = trader.get('enabled_accounts')
            if enabled_accounts:
                try:
                    accounts = json.loads(enabled_accounts) if isinstance(enabled_accounts, str) else enabled_accounts
                    trader['enabled_accounts_count'] = len(accounts) if isinstance(accounts, list) else 0
                except:
                    trader['enabled_accounts_count'] = 0
            else:
                trader['enabled_accounts_count'] = 0
            enabled_traders.append(trader)
        
        conn.close()

        # Check queue status and HIVE MIND worker status
        queue_size = broker_execution_queue.qsize()

        # In external engine mode, read stats from Redis; otherwise use local
        if _EXTERNAL_ENGINE:
            from redis_state import get_broker_stats, get_engine_health
            broker_stats = get_broker_stats()
            engine_health = get_engine_health()
            workers_alive = engine_health.get('workers_alive', 0)
        else:
            broker_stats = _broker_execution_stats
            engine_health = None
            workers_alive = sum(1 for t in _broker_execution_threads if t.is_alive()) if _broker_execution_threads else 0

        # Check fast webhook worker status
        fast_webhook_workers_alive = sum(1 for t in _fast_webhook_threads if t.is_alive()) if _fast_webhook_threads else 0
        fast_webhook_queue_size = _fast_webhook_queue.qsize()

        last_exec_time = broker_stats.get('last_execution_time')
        if isinstance(last_exec_time, str):
            try:
                last_exec_time = float(last_exec_time)
            except (ValueError, TypeError):
                last_exec_time = None

        response = {
            'success': True,
            'external_engine_mode': _EXTERNAL_ENGINE,
            'fast_webhook': {
                'enabled': _fast_webhook_enabled,
                'workers_configured': _fast_webhook_worker_count,
                'workers_alive': fast_webhook_workers_alive,
                'queue_size': fast_webhook_queue_size
            },
            'broker_execution': {
                'hive_mind_enabled': True,
                'workers_configured': _broker_execution_worker_count,
                'workers_alive': workers_alive,
                'queue_size': queue_size,
                'stats': broker_stats,
                'last_execution_ago_seconds': time.time() - last_exec_time if last_exec_time else None
            },
            'traders': {
                'total': trader_stats.get('total', 0),
                'enabled': trader_stats.get('enabled_count', 0),
                'enabled_traders': enabled_traders
            },
            'signal_blocking': {k: {'side': v['side'], 'age': round(time.time() - v['set_at'])} for k, v in dict(_signal_blocking_positions).items()} if not _EXTERNAL_ENGINE else {}
        }
        if engine_health:
            response['trading_engine'] = engine_health

        return jsonify(response)
    except Exception as e:
        logger.error(f"Error getting broker execution status: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/broker-execution/failures', methods=['GET'])
@admin_or_api_key_required
def api_broker_failures():
    """Get recent broker execution failures with details for debugging."""
    limit = request.args.get('limit', 20, type=int)
    if _EXTERNAL_ENGINE:
        from redis_state import get_broker_failures_from_redis
        failures = get_broker_failures_from_redis(limit)
    else:
        failures = get_broker_failures(limit)
    return jsonify({
        'success': True,
        'total_failures': len(failures),
        'failures': failures
    })


@app.route('/api/recorders/<int:recorder_id>/execution-status', methods=['GET'])
def api_recorder_execution_status(recorder_id):
    """Diagnostic endpoint to check why broker execution isn't happening for a recorder"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        is_postgres = is_using_postgres()
        placeholder = '%s' if is_postgres else '?'
        
        # Get recorder info
        cursor.execute(f'SELECT id, name, webhook_token, recording_enabled FROM recorders WHERE id = {placeholder}', (recorder_id,))
        recorder_row = cursor.fetchone()
        
        if not recorder_row:
            conn.close()
            return jsonify({'success': False, 'error': 'Recorder not found'}), 404
        
        recorder = dict(recorder_row) if hasattr(recorder_row, 'keys') else {
            'id': recorder_row[0],
            'name': recorder_row[1],
            'webhook_token': recorder_row[2],
            'recording_enabled': recorder_row[3]
        }
        
        # Get trader info
        cursor.execute(f'''
            SELECT t.id, t.enabled, t.enabled_accounts, t.recorder_id,
                   a.id as account_id, a.name as account_name, a.enabled as account_enabled
            FROM traders t
            LEFT JOIN accounts a ON t.account_id = a.id
            WHERE t.recorder_id = {placeholder}
        ''', (recorder_id,))
        trader_rows = cursor.fetchall()
        
        traders = []
        for row in trader_rows:
            trader = dict(row) if hasattr(row, 'keys') else {
                'id': row[0],
                'enabled': row[1],
                'enabled_accounts': row[2],
                'recorder_id': row[3],
                'account_id': row[4],
                'account_name': row[5],
                'account_enabled': row[6]
            }
            
            # Parse enabled_accounts
            enabled_accounts_raw = trader.get('enabled_accounts')
            enabled_accounts_parsed = None
            if enabled_accounts_raw:
                try:
                    enabled_accounts_parsed = json.loads(enabled_accounts_raw) if isinstance(enabled_accounts_raw, str) else enabled_accounts_raw
                except:
                    pass
            
            trader['enabled_accounts_parsed'] = enabled_accounts_parsed
            trader['enabled_accounts_count'] = len(enabled_accounts_parsed) if enabled_accounts_parsed else 0
            traders.append(trader)
        
        conn.close()

        # Check broker execution workers (HIVE MIND)
        workers_alive = sum(1 for t in _broker_execution_threads if t.is_alive()) if _broker_execution_threads else 0
        queue_size = broker_execution_queue.qsize()

        # Analyze issues
        issues = []
        if not recorder.get('recording_enabled'):
            issues.append("Recorder is disabled (recording_enabled = false)")
        
        enabled_traders = [t for t in traders if t.get('enabled')]
        if len(enabled_traders) == 0:
            issues.append("No enabled traders linked to this recorder")
        else:
            for trader in enabled_traders:
                if not trader.get('enabled_accounts_parsed') or trader.get('enabled_accounts_count', 0) == 0:
                    issues.append(f"Trader {trader.get('id')} has no enabled_accounts (empty or invalid)")
                if not trader.get('account_enabled'):
                    issues.append(f"Trader {trader.get('id')} linked to disabled account {trader.get('account_id')}")
        
        if workers_alive == 0:
            issues.append("No broker execution workers running (CRITICAL)")

        if queue_size >= broker_execution_queue.maxsize:
            issues.append(f"Broker execution queue is full ({queue_size}/{broker_execution_queue.maxsize})")

        return jsonify({
            'success': True,
            'recorder': {
                'id': recorder.get('id'),
                'name': recorder.get('name'),
                'recording_enabled': bool(recorder.get('recording_enabled')),
                'webhook_token': recorder.get('webhook_token')
            },
            'traders': traders,
            'broker_execution': {
                'hive_mind_enabled': True,
                'workers_alive': workers_alive,
                'workers_configured': _broker_execution_worker_count,
                'queue_size': queue_size,
                'queue_maxsize': broker_execution_queue.maxsize,
                'stats': _broker_execution_stats
            },
            'issues': issues,
            'ready_for_execution': len(issues) == 0
        })
    except Exception as e:
        logger.error(f"Error getting recorder execution status: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/broker-execution/test', methods=['POST'])
@admin_or_api_key_required
def api_test_broker_execution():
    """Test endpoint to manually trigger broker execution for debugging"""
    try:
        data = request.get_json() or {}
        recorder_id = data.get('recorder_id')
        
        if not recorder_id:
            return jsonify({'success': False, 'error': 'recorder_id required'}), 400
        
        # Create a test broker task
        broker_task = {
            'recorder_id': int(recorder_id),
            'action': data.get('action', 'BUY'),
            'ticker': data.get('ticker', 'MNQ'),
            'quantity': int(data.get('quantity', 1)),
            'tp_ticks': int(data.get('tp_ticks', 10)),
            'sl_ticks': int(data.get('sl_ticks', 0)),
            'retry_count': 0
        }
        
        logger.info(f"üß™ TEST: Manually triggering broker execution: {broker_task}")
        
        # Try to queue it
        try:
            broker_execution_queue.put_nowait(broker_task)
            workers_alive = sum(1 for t in _broker_execution_threads if t.is_alive()) if _broker_execution_threads else 0
            logger.info(f"‚úÖ TEST: Broker task queued successfully")
            return jsonify({
                'success': True,
                'message': 'Broker task queued',
                'task': broker_task,
                'queue_size': broker_execution_queue.qsize(),
                'hive_mind_workers_alive': workers_alive
            })
        except Exception as queue_err:
            logger.error(f"‚ùå TEST: Failed to queue broker task: {queue_err}")
            return jsonify({
                'success': False,
                'error': f'Queue error: {str(queue_err)}',
                'queue_size': broker_execution_queue.qsize(),
                'queue_full': broker_execution_queue.qsize() >= broker_execution_queue.maxsize
            }), 500
            
    except Exception as e:
        logger.error(f"Error in test broker execution: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)}), 500

# Register all critical threads for monitoring
# TradingView WebSocket - CRITICAL for price tracking
if '_tradingview_ws_thread' in dir() and _tradingview_ws_thread:
    register_critical_thread('TradingView-WebSocket', _tradingview_ws_thread, restart_tradingview_websocket)

# TradingView Health Monitor - CRITICAL for auto-reconnect
if '_tradingview_health_thread' in dir() and _tradingview_health_thread:
    register_critical_thread('TradingView-Health', _tradingview_health_thread, restart_tradingview_health_monitor)

# TP/SL Polling - CRITICAL for taking profits
if '_recorder_tp_sl_thread' in dir() and _recorder_tp_sl_thread:
    register_critical_thread('TP-SL-Polling', _recorder_tp_sl_thread, restart_tp_sl_polling)

# Position Drawdown Polling - CRITICAL for tracking P&L
if '_position_drawdown_thread' in dir() and _position_drawdown_thread:
    register_critical_thread('Position-Drawdown', _position_drawdown_thread, restart_position_drawdown_polling)

# Token Refresh - CRITICAL for broker auth
if 'token_refresh_thread' in dir() and token_refresh_thread:
    register_critical_thread('Token-Refresh', token_refresh_thread, None)  # Can't easily restart

# Broker Execution Workers (HIVE MIND) - CRITICAL for executing trades
if '_broker_execution_threads' in dir() and _broker_execution_threads:
    for i, t in enumerate(_broker_execution_threads):
        register_critical_thread(f'Broker-Execution-Worker-{i}', t, restart_broker_execution_worker)

# Start the watchdog thread
watchdog_thread = threading.Thread(target=thread_health_watchdog, daemon=True, name="Thread-Watchdog")
watchdog_thread.start()
logger.info("üêï Thread Health Watchdog started - will restart dead threads automatically")

# API endpoint to check thread health
@app.route('/api/thread-health')
@admin_or_api_key_required
def api_thread_health():
    """Get status of all monitored threads"""
    thread_status = {}
    for name, info in _critical_threads.items():
        thread = info['thread']
        thread_status[name] = {
            'alive': thread.is_alive() if thread else False,
            'restarts': info['restarts'],
            'last_restart': info['last_restart']
        }
    
    all_alive = all(s['alive'] for s in thread_status.values())
    
    return jsonify({
        'success': True,
        'all_threads_healthy': all_alive,
        'threads': thread_status,
        'watchdog_active': watchdog_thread.is_alive(),
        'external_engine_mode': _EXTERNAL_ENGINE,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/trading-engine/health')
def api_trading_engine_health():
    """Get trading engine health status (only meaningful when EXTERNAL_TRADING_ENGINE=1)"""
    if not _EXTERNAL_ENGINE:
        return jsonify({
            'success': True,
            'external_engine_mode': False,
            'message': 'Trading engine runs in-process (single process mode)'
        })
    from redis_state import get_engine_health, get_broker_stats
    return jsonify({
        'success': True,
        'external_engine_mode': True,
        'engine': get_engine_health(),
        'broker_stats': get_broker_stats(),
        'queue_size': broker_execution_queue.qsize()
    })

# ============================================================================
# SUPPORT TICKET SYSTEM
# ============================================================================

@app.route('/api/support/tickets', methods=['GET', 'POST'])
@api_login_required
def support_tickets():
    """Get user's tickets or create a new ticket"""
    try:
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'  # Placeholder for parameterized queries
        
        if request.method == 'GET':
            # Get tickets for current user (or all for admin)
            user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
            
            conn = get_db_connection()
            if not is_postgres:
                conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            if user and user.is_admin:
                # Admin sees all tickets
                cursor.execute('''
                    SELECT t.*, 
                           (SELECT COUNT(*) FROM support_messages WHERE ticket_id = t.id) as message_count,
                           (SELECT message FROM support_messages WHERE ticket_id = t.id ORDER BY created_at DESC LIMIT 1) as last_message
                    FROM support_tickets t 
                    ORDER BY 
                        CASE WHEN t.status = 'open' THEN 0 ELSE 1 END,
                        t.updated_at DESC
                ''')
            elif user:
                # User sees only their tickets
                cursor.execute(f'''
                    SELECT t.*,
                           (SELECT COUNT(*) FROM support_messages WHERE ticket_id = t.id) as message_count
                    FROM support_tickets t 
                    WHERE t.user_id = {ph}
                    ORDER BY t.updated_at DESC
                ''', (user.id,))
            else:
                conn.close()
                return jsonify({'tickets': []})
            
            tickets = [dict(row) for row in cursor.fetchall()]
            conn.close()
            return jsonify({'tickets': tickets})
        
        elif request.method == 'POST':
            # Create new ticket
            data = request.get_json()
            message = data.get('message', '').strip()
            subject = data.get('subject', 'Support Request')
            
            if not message:
                return jsonify({'error': 'Message is required'}), 400
            
            user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
            user_id = user.id if user else None
            user_email = user.email if user else data.get('email', 'anonymous')
            user_name = user.display_name if user else data.get('name', 'Anonymous')
            
            conn = get_db_connection()
            cursor = conn.cursor()
            
            # Create ticket - handle PostgreSQL RETURNING clause
            if is_postgres:
                cursor.execute('''
                    INSERT INTO support_tickets (user_id, user_email, user_name, subject, status)
                    VALUES (%s, %s, %s, %s, 'open')
                    RETURNING id
                ''', (user_id, user_email, user_name, subject))
                ticket_id = cursor.fetchone()['id'] if is_postgres else cursor.fetchone()[0]
            else:
                cursor.execute('''
                    INSERT INTO support_tickets (user_id, user_email, user_name, subject, status)
                    VALUES (?, ?, ?, ?, 'open')
                ''', (user_id, user_email, user_name, subject))
                ticket_id = cursor.lastrowid
            
            # Add first message
            if is_postgres:
                cursor.execute('''
                    INSERT INTO support_messages (ticket_id, sender_type, sender_id, sender_name, message)
                    VALUES (%s, 'user', %s, %s, %s)
                ''', (ticket_id, user_id, user_name, message))
            else:
                cursor.execute('''
                    INSERT INTO support_messages (ticket_id, sender_type, sender_id, sender_name, message)
                    VALUES (?, 'user', ?, ?, ?)
                ''', (ticket_id, user_id, user_name, message))
            
            conn.commit()
            conn.close()
            
            logger.info(f"üì© New support ticket #{ticket_id} from {user_email}: {subject}")
            return jsonify({'success': True, 'ticket_id': ticket_id})
    
    except Exception as e:
        logger.error(f"Error in support_tickets: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

@app.route('/api/support/tickets/<int:ticket_id>/messages', methods=['GET', 'POST'])
@api_login_required
def ticket_messages(ticket_id):
    """Get or add messages to a ticket"""
    try:
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        
        conn = get_db_connection()
        if not is_postgres:
            conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Verify access to ticket
        cursor.execute(f'SELECT * FROM support_tickets WHERE id = {ph}', (ticket_id,))
        ticket = cursor.fetchone()
        if not ticket:
            conn.close()
            return jsonify({'error': 'Ticket not found'}), 404
        
        # Check permission (user can only access their own tickets, admin can access all)
        # Allow access if: admin, ticket owner, or ticket was anonymous (user_id is None)
        ticket_user_id = ticket['user_id']
        if user and not user.is_admin:
            # User is logged in but not admin - check ownership
            if ticket_user_id is not None and ticket_user_id != user.id:
                conn.close()
                logger.warning(f"Access denied: user {user.id} tried to access ticket {ticket_id} owned by {ticket_user_id}")
                return jsonify({'error': 'Access denied'}), 403
        
        if request.method == 'GET':
            cursor.execute(f'''
                SELECT * FROM support_messages 
                WHERE ticket_id = {ph} 
                ORDER BY created_at ASC
            ''', (ticket_id,))
            messages = [dict(row) for row in cursor.fetchall()]
            conn.close()
            return jsonify({'messages': messages, 'ticket': dict(ticket)})
        
        elif request.method == 'POST':
            data = request.get_json()
            message = data.get('message', '').strip()
            
            if not message:
                conn.close()
                return jsonify({'error': 'Message is required'}), 400
            
            # Determine sender type
            if user and user.is_admin:
                sender_type = 'admin'
                sender_name = f"{user.display_name} (Support)"
            elif user:
                sender_type = 'user'
                sender_name = user.display_name
            else:
                sender_type = 'user'
                sender_name = 'Anonymous'
            
            sender_id = user.id if user else None
            
            if is_postgres:
                cursor.execute('''
                    INSERT INTO support_messages (ticket_id, sender_type, sender_id, sender_name, message)
                    VALUES (%s, %s, %s, %s, %s)
                ''', (ticket_id, sender_type, sender_id, sender_name, message))
            else:
                cursor.execute('''
                    INSERT INTO support_messages (ticket_id, sender_type, sender_id, sender_name, message)
                    VALUES (?, ?, ?, ?, ?)
                ''', (ticket_id, sender_type, sender_id, sender_name, message))
            
            # Update ticket timestamp
            cursor.execute(f'''
                UPDATE support_tickets SET updated_at = CURRENT_TIMESTAMP WHERE id = {ph}
            ''', (ticket_id,))
            
            conn.commit()
            message_id = cursor.lastrowid
            conn.close()
            
            logger.info(f"üí¨ New message on ticket #{ticket_id} from {sender_name}")
            return jsonify({'success': True, 'message_id': message_id})
    
    except Exception as e:
        logger.error(f"Error in ticket_messages: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/support/tickets/<int:ticket_id>/status', methods=['POST'])
@api_login_required
def update_ticket_status(ticket_id):
    """Update ticket status (admin only)"""
    try:
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        data = request.get_json()
        status = data.get('status', 'open')
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        if status == 'closed':
            cursor.execute(f'''
                UPDATE support_tickets 
                SET status = {ph}, closed_at = CURRENT_TIMESTAMP, updated_at = CURRENT_TIMESTAMP
                WHERE id = {ph}
            ''', (status, ticket_id))
        else:
            cursor.execute(f'''
                UPDATE support_tickets 
                SET status = {ph}, closed_at = NULL, updated_at = CURRENT_TIMESTAMP
                WHERE id = {ph}
            ''', (status, ticket_id))
        
        conn.commit()
        conn.close()
        
        logger.info(f"üé´ Ticket #{ticket_id} status changed to {status} by {user.display_name}")
        return jsonify({'success': True})
    
    except Exception as e:
        logger.error(f"Error updating ticket status: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500

@app.route('/api/support/tickets/<int:ticket_id>', methods=['DELETE'])
@api_login_required
def delete_ticket(ticket_id):
    """Delete a support ticket and all its messages (admin only)"""
    try:
        is_postgres = is_using_postgres()
        ph = '%s' if is_postgres else '?'
        
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'error': 'Admin access required'}), 403
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Delete messages first (foreign key constraint)
        cursor.execute(f'DELETE FROM support_messages WHERE ticket_id = {ph}', (ticket_id,))
        # Delete ticket
        cursor.execute(f'DELETE FROM support_tickets WHERE id = {ph}', (ticket_id,))
        
        conn.commit()
        conn.close()
        
        logger.info(f"üóëÔ∏è Ticket #{ticket_id} deleted by {user.display_name}")
        return jsonify({'success': True})
    
    except Exception as e:
        logger.error(f"Error deleting ticket: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/support/tickets/count')
@api_login_required
def get_open_ticket_count():
    """Get count of open tickets (for polling)"""
    try:
        user = get_current_user() if USER_AUTH_AVAILABLE and is_logged_in() else None
        if not user or not user.is_admin:
            return jsonify({'count': 0})
        
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) as cnt FROM support_tickets WHERE status = 'open'")
        result = cursor.fetchone()
        count = result['cnt'] if isinstance(result, dict) else result[0]
        conn.close()
        
        return jsonify({'count': count})
    except Exception as e:
        return jsonify({'count': 0, 'error': str(e)})

@app.route('/admin/support')
def admin_support_page():
    """Admin support dashboard"""
    if USER_AUTH_AVAILABLE:
        user = get_current_user()
        if not user or not user.is_admin:
            return redirect('/login?next=/admin/support')
    return render_template('admin_support.html')

# ============================================================================

if __name__ == '__main__':
    # Railway/Production uses PORT env variable, local dev uses --port arg
    parser = argparse.ArgumentParser(description='Start the trading webhook server.')
    parser.add_argument('--port', type=int, default=8082, help='Port to run the server on.')
    args = parser.parse_args()

    # Railway sets PORT env variable - use it if available
    port = int(os.getenv('PORT', args.port))
    
    logger.info(f"Starting Just.Trades server on 0.0.0.0:{port}")
    logger.info("WebSocket support enabled (like Trade Manager)")
    
    # Initialize subscription system
    if SUBSCRIPTION_SYSTEM_AVAILABLE:
        try:
            init_subscription_system()
            create_webhook_handler(app)
            logger.info("‚úÖ Subscription system initialized")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Subscription system init failed: {e}")

    # Initialize copy trader system
    try:
        from copy_trader_models import init_copy_trader_system
        init_copy_trader_system()
        logger.info("Copy trader system initialized")
    except Exception as e:
        logger.warning(f"Copy trader init failed (non-fatal): {e}")

    # Initialize trial abuse protection
    try:
        from trial_abuse_protection import init_trial_abuse_protection
        init_trial_abuse_protection(app)
        logger.info("‚úÖ Trial abuse protection initialized")
    except ImportError:
        logger.debug("Trial abuse protection module not found")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Trial abuse protection init failed: {e}")

    # Initialize subaccount abuse detection
    try:
        from subaccount_abuse_detection import init_subaccount_abuse_detection
        init_subaccount_abuse_detection(app)
        logger.info("‚úÖ Subaccount abuse detection initialized")
    except ImportError:
        logger.debug("Subaccount abuse detection module not found")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Subaccount abuse detection init failed: {e}")

    # Initialize TradingView real-time price service
    if TV_PRICE_SERVICE_AVAILABLE:
        try:
            def on_price_update(symbol, price_data):
                global _market_data_cache
                # Update server-side price cache (used by paper trade TP/SL monitor)
                try:
                    last_price = price_data.get('last_price')
                    if last_price:
                        root = extract_symbol_root(symbol)
                        if root:
                            _market_data_cache[root] = {
                                'last': float(last_price),
                                'bid': float(price_data['bid']) if price_data.get('bid') else None,
                                'ask': float(price_data['ask']) if price_data.get('ask') else None,
                                'source': 'tv_websocket',
                                'updated': time.time()
                            }
                except:
                    pass
                # Emit to connected WebSocket clients
                try:
                    socketio.emit('price_update', {
                        'symbol': symbol,
                        'price': price_data.get('last_price'),
                        'bid': price_data.get('bid'),
                        'ask': price_data.get('ask'),
                        'change': price_data.get('change'),
                        'change_percent': price_data.get('change_percent'),
                        'timestamp': price_data.get('timestamp')
                    }, namespace='/')
                except:
                    pass  # Ignore if no clients connected

            ticker, paper_engine = start_price_service(on_price_update=on_price_update)
            logger.info(f"‚úÖ TradingView price service started - tracking {len(ticker.symbols)} symbols")

            # Start paper trading TP/SL monitor (uses live price feed)
            try:
                start_paper_tpsl_monitor()
                logger.info("‚úÖ Paper trading TP/SL monitor started")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Paper TP/SL monitor failed to start: {e}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è TradingView price service init failed: {e}")
    else:
        logger.info("‚ÑπÔ∏è TradingView price service not available")

    # Start shared WebSocket connection manager (ONE connection per token)
    try:
        from ws_connection_manager import get_connection_manager
        _ws_manager = get_connection_manager()
        _ws_manager.start()
        logger.info("‚úÖ Shared WebSocket connection manager started")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è WebSocket connection manager failed to start: {e}")

    # Start live account max loss monitor (WebSocket-based)
    try:
        from live_max_loss_monitor import start_live_max_loss_monitor
        start_live_max_loss_monitor()
        logger.info("‚úÖ Live max loss monitor started (WebSocket)")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Live max loss monitor failed to start: {e}")

    # Start Pro Copy Trader leader monitor (WebSocket-based)
    try:
        from ws_leader_monitor import start_leader_monitor
        start_leader_monitor()
        logger.info("Pro Copy Trader leader monitor started")
    except Exception as e:
        logger.warning(f"Leader monitor failed to start (non-fatal): {e}")

    # Start WebSocket position monitor (real-time broker sync)
    try:
        from ws_position_monitor import start_position_monitor
        start_position_monitor()
        logger.info("‚úÖ Position WebSocket monitor started (real-time broker sync)")
    except Exception as e:
        logger.warning(f"Position WebSocket monitor failed to start: {e} ‚Äî reconciliation daemon is safety net")

    # Start fast webhook workers (MUST be after app is created)
    try:
        start_fast_webhook_workers()
        logger.info("‚úÖ Fast webhook workers started (10 parallel)")
    except Exception as e:
        logger.error(f"‚ùå Fast webhook workers failed to start: {e}")

    # Pre-warm WebSocket connections for INSTANT execution
    try:
        from recorder_service import start_websocket_prewarm
        start_websocket_prewarm()
        logger.info("üî• WebSocket pre-warm initiated for instant execution")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è WebSocket pre-warm failed: {e}")

    socketio.run(app, host='0.0.0.0', port=port, debug=False, allow_unsafe_werkzeug=True)
