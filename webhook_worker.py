"""
Webhook Queue Worker
====================
Processes webhooks from Redis queue.
Ensures webhooks are never lost and processed in order.

Run as a background service:
    python webhook_worker.py

Or run multiple workers for parallelism:
    python webhook_worker.py --workers 4

Environment variables:
    REDIS_URL - Redis connection URL
    DATABASE_URL - PostgreSQL URL (or uses SQLite)
"""

import os
import sys
import json
import time
import asyncio
import logging
import argparse
import signal
from datetime import datetime
from typing import Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('webhook_worker')

try:
    import redis
    from production_db import db
except ImportError as e:
    logger.error(f"Missing dependencies: {e}")
    sys.exit(1)


class WebhookWorker:
    """
    Processes webhooks from Redis queue.
    
    Benefits over direct processing:
    - Webhooks never lost (persisted in Redis)
    - Rate limiting protection (process at controlled pace)
    - Retries on failure
    - Multiple workers for parallelism
    """
    
    def __init__(self, worker_id: int = 0):
        self.worker_id = worker_id
        self.redis_client = None
        self.running = False
        self.processed_count = 0
        self.error_count = 0
        
        self._init_redis()
    
    def _init_redis(self):
        """Initialize Redis connection."""
        redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')
        try:
            self.redis_client = redis.from_url(redis_url, decode_responses=True)
            self.redis_client.ping()
            logger.info(f"âœ… Worker {self.worker_id}: Redis connected")
        except Exception as e:
            logger.error(f"âŒ Worker {self.worker_id}: Redis connection failed: {e}")
            self.redis_client = None
    
    def _get_broker_position_from_redis(self, account_id: int, symbol: str) -> Optional[Dict[str, Any]]:
        """
        Get broker position from Redis (set by WebSocket listener).
        This is the SOURCE OF TRUTH for position data.
        """
        if not self.redis_client:
            return None
        
        try:
            # Try by symbol first
            key = f"broker_position:{account_id}:{symbol}"
            data = self.redis_client.get(key)
            if data:
                return json.loads(data)
            
            # Try to find any position for this account
            pattern = f"broker_position:{account_id}:*"
            keys = self.redis_client.keys(pattern)
            for k in keys:
                data = self.redis_client.get(k)
                if data:
                    pos = json.loads(data)
                    # Check if symbol matches (partial match for futures)
                    if symbol[:3] in str(pos.get('symbol', '')):
                        return pos
            
            return None
        except Exception as e:
            logger.warning(f"Error getting broker position from Redis: {e}")
            return None
    
    def _process_webhook(self, webhook_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process a single webhook.
        
        This is where the trading logic happens.
        Uses Redis-cached broker position as source of truth.
        """
        result = {
            'success': False,
            'webhook_id': webhook_data.get('id'),
            'recorder': webhook_data.get('recorder'),
            'error': None
        }
        
        try:
            recorder_name = webhook_data.get('recorder')
            action = webhook_data.get('action', '').lower()
            ticker = webhook_data.get('ticker')
            price = webhook_data.get('price')
            
            logger.info(f"ðŸ“¨ Processing webhook: {recorder_name} | {action} | {ticker}")
            
            # Get recorder info
            recorder = db.execute('''
                SELECT r.*, t.subaccount_id, t.is_demo, a.tradovate_token
                FROM recorders r
                JOIN traders t ON r.trader_id = t.id
                JOIN accounts a ON t.account_id = a.id
                WHERE r.name = ? AND r.enabled = 1
            ''', (recorder_name,), fetch='one')
            
            if not recorder:
                result['error'] = f"Recorder '{recorder_name}' not found or disabled"
                return result
            
            # Get broker position from Redis (source of truth)
            account_id = recorder.get('subaccount_id')
            broker_pos = self._get_broker_position_from_redis(account_id, ticker)
            
            if broker_pos:
                broker_qty = broker_pos.get('qty', 0)
                broker_avg = broker_pos.get('avg_price', 0)
                logger.info(f"ðŸ” Redis position: {broker_qty} @ {broker_avg}")
            else:
                broker_qty = 0
                broker_avg = 0
                logger.info(f"ðŸ” Redis position: FLAT (no cached position)")
            
            # Determine if this is DCA or NEW based on broker position
            is_dca = False
            if action == 'buy' and broker_qty > 0:
                is_dca = True
                logger.info(f"ðŸ“ˆ DCA LONG: Adding to existing {broker_qty} position")
            elif action == 'sell' and broker_qty < 0:
                is_dca = True
                logger.info(f"ðŸ“‰ DCA SHORT: Adding to existing {abs(broker_qty)} position")
            else:
                logger.info(f"ðŸ†• NEW {action.upper()} position")
            
            # TODO: Call the actual trade execution function
            # For now, we'll import and call the existing function
            # This would be refactored to use the production_db module
            
            # Import the existing signal processing function
            try:
                from recorder_service import process_signal
                
                # Process the signal
                trade_result = process_signal(
                    recorder_name=recorder_name,
                    action=action,
                    ticker=ticker,
                    price=float(price) if price else None,
                    webhook_data=webhook_data
                )
                
                result['success'] = trade_result.get('success', False)
                result['trade_result'] = trade_result
                
            except ImportError:
                # Fallback - just log for now
                logger.warning("recorder_service.process_signal not available")
                result['success'] = True
                result['note'] = 'Webhook logged but not executed (process_signal not available)'
            
            self.processed_count += 1
            
        except Exception as e:
            logger.error(f"Error processing webhook: {e}")
            result['error'] = str(e)
            self.error_count += 1
        
        return result
    
    def _requeue_webhook(self, webhook_data: Dict[str, Any], error: str):
        """Requeue a failed webhook for retry."""
        if not self.redis_client:
            return
        
        retries = webhook_data.get('retries', 0)
        max_retries = 3
        
        if retries >= max_retries:
            # Move to dead letter queue
            webhook_data['final_error'] = error
            self.redis_client.rpush('webhook_dead_letter', json.dumps(webhook_data))
            logger.error(f"âŒ Webhook moved to dead letter queue after {max_retries} retries")
        else:
            # Requeue with incremented retry count
            webhook_data['retries'] = retries + 1
            webhook_data['last_error'] = error
            self.redis_client.rpush('webhook_queue', json.dumps(webhook_data))
            logger.warning(f"ðŸ”„ Webhook requeued (retry {retries + 1}/{max_retries})")
    
    def run(self):
        """Main worker loop."""
        self.running = True
        logger.info(f"ðŸš€ Worker {self.worker_id} started")
        
        while self.running:
            try:
                # Block waiting for webhook (5 second timeout)
                webhook = db.dequeue_webhook(timeout=5)
                
                if webhook:
                    result = self._process_webhook(webhook)
                    
                    if not result['success'] and result.get('error'):
                        self._requeue_webhook(webhook, result['error'])
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"Worker error: {e}")
                time.sleep(1)
        
        logger.info(f"ðŸ›‘ Worker {self.worker_id} stopped (processed: {self.processed_count}, errors: {self.error_count})")
    
    def stop(self):
        """Stop the worker."""
        self.running = False


def run_workers(num_workers: int):
    """Run multiple workers in parallel."""
    from multiprocessing import Process
    
    processes = []
    
    def worker_process(worker_id):
        worker = WebhookWorker(worker_id)
        worker.run()
    
    for i in range(num_workers):
        p = Process(target=worker_process, args=(i,))
        p.start()
        processes.append(p)
    
    logger.info(f"Started {num_workers} webhook workers")
    
    # Wait for all processes
    try:
        for p in processes:
            p.join()
    except KeyboardInterrupt:
        logger.info("Shutting down workers...")
        for p in processes:
            p.terminate()


def main():
    parser = argparse.ArgumentParser(description='Webhook Queue Worker')
    parser.add_argument('--workers', type=int, default=1, help='Number of workers')
    args = parser.parse_args()
    
    logger.info("=" * 50)
    logger.info("WEBHOOK QUEUE WORKER")
    logger.info("=" * 50)
    logger.info(f"Workers: {args.workers}")
    logger.info("=" * 50)
    
    if args.workers > 1:
        run_workers(args.workers)
    else:
        worker = WebhookWorker(0)
        
        # Handle shutdown signal
        def signal_handler(sig, frame):
            worker.stop()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        worker.run()


if __name__ == '__main__':
    main()
